[{"title":"Kubernetes部署csi-driver","date":"2021-06-19T18:52:38.000Z","path":"p/20900/","text":"本文介绍在k8s集群中部署nfs-csi-driver 环境依赖 Docker Kubernetes（本文是基于Rancher的RKE集群） 前言csi-driver是非常丰富的，新搭建的集群，推荐使用阿里云的NAS作为存储后端（NAS对比云盘，扩展性高、费用低、容错好），使用阿里云提供的NAS CSI Plugin进行csi-driver的安装。 对于有云盘的ECS或者本地服务器来说，可以使用NFS作为存储后端，k8s官方提供了nfs-csi-driver 如果是阿里的ECS，还可以使用阿里提供的disk-csi-driver，使用云盘作为存储后端（有单机故障的风险） 部署nfs-csi-driver1、安装nfs-server1234567891011121314151617181920# 选择一台主机，安装nfs-server$ sudo apt update$ sudo apt install nfs-kernel-server$ mkdir -p /data/nfs$ sudo vim /etc/fstab172.16.1.23:/data/nfs /data/nfs nfs defaults,timeo=900,retrans=5,_netdev 0 0$ sudo vim /etc/exports/data/nfs 172.16.1.23/12(rw,sync,all_squash,anonuid=1001,anongid=1001,no_subtree_check) # 这里设置anonuid和anongid，是为了便于部署bitnami提供的helm charts$ sudo exportfs -ra# bitnami提供的helm-chart的fsGroup默认是1001，如果1001的用户被占用，可以修改用户的uid和gid，将1001空出来给bitnami使用# 其他主机，配置nfs-client$ apt install nfs-common$ sudo mount -t nfs -o vers=4 172.16.1.23:/data/nfs /data/nfs$ sudo vim /etc/fstab172.16.1.23:/data/nfs /data/nfs nfs defaults,timeo=900,retrans=5,_netdev 0 0# 测试nfs访问# 在任意一台机器上创建、修改、删除文件，在其他机器上会对应列出修改 2、安装nfs-csi-driver这里有个小插曲，由于国内的网络环境，无法下载谷歌的docker镜像，所以只能一个个的把它们转到dockerhub上 1234567891011docker pull k8s.gcr.io/sig-storage/livenessprobe:v2.1.0 \\ &amp;&amp; docker tag k8s.gcr.io/sig-storage/livenessprobe:v2.1.0 qq1398371419/gcr.sig-storage.livenessprobe:v2.1.0 \\ &amp;&amp; docker push qq1398371419/gcr.sig-storage.livenessprobe:v2.1.0docker pull k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 \\ &amp;&amp; docker tag k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 qq1398371419/gcr.sig-storage.csi-provisioner:v2.1.0 \\ &amp;&amp; docker push qq1398371419/gcr.sig-storage.csi-provisioner:v2.1.0 docker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 \\ &amp;&amp; docker tag k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 qq1398371419/gcr.sig-storage.csi-node-driver-registrar:v2.1.0 \\ &amp;&amp; docker push qq1398371419/gcr.sig-storage.csi-node-driver-registrar:v2.1.0 安装nfs-csi-driver 1curl -skSL https://gitee.com/qq1398371419/csi-driver-nfs/raw/master/deploy/install-driver.sh | bash -s master -- 3、安装StorageClassnfs-sc.yaml 1234567891011121314---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-csiprovisioner: nfs.csi.k8s.ioparameters: server: 172.16.1.23 # 这里改成自己的内网IP share: /data/nfs # 这里改成自己的nfs共享目录reclaimPolicy: RetainvolumeBindingMode: ImmediatemountOptions: - hard - nfsvers=4.1 1$ kubectl create -f nfs-sc.yaml 4、测试部署Redis123456789101112131415$ vim redis-test.yaml---master: persistence: storageClass: nfs-csi service: nodePort: 30001 type: NodePortreplica: persistence: storageClass: nfs-csiauth: enabled: false$ helm repo add bitnami https://charts.bitnami.com/bitnami$ helm install my-release bitnami/redis -f redis-test.yaml 问题 前面提到的设置anonuid和anongid，是因为部署bitnami提供的helm charts遇到了无法写入文件的问题，之所以设置为1001，因为bitnami默认的uid是1001 容器对于mount的路径的访问权限问题，可以参考https://www.cnblogs.com/sammyliu/p/10129670.html 相关资料 Kubernetes CSI Developer Documentation alibaba-cloud-csi-driver Kubernetes CSI","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://gcdd1993.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://gcdd1993.github.io/tags/Docker/"}]},{"title":"Dubbo使用Kryo序列化协议的思考","date":"2021-06-19T18:04:07.000Z","path":"p/13982/","text":"本文是使用Dubbo过程中的一些思考，不足以作为参考，只为留存记录。 前言Dubbo官方推荐使用kryo或者fst作为RPC序列化协议，原因是这两款序列化协议，性能显著优于其他序列化协议。虽然高效，但其实Dubbo对这两款的支持却不是那么理想，导致使用过程中出现了一些问题 多服务下kryo序列化问题Q1、无法使用kryo类索引红利 在多模块协作的系统中，无法保证服务提供方和消费方DTO数量以及注册顺序的一致，可能A作为服务提供方，提供了DTO，而B作为消费方，可能同时消费A、C的服务，同时自身也对外提供服务，在使用Kryo序列化时，需要同时注册A、B、C服务的DTO。 例如A系统的SerializationOptimizer实现 1234567891011public class ASerializationOptimizerImpl implements SerializationOptimizer &#123; @Override public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() &#123; return A1.class; return A2.class; return A3.class; return A4.class; return C1.class; return C2.class; &#125;&#125; B系统的SerializationOptimizer实现 12345678910111213public class BSerializationOptimizerImpl implements SerializationOptimizer &#123; @Override public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() &#123; return A1.class; return A2.class; return A3.class; return A4.class; return B1.class; return B2.class; return C1.class; return C2.class; &#125;&#125; 这里需要说明的是，kryo之所以高效，不仅仅是因为其二进制序列化，更由于其可以预先为待序列化的类指定索引，在RPC传输过程中，只需要使用索引代替全类名，在类使用非常频繁的情况下，可以节省大量字节，从而大大提升传输效率。 而Dubbo似乎也意识到了这个问题，所以将一些使用频繁的类进行了预处理，见org.apache.dubbo.common.serialize.kryo.utils.AbstractKryoFactory#create 但是其中很重要的一处 1234567SerializableClassRegistry.getRegisteredClasses().forEach((clazz, ser) -&gt; &#123; if (ser == null) &#123; kryo.register(clazz); &#125; else &#123; kryo.register(clazz, (Serializer) ser); &#125;&#125;); 使用的是kryo.register(clazz)，无法指定kryo class index，这里我分析了一下，可能是为了兼容老的序列化协议，比如json等，而且SerializableClassRegistry是很早就有的接口，并没有考虑到类索引的问题。 所以这里要注册的话，只能将项目内所有类都写到同一个jar中，然后写一个统一的SerializationOptimizer实现。 这在微服务系统中是无法实现的，因为每个服务必然会提供自己的dto.jar和dubbo-service.jar，而且服务之间也不可能依赖其他所有的服务，所以这一条无法满足。 这样一来，类的注册顺序不能保证，类的数量也无法保证，所以Dubbo的kryo序列化只能说在一定程度上提升了效率，但是并没有完全发挥出kryo的性能优势。 Q2、社区不活跃 虽然阿里重启了Dubbo，并且加入了Apache进行孵化，但是社区相较于Spring Cloud，还是不够活跃，而且阿里的开源产品，多多少少带了点阿里内部的味道，不如Spring Cloud通用和考虑周全。一些功能我们不需要（比如dubbo注册时的一堆参数，很多都是需要阿里的一套开发体系才能使用到），我们需要的又迟迟不添加（kryo的ClassId支持）。 所以目前项目已经全面切换到Spring Cloud Kubernetes，有时间也会总结一下Dubbo切换到Spring Cloud的经验，以及在云原生时代Spring Cloud所做出的努力。 相关资料 Dubbo Issue Kryo 和 FST 序列化 Kryo序列化协议","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://gcdd1993.github.io/tags/Dubbo/"}]},{"title":"Gradle 配置片段","date":"2021-06-19T17:29:55.000Z","path":"p/9306/","text":"本文介绍Gradle常用配置，本人很早就开始使用Gradle代替Maven作为项目构建工具，Gradle相较于Maven繁琐的XML配置来说，确实更为先进，依托于Groovy脚本的强大，也更加灵活。 1、Jetbrains IDEA设置Gradle不自动创建模块 在IDEA某个版本之后，创建或导入Gradle项目的时候，无法再取消勾选Create separate module per source set，多少有点强加的意思，因为自动为每个资源文件夹创建一个目录，模块多了以后，会出现意料之外的错误，比如某个依赖无法加载等问题 编辑.idea/gradle.xml，加上一行&lt;option name=&quot;resolveModulePerSourceSet&quot; value=&quot;false&quot; /&gt;就行 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project version=&quot;4&quot;&gt; &lt;component name=&quot;GradleMigrationSettings&quot; migrationVersion=&quot;1&quot; /&gt; &lt;component name=&quot;GradleSettings&quot;&gt; &lt;option name=&quot;linkedExternalProjectsSettings&quot;&gt; &lt;GradleProjectSettings&gt; &lt;option name=&quot;delegatedBuild&quot; value=&quot;true&quot; /&gt; &lt;option name=&quot;testRunner&quot; value=&quot;GRADLE&quot; /&gt; &lt;option name=&quot;distributionType&quot; value=&quot;LOCAL&quot; /&gt; &lt;option name=&quot;externalProjectPath&quot; value=&quot;$PROJECT_DIR$&quot; /&gt; &lt;option name=&quot;gradleHome&quot; value=&quot;$PROJECT_DIR$/../../../../DevTools/Gradle/gradle-6.8.3&quot; /&gt; &lt;option name=&quot;gradleJvm&quot; value=&quot;#JAVA_HOME&quot; /&gt; &lt;option name=&quot;resolveModulePerSourceSet&quot; value=&quot;false&quot; /&gt; &lt;/GradleProjectSettings&gt; &lt;/option&gt; &lt;/component&gt;&lt;/project&gt; 然后重新导入Gradle项目 2、使用阿里云Maven仓库加速依赖下载 Gradle 6.8.3即以上适用 编辑settings.gradle 123456789101112131415161718192021222324pluginManagement &#123; repositories &#123; mavenLocal() repositories &#123; maven &#123; url &#x27;https://maven.aliyun.com/repository/google&#x27; &#125; maven &#123; url &#x27;https://maven.aliyun.com/repository/gradle-plugin&#x27; &#125; maven &#123; url &#x27;https://maven.aliyun.com/repository/public/&#x27; &#125; &#125; mavenCentral() gradlePluginPortal() &#125;&#125;dependencyResolutionManagement &#123; repositories &#123; mavenLocal() maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/central&quot;) &#125; // central maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/public&quot;) &#125; // jcenter &amp; public maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/google&quot;) &#125; // google maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/spring&quot;) &#125; // spring maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/spring-plugin&quot;) &#125; // spring plugin maven &#123; url = uri(&quot;https://maven.aliyun.com/repository/grails-core&quot;) &#125; // spring plugin &#125;&#125;rootProject.name = &#x27;my-prject&#x27; 3、maven-publish.gradle1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// jar添加以下插件//plugins &#123;// id &#x27;java-library&#x27;// id &#x27;maven-publish&#x27;//&#125;// bom添加以下插件//plugins &#123;// id &#x27;java-platform&#x27;// id &#x27;maven-publish&#x27;//&#125;def artifactory = &#x27;https://packages.aliyun.com/maven/repository/&#x27;def releasePath = &quot;xxx&quot;def snapshotsPath = &quot;xxx&quot;def mavenName = &#x27;my-project&#x27;afterEvaluate &#123; Project project -&gt; if (project.plugins.hasPlugin(&quot;maven-publish&quot;)) &#123; if (project.plugins.hasPlugin(&quot;java&quot;)) &#123; java &#123; withSourcesJar() &#125; &#125; publishing &#123; repositories &#123; maven &#123; name &quot;$&#123;mavenName&#125;&quot; url = &quot;$&#123;artifactory&#125;$&#123;(version.endsWith(&#x27;SNAPSHOT&#x27;) ? snapshotsPath : releasePath)&#125;&quot; credentials &#123; username &quot;$&#123;maven_username&#125;&quot; password &quot;$&#123;maven_password&#125;&quot; &#125; authentication &#123; basic(BasicAuthentication) &#125; &#125; &#125; publications &#123; omac(MavenPublication) &#123; from components.java versionMapping &#123; usage(&#x27;java-api&#x27;) &#123; fromResolutionOf(&#x27;runtimeClasspath&#x27;) &#125; usage(&#x27;java-runtime&#x27;) &#123; fromResolutionResult() &#125; &#125; &#125; &#125; &#125; &#125;&#125; 在build.gradle引入即可 1apply from: &quot;$&#123;rootDir&#125;/gradle/maven-publish.gradle&quot; 4、docker.gradle123456789101112131415161718192021222324252627282930afterEvaluate &#123; if (pluginManager.hasPlugin(&quot;application&quot;)) &#123; task archiveDeps(type: Tar) &#123; task -&gt; def dependencies = task.project.configurations.runtimeClasspath.fileCollection &#123; true &#125; from dependencies.sort() compression = Compression.GZIP archiveFileName = &quot;dep-libs.tar.gz&quot; from jar.archiveFile destinationDirectory = file(&quot;$buildDir/docker&quot;) &#125; task copyDeps(type: Copy) &#123; dependsOn(archiveDeps) from tarTree(resources.gzip(&quot;$buildDir/docker/dep-libs.tar.gz&quot;)) into &quot;$buildDir/docker/libs/&quot; &#125; task copyApp(type: Copy) &#123; dependsOn(jar, startScripts) from &quot;$buildDir/scripts/&quot; from jar.outputs into &quot;$buildDir/docker/&quot; &#125; task prepare() &#123; dependsOn(copyApp, copyDeps) &#125; &#125;&#125; 添加插件application 123plugins &#123; id &quot;application&quot;&#125; 并配合以下Dockerfile 123456789101112131415161718192021222324252627282930313233343536373839# First stage: complete build environmentFROM registry.cn-shanghai.aliyuncs.com/halmawork/gradle:6.8.3-jdk11-openj9 AS builderARG MAVEN_USERNAMEARG MAVEN_PASSWORDARG MODULERUN echo &quot;org.gradle.daemon=false&quot; &gt;&gt; ~/.gradle/gradle.properties \\ &amp;&amp; echo &quot;org.gradle.parallel=true&quot; &gt;&gt; ~/.gradle/gradle.properties \\ &amp;&amp; echo &quot;maven_username=$MAVEN_USERNAME&quot; &gt;&gt; ~/.gradle/gradle.properties \\ &amp;&amp; echo &quot;maven_password=$MAVEN_PASSWORD&quot; &gt;&gt; ~/.gradle/gradle.propertiesWORKDIR /builderCOPY . /builder# package jarRUN gradle :$MODULE:clean :$MODULE:prepare# Second stage: minimal runtime environmentFROM adoptopenjdk/openjdk11:jreENV MODULE &quot;&quot;ENV TZ=Asia/ShanghaiLABEL name=$MODULERUN ln -fs /usr/share/zoneinfo/$TZ /etc/localtime \\ &amp;&amp; dpkg-reconfigure -f noninteractive tzdataWORKDIR /appCOPY --from=builder /builder/build/docker/libs/*.jar /app/lib/COPY --from=builder /builder/build/docker/$MODULE /app/bin/runCOPY --from=builder /builder/build/docker/$MODULE-*.jar /app/lib/EXPOSE 8080CMD [&quot;/app/bin/run&quot;] 打包docker镜像 1$ docker build -f gradle/Dockerfile -t gcdd1993/demo:latest-snapshot . 5、支持kotlingradle.properties 1kotlin_version=1.5.10 build.gradle 12345678910111213141516171819202122plugins &#123; id &quot;org.jetbrains.kotlin.jvm&quot; version &quot;$&#123;kotlin_version&#125;&quot; apply false // jvm 插件 id &quot;org.jetbrains.kotlin.plugin.spring&quot; version &quot;$&#123;kotlin_version&#125;&quot; apply false // spring 插件，allopen 插件 id &quot;org.jetbrains.kotlin.kapt&quot; version &quot;$&#123;kotlin_version&#125;&quot; apply false // kapt，代替annotationProcessor id &quot;org.jetbrains.kotlin.plugin.noarg&quot; version &quot;$&#123;kotlin_version&#125;&quot; apply false // 用于自定义注解生成no arg constructor&#125;apply plugin: &quot;org.jetbrains.kotlin.jvm&quot;apply plugin: &quot;org.jetbrains.kotlin.plugin.spring&quot;apply plugin: &quot;org.jetbrains.kotlin.kapt&quot;apply plugin: &quot;org.jetbrains.kotlin.plugin.noarg&quot;compileKotlin &#123; kotlinOptions &#123; freeCompilerArgs = [&quot;-Xjsr305=warn&quot;] jvmTarget = &quot;11&quot; &#125;&#125;noArg &#123; annotation(&quot;io.github.gcdd1993.NoArg&quot;) // 该注解注释的class，会生成NoArg Constructor&#125; 6、限制项目JDK版本 对于多人协同时，很有用，避免因为JDK版本不一导致的各种问题 1234def javaVersion = System.getProperty(&quot;java.version&quot;)if (!javaVersion.startsWith(&quot;11&quot;)) &#123; throw new RuntimeException(&quot;Incompatible JRE version: &quot; + javaVersion + &quot;. Use JRE 11 instead.&quot;)&#125; 7、为项目编译jar文件添加项目编译信息1234567891011121314151617181920212223242526plugins &#123; id &quot;org.springframework.boot&quot; version &quot;$&#123;spring_boot_version&#125;&quot; apply false id &quot;com.gorylenko.gradle-git-properties&quot; version &quot;2.2.4&quot;&#125;apply plugin: &quot;io.spring.dependency-management&quot;apply plugin: &quot;org.springframework.boot&quot;jar &#123; enabled = true manifest &#123; attributes( &quot;Implementation-Title&quot;: project.name, &quot;Implementation-Version&quot;: project.version, &quot;Built-By&quot;: System.properties[&quot;user.name&quot;], &quot;Build-Timestamp&quot;: new SimpleDateFormat(&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ&quot;).format(new Date()), &quot;Created-By&quot;: &quot;Gradle $&#123;gradle.gradleVersion&#125;&quot;, &quot;Build-Jdk&quot;: &quot;$&#123;System.properties[&quot;java.version&quot;]&#125; ($&#123;System.properties[&quot;java.vendor&quot;]&#125; $&#123;System.properties[&quot;java.vm.version&quot;]&#125;)&quot;, &quot;Build-OS&quot;: &quot;$&#123;System.properties[&quot;os.name&quot;]&#125; $&#123;System.properties[&quot;os.arch&quot;]&#125; $&#123;System.properties[&quot;os.version&quot;]&#125;&quot; ) &#125;&#125;springBoot &#123; buildInfo()&#125; 如果是Spring Boot项目，还可以通过以下方法，在程序启动时，打印出编译信息 12345678910111213141516171819202122232425262728293031323334353637383940/** * 在应用启动时，打印应用基本信息 * * @author gcdd1993 * @date 2021/2/20 * @since 1.0.0 */class AppInfoPreviewAutoConfiguration &#123; private val log = LoggerFactory.getLogger(javaClass) @Autowired(required = false) private val gitProperties: GitProperties? = null @Autowired(required = false) private val buildProperties: BuildProperties? = null @Value(&quot;\\$&#123;spring.application.name&#125;&quot;) private val name: String? = null @EventListener(ApplicationStartedEvent::class) fun onBootUp(event: ApplicationStartedEvent?) &#123; log.info(&quot;&#123;&#125; Started.&quot;, name) if (buildProperties != null) &#123; log.info(&quot;build.name : &#123;&#125;&quot;, buildProperties.name) log.info(&quot;build.artifact : &#123;&#125;&quot;, buildProperties.artifact) log.info(&quot;build.group : &#123;&#125;&quot;, buildProperties.group) log.info(&quot;build.version : &#123;&#125;&quot;, buildProperties.version) log.info(&quot;build.time : &#123;&#125;&quot;, buildProperties.time.atZone(ZoneId.systemDefault()).toLocalDateTime()) &#125; else &#123; log.warn(&quot;cannot find any build properties file from this project. please reference: https://stackoverflow.com/questions/47283048/how-to-capture-build-info-using-gradle-and-spring-boot.&quot;) &#125; if (gitProperties != null) &#123; log.info(&quot;commit.branch : &#123;&#125;&quot;, gitProperties.branch) log.info(&quot;commit.commit.id : &#123;&#125;&quot;, gitProperties.commitId) log.info(&quot;commit.commit.time : &#123;&#125;&quot;, gitProperties.commitTime.atZone(ZoneId.systemDefault()).toLocalDateTime()) &#125; else &#123; log.warn(&quot;cannot find any git properties file from this project. please add gradle plugin: https://plugins.gradle.org/plugin/com.gorylenko.gradle-git-properties.&quot;) &#125; &#125;&#125; 未完待续。。。","tags":[{"name":"Gradle","slug":"Gradle","permalink":"https://gcdd1993.github.io/tags/Gradle/"}]},{"title":"使用certbot给网站上免费的SSL证书","date":"2021-06-19T16:55:06.000Z","path":"p/16060/","text":"本文介绍使用certbot为网站添加HTTPS支持，并自动更新 前提 docker docker-compose 部署克隆仓库 这一步必不可少，一定要按照作者的仓库目录结构来执行，完成后，可以自行更改nginx/conf.d下的配置文件。 具体原因我也不知，但是不照做，会出现一些奇怪的问题。 1234567891011121314$ mkdir -p /data$ cd /data$ git clone https://ghproxy.com/https://github.com/gcdd1993/nginx-certbot$ cd nginx-certbot$ ls -ldrwxr-xr-x 4 root root 4096 Jun 8 22:01 ./drwxr-xr-x 5 root root 4096 Jun 8 21:49 ../drwxr-xr-x 4 root root 4096 Jun 8 21:53 data/-rw-r--r-- 1 root root 660 Jun 8 21:49 docker-compose.ymldrwxr-xr-x 8 root root 4096 Jun 8 21:49 .git/-rw-r--r-- 1 root root 14 Jun 8 21:49 .gitignore-rwxr-xr-x 1 root root 2286 Jun 8 22:01 init-letsencrypt.sh*-rw-r--r-- 1 root root 1074 Jun 8 21:49 LICENSE-rw-r--r-- 1 root root 1376 Jun 8 21:49 README.md 为域名添加证书 💡在这一步执行前，请确认已经将需要添加证书的域名指向本机公网IP，因为在执行过程中，会进行服务器所属权校验，需要访问你所操作的域名 1、修改init-letsencrypt.sh的email为你的邮箱1234$ vim init-letsencrypt.sh...email=&quot;gcwm99@gmail.com&quot;... 2、修改操作域名12$ sed -i &#x27;s/example.org/your_domain/g&#x27; data/nginx/app.conf \\ &amp;&amp; sed -i &#x27;s/example.org/your_domain/g&#x27; init-letsencrypt.sh 3、执行init-letsencrypt.sh 直到出现以下内容，说明已经完成 123456789101112131415161718$ ./init-letsencrypt.sh...Requesting a certificate for your_domainSuccessfully received certificate.Certificate is saved at: /etc/letsencrypt/live/your_domain/fullchain.pemKey is saved at: /etc/letsencrypt/live/your_domain/privkey.pemThis certificate expires on 2021-09-06.These files will be updated when the certificate renews.NEXT STEPS:- The certificate will need to be renewed before it expires. Certbot can automatically renew the certificate in the background, but you may need to take steps to enable that functionality. See https://certbot.org/renewal-setup for instructions.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -If you like Certbot, please consider supporting our work by:* Donating to ISRG / Let&#x27;s Encrypt: https://letsencrypt.org/donate* Donating to EFF: https://eff.org/donate-le- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 4、多域名操作 步骤同上，先修改域名为待操作域名，然后执行init-letsencrypt.sh 1234$ sed -i &#x27;s/your_domain/your_domain2/g&#x27; data/nginx/app.conf \\ &amp;&amp; sed -i &#x27;s/your_domain/your_domain2/g&#x27; init-letsencrypt.sh$ ./init-letsencrypt.sh... 5、启动你的网站1234# 注释app.conf$ cd data/nginx$ mv app.conf app.conf.bak# 添加你的网站配置 示例配置 12345678910111213141516171819202122232425262728293031upstream my.site &#123; server localhost:8080;&#125;server &#123; server_name your_domain; proxy_read_timeout 600s; proxy_send_timeout 600s; location / &#123; add_header X-Frame-Options deny; proxy_pass http://my.site; &#125; listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/your_domain/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/your_domain/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; server_tokens off;&#125;server &#123; if ($host = your_domain) &#123; return 301 https://$host$request_uri; &#125; # managed by Certbot server_name your_domain; listen 80; return 404; # managed by Certbot&#125; 更新证书 作者给出的docker-compose.yml已经默认12小时检查并更新一次，所以非常省心 12345$ docker exec -it nginx-certbot_certbot_1 certbot renew...The following certificates are not due for renewal yet: /etc/letsencrypt/live/your_domain/fullchain.pem expires on 2021-09-06 (skipped)No renewals were attempted. 相关资料 certbot nginx-certbot","tags":[{"name":"默认","slug":"默认","permalink":"https://gcdd1993.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Ubuntu部署docker","date":"2021-06-19T16:29:58.000Z","path":"p/42583/","text":"本文介绍在Ubuntu上部署docker 安装docker123sudo apt-get update &amp;&amp; sudo apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common &amp;&amp; curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - &amp;&amp; sudo apt-key fingerprint 0EBFCD88 &amp;&amp; sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get -y install docker-ce docker-ce-cli containerd.io &amp;&amp; sudo docker --version...Docker version 19.03.12, build 48a66213fe 安装docker-compose123sudo curl -L &quot;https://github.91chifun.workers.dev/https://github.com//docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose &amp;&amp; sudo chmod +x /usr/local/bin/docker-compose &amp;&amp; docker-compose --version...docker-compose version 1.26.2, build eefe0d31 配置docker 可选，但是由于国内网络环境等因素，还是建议做一下配置 1234567891011121314151617181920212223$ sudo vim /etc/docker/daemon.json&#123; &quot;data-root&quot;: &quot;/data/docker&quot;, // 将docker镜像等内容移动到数据盘，注意：之前如果有启动过的容器，或者拉取的镜像，修改这个值，将会失效，谨慎修改 &quot;log-driver&quot;:&quot;json-file&quot;, // 设置log文件格式 &quot;log-opts&quot;:&#123; // 设置log文件 &quot;max-size&quot;:&quot;10m&quot;, &quot;max-file&quot;:&quot;3&quot;, &quot;labels&quot;:&quot;production_status&quot;, &quot;env&quot;:&quot;os,customer&quot; &#125;, &quot;insecure-registries&quot;:[ &quot;registryhost:5000&quot;, &quot;10.0.0.0/8&quot; ], &quot;registry-mirrors&quot;:[ // 阿里云镜像加速 &quot;https://mubkcb81.mirror.aliyuncs.com&quot; ]&#125;$ sudo systemctl daemon-reload$ sudo systemctl restart docker# 配置docker开机自启动sudo systemctl enable docker 相关资料 CentOS Ubuntu Debian Fedora","tags":[{"name":"默认","slug":"默认","permalink":"https://gcdd1993.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"算法很美（蓝桥） | 位运算的奇技淫巧","date":"2021-06-19T16:18:43.534Z","path":"p/60563/","text":"我有话说前阵子跑去面试，有家公司是做电商广告数据分析的，很有意思，上来先做了三道算法题（答得不好），然后面试官花了很长时间为我解答了三道题目，一度让我以为已经是这家公司的员工了。临了，征求了下面试官对我的建议，“不是科班出身（我本科学的物理），计算机基础和算法是偏弱些，这两块要好好打磨打磨。” 前言 在学习算法很美课程的时候，学习到了一些位运算的奇技淫巧，收录在此 判断奇偶数判断奇数1 &amp; x == 11System.out.println((1991 &amp; 1) == 1); 判断偶数1 &amp; x == 0 1System.out.println((1990 &amp; 1) == 0); 获取二进制数x位y是1还是0 将x右移y - 1位，与1 1234int x = 0b010110010;int y = 5;int res = (x &gt;&gt; (y - 1)) &amp; 1;System.out.println(res); 交换两个整数变量的值不用判断语句，求整数的绝对值 异或，可以理解为不进位加法，1 + 1 = 0，0 + 0 = 0，1 + 0 = 1 异或的性质 交换律：可任意交换运算因子的位置，结果不变 结合律：即（a ^ b）^ c == a ^ ( b ^ c ) 对于任何数x，都有x ^ x = 0, x ^ 0 = x，同自己求异或为0，同0求异或为自己 自反性：A ^ B ^ B = A ^ 0 = A，连续和同一个因子做异或运算，最终结果为自己 1234int a = -100;// a为正数，a &gt;&gt; 31 = 00000000 00000000 00000000 00000000，求绝对值就是自己// a为负数，a &gt;&gt; 31 = 11111111 11111111 11111111 11111111，求绝对值就是自己 * -1（00000000 00000000 00000000 00000001）System.out.println((a + (a &gt;&gt; 31) ^ (a &gt;&gt; 31))); 这边课程讲的不是很明白，可以参考下位运算求整数的绝对值，写的很好。","tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://gcdd1993.github.io/tags/algorithm/"}]},{"title":"在Dubbo中使用Kryo序列化协议","date":"2020-12-09T03:43:12.000Z","path":"p/34460/","text":"Kryo是什么？Kryo是用于Java的快速高效的二进制对象图序列化框架。 该项目的目标是高速，小尺寸和易于使用的API。不管是将对象持久保存到文件、数据库还是通过网络传输时，都可以尝试Kryo。 Kryo还可以执行自动的深浅复制/克隆。这是从对象到对象的直接复制，而不是从对象到字节的复制。 具体可以参考Kryo官网 在Dubbo中使用Kryo 本文基于Dubbo版本2.7.8 Dubbo支持非常多的序列化方式，如hession2、avro、FST等等，其中Dubbo官网推荐的序列化方式是Kryo，因为Kryo是一种非常成熟的序列化实现，已经在Twitter、Groupon、Yahoo以及多个著名开源项目（如Hive、Storm）中广泛的使用。 开始在Dubbo中使用Kryo非常方便，首先引入依赖 1234// 解决一些Kryo特殊序列化，https://github.com/magro/kryo-serializersimplementation &#x27;de.javakaffee:kryo-serializers:0.43&#x27;// 高性能序列化框架, https://github.com/EsotericSoftware/kryoimplementation &#x27;com.esotericsoftware:kryo:4.0.2&#x27; 如果只是简单使用，引入kryo即可，如果要支持一些例如List接口，则需要引入kryo-serializers，它针对一些特殊类为Kryo做了适配。 配置在Dubbo中启用Kryo序列化方式，这里使用SpringBoot YML配置方式 123protocol: serialization: kryo optimizer: org.hmwebframework.microservice.dubbo.serialize.SerializationOptimizerImpl 其中org.hmwebframework.microservice.dubbo.serialize.SerializationOptimizerImpl是指定Kryo序列化类，例如 123456789101112public class SerializationOptimizerImpl implements SerializationOptimizer &#123; public Collection&lt;Class&gt; getSerializableClasses() &#123; List&lt;Class&gt; classes = new LinkedList&lt;Class&gt;(); classes.add(BidRequest.class); classes.add(BidResponse.class); classes.add(Device.class); classes.add(Geo.class); classes.add(Impression.class); classes.add(SeatBid.class); return classes; &#125;&#125; 到这，Dubbo使用Kryo就已经OK了。 为什么要定义SerializationOptimizer实现类？首先我们分析下SerializationOptimizer 123456789public interface SerializationOptimizer &#123; /** * Get serializable classes * * @return serializable classes * */ Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses();&#125; 提供了一个接口方法，用于获取序列化的java类型列表，在DubboProtocol#optimizeSerialization中被使用 12345678910111213141516171819202122232425private void optimizeSerialization(URL url) throws RpcException &#123; // ... try &#123; Class clazz = Thread.currentThread().getContextClassLoader().loadClass(className); // 判断是否为SerializationOptimizer实现类 if (!SerializationOptimizer.class.isAssignableFrom(clazz)) &#123; throw new RpcException(&quot;The serialization optimizer &quot; + className + &quot; isn&#x27;t an instance of &quot; + SerializationOptimizer.class.getName()); &#125; SerializationOptimizer optimizer = (SerializationOptimizer) clazz.newInstance(); if (optimizer.getSerializableClasses() == null) &#123; return; &#125; // 将SerializationOptimizer中定义的类型列表，注册到SerializableClassRegistry for (Class c : optimizer.getSerializableClasses()) &#123; SerializableClassRegistry.registerClass(c); &#125; optimizers.add(className); &#125; catch (ClassNotFoundException e) &#123; // ... &#125;&#125; 接着，从SerializableClassRegistry中拿出注册的类型，进行Kryo的类型注册，可以看到SerializableClassRegistry#getRegisteredClasses被FST和Kryo使用，证明FST和Kryo都需要进行序列化类的注册，当然FST也支持不注册序列化类型。 Kryo类注册的具体细节，AbstractKryoFactory#create 123456789101112// ...for (Class clazz : registrations) &#123; kryo.register(clazz);&#125;// 遍历取出SerializableClassRegistry的注册类，依次将类注册到KryoSerializableClassRegistry.getRegisteredClasses().forEach((clazz, ser) -&gt; &#123; if (ser == null) &#123; kryo.register(clazz); &#125; else &#123; kryo.register(clazz, (Serializer) ser); &#125;&#125;); 循环取出SerializableClassRegistry中的注册类进行注册，看到这里也能明白，为什么Dubbo官网的SerializationOptimizer例子需要使用LinkedList。 为什么Kryo需要进行类的注册，且保持顺序？类的注册在Dubbo这样的RPC框架进行通信时，性能瓶颈往往在于RPC传输过程中的网络IO耗时，提升网络IO的办法，一是加大带宽，二是减小传输的字节数，而高性能序列化框架可以做到的就是减小传输的字节数。 Kryo注册类的时候，使用了一个int类型的ID来与类进行关联，在序列化该类的实例时，用int ID来标识类型，反序列化该类时，同样通过int ID来找到类型，这比写出类名高效的多。 维持类注册顺序Kryo注册类的时候，可以指定类关联的int ID，例如 1234Kryo kryo = new Kryo();kryo.register(SomeClass.class, 10);kryo.register(AnotherClass.class, 11);kryo.register(YetAnotherClass.class, 12); 但是上面我们讲到，Dubbo对Kryo做了相当程度的集成，导致我们没法给类指定int ID，但是我们可以保证服务提供方和消费方类注册顺序的一致，间接地保证了int ID的一致性。 优化反射获取代注册的类 在Dubbo中使用Kryo时，我们需要实现一个SerializationOptimizer，并提供一个注册类列表。随着项目规模扩大，不可能时时刻刻想着维护这个注册类列表，所以我们可以使用反射来自动获取这个注册类列表 引入依赖 12// Java反射工具包implementation &#x27;org.reflections:reflections:0.9.11&#x27; 编写接口， 123public interface KryoDubboSerializable extends Serializable &#123;&#125; 编写SerializationOptimizer实现类 123456789101112131415161718192021222324252627282930313233@Slf4jpublic abstract class AbstractSerializationOptimizerImpl implements SerializationOptimizer &#123; private final List&lt;Class&lt;?&gt;&gt; classList; public AbstractSerializationOptimizerImpl() &#123; var reflections = new Reflections( new ConfigurationBuilder() .forPackages(basePackage()) .addScanners(new SubTypesScanner()) ); this.classList = reflections.getSubTypesOf(KryoDubboSerializable.class) .stream() // Kryo序列化协议要求类注册顺序一致 .sorted(Comparator.comparing(Class::getSimpleName)) .collect(Collectors.toList()); log.info(&quot;load &#123;&#125; classes to use kryo serializable&quot;, this.classList.size()); log.debug(&quot;kryo serializable classes: &#123;&#125;&quot;, this.classList.stream().map(Class::getSimpleName).collect(Collectors.joining(&quot;,&quot;))); &#125; @Override public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() &#123; return classList; &#125; /** * 扫描包路径 * * @return packages */ protected abstract String[] basePackage();&#125; 每次使用时，只需要继承AbstractSerializationOptimizerImpl，并提供待注册包路径（支持多个），待注册的类需要实现KryoDubboSerializable接口，这是为了在一定程度上提升灵活性（如果不需要注册到Kryo，不实现该接口即可）。 参考 https://dubbo.apache.org/zh/docs/v2.7/user/serialization/ https://github.com/EsotericSoftware/kryo","tags":[{"name":"Java","slug":"Java","permalink":"https://gcdd1993.github.io/tags/Java/"}]},{"title":"掘金解绑方案（已成功解绑微信）","date":"2020-12-09T03:43:12.000Z","path":"p/34461/","text":"掘金的绑定限制为同一个第三方账号只能绑定一个掘金账号，且必须留存一个第三方绑定。 比如，我只绑定了微信，想要解绑微信，对不起，不支持。 解决方案我突然想到，既然必须留存一个第三方绑定，那我留存一个邮箱绑定不就好了吗？ 思路就是临时电子邮件地址，通过绑定一个邮箱账号，来绑定邮箱，从而将我们的账号解放出来，绑定我们的大号或者其他账号。 获取临时电子邮件地址打开临时电子邮件地址，你将会获取到一个临时电子邮件地址，我们将使用这个邮件绑定我们的掘金账号。 绑定临时邮箱点击绑定邮箱，输入我们上一步获取到的临时邮件地址，返回临时电子邮件地址网站，耐心等待10s左右。 你将会受到掘金的邮箱绑定验证邮件，打开并点击，直到绑定成功。 解绑接下来我们就可以开心的解绑我们自己的账号了。我要解绑的是微信，试一下吧。 友情提醒 由于使用的是一次性邮件地址，该做法可能会导致你解绑的账号登录不上，请谨慎操作！","tags":[{"name":"技巧","slug":"技巧","permalink":"https://gcdd1993.github.io/tags/%E6%8A%80%E5%B7%A7/"}]},{"title":"Scala学习笔记","date":"2020-02-02T05:49:38.000Z","path":"p/37757/","text":"写在前面Scala是一门优秀的编程语言，它是一门纯面向对象的语言，且支持函数式编程。 Scala运行于Jvm，所有Scala的代码，都需要经过编译为字节码，然后交由Java虚拟机来运行。Scala和Java是可以无缝互操作的。Scala可以任意调用Java的代码。 安装Scala 从Scala官方网站下载，http://www.scala-lang.org/download/，windows版本的安装包是`scala-2.11.7.msi`。 使用下载下来的安装包安装Scala。 在PATH环境变量中，配置$SCALA_HOME/bin目录。 在windows命令行内即可直接键入scala，打开scala命令行，进行scala编程。 12345$ scalaWelcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231).Type in expressions for evaluation. Or try :help.scala&gt; 基础语法Scala解释器的使用REPL scala解释器也被称为REPL，会快速编译scala代码为字节码，然后交给JVM来执行。 Read（取值）-&gt; Evaluation（求值）-&gt; Print（打印）-&gt; Loop（循环）。 计算表达式 在scala&gt;命令行内，键入scala代码，解释器会直接返回结果给你。如果你没有指定变量来存放这个值，那么值默认的名称为res，而且会显示结果的数据类型，比如Int、Double、String等等。 例如，输入1 + 1，会看到res0: Int = 2 12scala&gt; 1 + 1res0: Int = 2 内置变量 在后面可以继续使用res这个变量，以及它存放的值。 例如，2.0 * res0，返回res1: Double = 4.0 12scala&gt; 2.0 * res0res1: Double = 4.0 例如，”Hi, “ + res0，返回res2: String = Hi, 2 12scala&gt; &quot;Hi, &quot; + res0res2: String = Hi, 2 自动补全 在scala&gt;命令行内，可以使用Tab键进行自动补全。 例如，输入res2.to，敲击Tab键，解释器会显示出以下选项，toCharArray，toLowerCase，toString，toUpperCase。因为此时无法判定你需要补全的是哪一个，因此会提供给你所有的选项。 12scala&gt; res2.toto toCharArray toIterable toMap ... 例如，输入res2.toU，敲击Tab键，直接会给你补全为res2.toUpperCase。 声明变量声明val变量 可以声明val变量来存放表达式的计算结果。 例如，val result = 1 + 1 12scala&gt; val result = 1 + 1result: Int = 2 后续这些常量是可以继续使用的，例如，2 * result 12scala&gt; 2 * resultres6: Int = 4 但是常量声明后，是无法改变它的值的，例如，result = 1，会返回error: reassignment to val的错误信息。 1234scala&gt; result = 1&lt;console&gt;:12: error: reassignment to val result = 1 ^ 声明var变量 如果要声明值可以改变的引用，可以使用var变量。 例如，val myresult = 1，myresult = 2 但是在Scala程序中，通常建议使用val，也就是常量，因此比如类似于spark的大型复杂系统中，需要大量的网络传输数据，如果使用var，可能会担心值被错误的更改。 在Java的大型复杂系统的设计和开发中，也使用了类似的特性，我们通常会将传递给其他模块 / 组件 / 服务的对象，设计成不可变类（Immutable Class）。在里面也会使用Java的常量定义，比如final，阻止变量的值被改变。从而提高系统的健壮性（robust，鲁棒性），和安全性。 指定类型 无论声明val变量，还是声明var变量，都可以手动指定其类型，如果不指定的话，Scala会自动根据值，进行类型的推断。 例如，val name: String = null 例如，val name: Any = &quot;leo&quot;","tags":[{"name":"Scala","slug":"Scala","permalink":"https://gcdd1993.github.io/tags/Scala/"}]},{"title":"MacOs使用CleanMyMac X清除可清除空间","date":"2019-10-11T10:41:15.000Z","path":"p/46997/","text":"写在前面本文介绍如何使用CleanMyMac X清除可清除的空间 可以看到，可清除的空间达到了125.79GB，虽然说不影响系统的使用，但是在使用时间机器进行备份的时候，仍然会将可清除空间当成备份的一部分，造成备份文件过大，首次备份时间过长。 准备清除可清除空间，你只需要CleanMyMac X这个工具即可，我分享下我使用的版本，当然有能力的建议使用正版。 https://pan.baidu.com/s/1L05kBwZIghM73IRC8rMpMw 开始安装完毕后，打开CleanMyMac X 点击”维护“，你可以使用“释放可清除空间”或者是“时间机器快照瘦身”，我使用的是“时间机器快照瘦身” 建议先使用“时间机器快照瘦身”，如果不行，再释放可清除空间，因为释放可清除空间耗时较长 点击运行，稍作等待即可 这时候，我们回到磁盘工具，再次查看可清除空间，可以发现，可清除空间小了不少！","tags":[{"name":"macOs","slug":"macOs","permalink":"https://gcdd1993.github.io/tags/macOs/"}]},{"title":"TamperMonkey 使用指南以及脚本推荐","date":"2019-10-07T12:56:20.000Z","path":"p/29031/","text":"写在前面Chrome浏览器是最适合开发者使用的浏览器，不仅仅是因为Chrome对于Js的友好支持，更是由于Chrome支持丰富且功能强大的插件，扩展了浏览器的功能和使用体验。 在这些插件里面，相信你一定使用过TamperMonkey，他可以让你加速下载百度网盘，跟百度限速说拜拜，也可以让你免费观看VIP影视和音乐，反正一句话，黑科技！ TamperMonkey使用TamperMonkey的官网是：https://www.tampermonkey.net，支持各类Chrome内核的浏览器以及火狐浏览器（FireFox）。 以下以Chrome浏览器为例。 安装安装非常简单，打开Chrome商店，点击”添加到Chrome” 安装脚本 点击”获取新脚本” 选择合适的脚本源，这里推荐GreasyFork 安装脚本，以VIP视频解析为例 我们点击第一个脚本（有时候会比较慢，请耐心等待下），点击”安装此脚本” 点击安装 下面可以直接看到脚本的源码，有能力的话，可以自己修改或者编写脚本。 看看脚本的效果 我们打开爱奇艺，找个vip电影，比如最近热播的银河补习班 TamperMonkey脚本推荐在TamperMonkey的管理面板中，可以看到已经安装的所有脚本 下面列举出我常用的脚本 52pojie吾爱破解论坛自动签到助手-免打扰 吾爱破解论坛-百度网盘链接激活-提取码自动补全 ac-baidu-重定向优化百度搜狗谷歌搜索-去广告-favicon-双列 ac-baidu-优化百度-搜狗-谷歌搜索结果之关键词自动高亮 csdn自动展开-去广告-净化剪贴板-免登陆 一键vip视频解析-去广告-全网-一站式音乐搜索下载-百度云离线跳转-获取b站封面-淘宝京东优惠券-2019-10-01-更新-报错请及时反馈 城通网盘-皮皮盘-牛盘显示正确下载地址 百度文库文档免费下载-原文档-转换提取文档-文档内容自由复制-移除广告-豆丁网文档下载-解除大部分网站操作限制-全网vip视频免费在线看-支持电视剧免跳出选集 百度网盘直链下载助手 知乎网页助手-5大功能集于一身 贴吧全能助手 TamperMonkey脚本同步TamperMonkey提供了非常方便的方法让我们同步脚本，从而避免每次安装都要重新安装脚本的烦恼。 进入TamperMonkey插件设置页 配置模式选择”初学者” 勾选”启用 TESLA”，类型选择”浏览器同步” 点击保存","tags":[{"name":"Chrome","slug":"Chrome","permalink":"https://gcdd1993.github.io/tags/Chrome/"}]},{"title":"MacOs科学上网","date":"2019-10-06T15:47:37.000Z","path":"p/709/","text":"前言之前使用Windows的时候，有非常优秀的全局代理软件SSTap用来翻墙，但是到了MacOs上，没有找到类似SSTap的全局翻墙神器。 最终采取的方案是ShadowsocksX-NG R8+Proxifier的方式来实现。 软件ShadowsocksX-NG R8 ShadowsocksX-NG是Mac下的SSR工具，具有和Windows下同样的体验，使用起来也非常方便，支持服务器订阅。 配置就不多说了，唯一要注意的是，Socks5的监听地址是：127.0.0.1:1086，这个我们一会要用到。 Proxifier Proxifier是Mac下的全局代理工具，可以将流量统统都转到代理上，配合ShadowsocksX-NG，我们很轻松的就可以实现全局翻墙 软件本身是收费的，当然了，在Xclient.info上可以找到破解版：https://xclient.info/s/proxifier.html 安装完毕之后，我们只需要简单的配置一下，就可以实现全局科学上网了！ 添加Socks5代理 点击Proxies 点击Add，添加代理 添加完毕后，回到主界面，点击Rules，修改默认规则，将动作指向我们新添加的Socks5代理 测试 打开ITerm2测试下，在终端输入 1$ curl www.google.com 同时，Proxifier也打印出了ITerm2的流量信息 另外，Proxifier也支持指定软件走代理，具体步骤如下 在Rules标签中点击Add，新建一个规则 添加完成后勾选使用，同时，不要忘记把Default规则的动作设置为Direct，不然的话，还是全局都走代理的。 相关软件 ShadowsocksX-NG：链接:https://pan.baidu.com/s/10i6PZZIParFRkvaVu5KhxA 密码:jvzx Proxifier：链接:https://pan.baidu.com/s/1ymZZRDJrjrIXXFrYfIxV_w 密码:exns","tags":[{"name":"科学上网","slug":"科学上网","permalink":"https://gcdd1993.github.io/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"}]},{"title":"Dell 工作站M4800 安装macOs Mojave","date":"2019-10-01T01:43:12.000Z","path":"p/62106/","text":"前言最近，入手了一台二手Dell工作站M4800，价格为3600，配置如下 个人感觉还是很好用的，配置够用，关键是用料真的足！虽然是16年的机器，但是做工吊打一众游戏本。 然后，重点来了，我安装上了黑苹果macOs Mojave，等于说花了3600买了台MBP，而且是非常的高配。 目前使用上基本完美，除了无线网卡（买了免驱内置无线网卡在路上），HDMI（暂时不怎么用，不过不是无解）。 黑苹果安装记录 以下内容仅作记录，不保证能安装成功。 安装教程都大差不差，主要是要找到适用机型的EFI文件，然后替换就可以，当然了，如果你有能力，可以自己适配EFI，然后贡献给大家👍👍👍。 如果机型跟我一致，可以使用我整理好的：https://github.com/gcdd1993/Dell-M4800-Hackintosh 教程 我也是小白，不班门弄斧，放上我装的时候的参考链接，照着装，一般都能搞定 单硬盘单系统（推荐） 单硬盘双系统（不推荐，可能出现各种各样的问题） 启动U盘装好后，不要忘记替换适配机型的EFI 然后正式进入黑苹果的安装。 修改BIOS设置（开机按F2） Advanced Boot Options = Enable Legacy Integrated NIC = Enable Parallel Ports = AT Serial Ports = Disabled ( If you are using Dock station then Enable it - Expermental ) Sata Operation = AHCI Drivers = Check all Switchable Graphics = Enable Switchable Graphics Secure Boot = Disabled Virtualization = Disable 完成后退出重启 重启选择从U盘启动（开机按F12）按照教程，抹盘–安装–进入系统 要注意的是，安装会经历3次重启 第一次是安装剩余2分钟的时候，这里要选择硬盘启动（Clover界面会出现Boot install macos from 硬盘分区名），如果你选了U盘启动，那就要再来一次了。 第二次重启，还是选择硬盘启动 第三次重启，可以选择Boot macos from 硬盘分区名，启动macOs了 成功进入系统后，执行最后一步，也是最重要的一步，那就是安装驱动 替换驱动文件这里要用到Clover Configurator密码:zcyj 打开Clover Configurator，选择挂载分区，然后打开分区 替换EFI文件夹（先移除原先的，将大佬们提供的EFI复制进去） 替换驱动文件（kexts） 最后重启试下吧，可以摆脱U盘了 Clover主题修改黑苹果每次开机都会进入Clover引导界面，但是默认的Clover主题是黑黑的，丑丑的，所以我们要替换掉，换一个高大上的引导界面。 这里推荐工具CloverThemeManager WIFI网卡已安装上，使用的是BCM43224，不带蓝牙，免驱，淘宝25块钱左右，mSata接口，使用无异常。 参考 Clover 更新和界面美化 macOs Mojave on M4800 Hackintosh黑苹果长期维护机型EFI及安装教程整理 Mojave硬件支持列表（持续更新中） 黑苹果安装教程","tags":[{"name":"黑苹果","slug":"黑苹果","permalink":"https://gcdd1993.github.io/tags/%E9%BB%91%E8%8B%B9%E6%9E%9C/"}]},{"title":"SpringJpa CRUD代码生成器","date":"2019-09-10T16:34:19.000Z","path":"p/30009/","text":"利用业余时间撸了一个Spring Jpa代码生成器jpa-codegen。 简介这是一款基于Freemarker模板驱动的代码生成器。 依据现有的实体类代码，自动生成CRUD代码，解放双手，加快开发速度。 生成的代码包括但不仅限于（可以自定义生成模块） Form表单代码 Repository代码 Service代码 Controller代码 SpringBoot使用示例克隆示例项目，体会解放双手的美妙感受！ 如何使用导入仓库12345678maven &#123; url &#x27;https://dl.bintray.com/gcdd1993/maven&#x27;&#125;dependencies &#123; // jpa code generator testCompile &#x27;io.github.gcdd1993:jpa-codegen:v1.0.1&#x27; testCompile &#x27;org.freemarker:freemarker:2.3.28&#x27;&#125; 配置代码生成器配置文件1234567891011121314151617181920212223242526272829## 作者author=gcdd1993## 代码注释comments=code generated by jpa-codegen## 是否覆盖原文件，除非特殊情况，不然请不要覆盖cover=false## 代码模板目录template.dir=src/test/resources/template/## 实体类包名 Deprecated从v1.0.1开始从配置文件中移除- entity.package=com.maxtropy.sample.entity## 实体类标识符 Deprecated从v1.0.1开始从配置文件中移除- entity.flag=entity## 以下配置是模块配置(格式 模块名.配置名)，必须在模板目录下提供与模块名相同的模板## 生成的代码后缀repository.suffix=Repository## 模板名称repository.template=repository.ftl## 模块标识符repository.flag=entity.reposervice.suffix=Serviceservice.template=service.ftlservice.flag=serviceform.suffix=Formform.template=form.ftlform.flag=formcontroller.suffix=Controllercontroller.template=controller.ftlcontroller.flag=web 其中 123repository.suffix=Repositoryrepository.template=repository.ftlrepository.flag=entity.repo 是模块配置，什么是模块？ 编写代码模板模板主要基于Freemarker，如Spring Boot2.x代码模板可以像下面这样 123456789101112131415161718package $&#123;packageName&#125;;import $&#123;entity.packageName&#125;.$&#123;entity.className&#125;;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.querydsl.QuerydslPredicateExecutor;&lt;#list imports as import&gt;import $&#123;import&#125;;&lt;/#list&gt;/** * repository for $&#123;entity.className&#125; generated by jpa-codegen * $&#123;comments&#125; * * @author $&#123;author&#125; * Created On $&#123;date&#125;. */public interface $&#123;className&#125; extends JpaRepository&lt;$&#123;entity.className&#125;, $&#123;entity.id.className&#125;&gt;, QuerydslPredicateExecutor&lt;$&#123;entity.className&#125;&gt; &#123;&#125; Spring Boot 2.x模板 如何编写模板? 编写生成器入口在test模块中编写生成器入口，如 12345678910public class Codegen &#123; @Test public void generate() &#123; new CodeGenerator(&quot;src/test/resources/codegen.properties&quot;) .registerRender(&quot;repository&quot;) .generate(); &#125; &#125; 然后运行generate()，在项目目录下将会生成 生成的代码完全由模板以及实体类信息决定。 如何编写模板？模板完全基于FreeMarker以及实体类信息，FreeMarker参考FreeMarker Docs 支持的元素定义如下 基本信息 Freemarker元素 解释 示例输出 $&#123;ftlName&#125; 模板名称 controller.ftl $&#123;ftlPath&#125; 模板目录 src/main/resources/template/ $&#123;savePath&#125; 保存路径 src/main/resources/io/github/gcdd1993/controller $&#123;packageName&#125; java文件包名 io.github.gcdd1993.controller $&#123;className&#125; java文件类名 UserController $&#123;author&#125; 作者 gaochen $&#123;date&#125; 创建日期，默认为当前日期 2019/6/23 $&#123;comments&#125; 注释信息 generated by jpa-codegen $&#123;imports&#125; java文件引入信息 org.springframework.beans.factory.annotation.Autowired 实体信息 Freemarker元素 解释 示例输出 $&#123;entity.className&#125; 实体类名，class.getSimpleName() User $&#123;entity.packageName&#125; 实体包名，class.getPackage().getName() io.github.gcdd1993 $&#123;entity.tableName&#125; 实体表名，@Table(name=&quot;&quot;) sys_user $&#123;entity.id.className&#125; 实体主键类名，@Id注释的字段的类名 Integer $&#123;entity.id.packageName&#125; 实体主键包名，@Id注释的字段的包名 java.lang $&#123;entity.fields.className&#125; 实体所有字段（只支持基本类型）类名 String $&#123;entity.fields.packageName&#125; 实体所有字段（只支持基本类型）包名 java.lang $&#123;entity.fields.name&#125; 实体所有字段（只支持基本类型）属性名 name $&#123;entity.fields.annotations.className&#125; 实体所有字段注解的类名 Id $&#123;entity.fields.annotations.packageName&#125; 实体所有字段注解的包名 javax.persistence 自定义配置除了以上默认的信息之外，可能会有额外的信息需要填入生成的代码中，jpa-codegen提供直接将配置文件中的配置渲染到模板的能力。 例如在配置文件autogen.properties写下一行 1custom.additional.comment=this is additional comment 在模板中可以使用$&#123;otherParams.additional_comment&#125;获取到该配置。 要注意的是：自定义配置使用custom开头，后面的**配置会将.替换为_**作为FreeMarker模板的key，例如上述的additional.comment使用$&#123;otherParams.additional_comment&#125;获取。 什么是模块？由于代码千变万化，为了尽可能的做到通用性，jpa-codegen将每一种类型的代码抽象为模块，每一个模块将使用各自的模板，依照实体信息生成代码。 需要为模板配置一下信息： repository.suffix=Repository 模块类名后缀，生成的类名规则由实体类名+后缀构成 repository.template=repository.ftl 模块使用的Freemarker模板 repository.flag=entity.repo 模块标识符，生成的代码包名由实体类将实体标识符替换为模块标识符来确认。 如 实体包名：io.github.gcdd1993.entity 实体标识符：entity 模块标识符：entity.repo 则生成的repository代码包名为 –&gt; io.github.gcdd1993.entity.repo","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Ubuntu禁用root账号，开启Ubuntu密钥登录","date":"2019-09-10T04:08:44.000Z","path":"p/46186/","text":"新建普通用户12345## 新建普通用户$ adduser ubuntu$ apt-get install sudo## 将用户加入sudo组$ usermod -a -G sudo ubuntu 为普通用户添加公钥123456789$ su ubuntu$ mkdir -p ~/.ssh$ cd ~/.ssh## 添加公钥$ touch authorized_keys$ cat &#x27;你的公钥字符串&#x27; &gt;&gt; authorized_keys$ chmod 600 authorized_keys$ chmod 700 ~/.ssh 设置 SSH，打开密钥登录12345678910$ vim /etc/ssh/sshd_configRSAAuthentication yesPubkeyAuthentication yes## 禁用root账号登录PermitRootLogin no## 禁用密码登录PasswordAuthentication no$ service sshd restart","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://gcdd1993.github.io/tags/Ubuntu/"}]},{"title":"Java设计模式（四）工厂方法模式","date":"2019-07-28T15:52:33.000Z","path":"p/30626/","text":"定义与类型 定义：定义一个创建对象的接口，但让实现这个接口的类来决定实例化哪个类，工厂方法让类的实例化推迟到子类中进行。 类型：创建型 适用场景 创建对象需要大量重复的代码 客户端(应用层)不依赖于产品类实例如何被创建、实现等细节 一个类通过其子类来指定创建哪个对象 优点 用户只需要关心所需产品对应的工厂，无须关心创建细节 加入新产品符合开闭原则，提高可扩展性 缺点 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 Coding工厂方法模式从一定意义上讲是从简单工厂模式衍生过来的，创建产品抽象类 123public abstract class Video &#123; public abstract void produce();&#125; 创建具体产品 123456789101112public class JavaVideo extends Video &#123; @Override public void produce() &#123; System.out.println(&quot;录制Java课程&quot;); &#125;&#125;public class PythonVideo extends Video &#123; @Override public void produce() &#123; System.out.println(&quot;录制Python视频&quot;); &#125;&#125; 创建产品工厂方法抽象类 123public abstract class VideoFactory &#123; public abstract Video getVideo();&#125; 创建产品工厂方法实现类（每个产品都有对应的实现类） 123456789101112public class JavaVideoFactory extends VideoFactory &#123; @Override public Video getVideo() &#123; return new JavaVideo(); &#125;&#125;public class PythonVideoFactory extends VideoFactory &#123; @Override public Video getVideo() &#123; return new PythonVideo(); &#125;&#125; 测试类 1234567891011public class Test &#123; public static void main(String[] args) &#123; VideoFactory javaVideoFactory = new JavaVideoFactory(); VideoFactory pythonVideoFactory = new PythonVideoFactory(); Video video = javaVideoFactory.getVideo(); video.produce(); video = pythonVideoFactory.getVideo(); video.produce(); &#125;&#125; 控制台输出 12录制Java课程录制Python视频 如果我们现在新增一个产品–前端课程，我们需要创建产品类，产品工厂类，但是无需改动其他代码，做到了对扩展开放，对修改关闭，符合开闭原则。 123456789101112public class FEVideo extends Video &#123; @Override public void produce() &#123; System.out.println(&quot;录制前端课程&quot;); &#125;&#125;public class FEVideoFactory extends VideoFactory &#123; @Override public Video getVideo() &#123; return new FEVideo(); &#125;&#125; 但是，我们也不难看出工厂方法模式的缺点–类的个数容易过多，增加复杂度。 因为一旦我们需要现在产品，就需要创建产品对应的产品实现类，以及产品工厂方法类，无疑增加了类的个数和系统的复杂度。 完整的UML类图 源码解析Collection源码jdk中典型的工厂方法模式体现为java.util.Collection 抽象产品为java.util.Iterator 123public interface Iterator&lt;E&gt; &#123; ...&#125; 抽象工厂定义了创建产品族的方法java.util.Collection.#iterator 1Iterator&lt;E&gt; iterator(); 由子类来定义具体创建产品的逻辑，如java.util.ArrayList.#iterator 123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; 而具体的产品定义为java.util.ArrayList$Itr 123private class Itr implements Iterator&lt;E&gt; &#123; ...&#125; UML类图 URLStreamHandlerFactory源码再来看一个典型例子，java.net.URLStreamHandlerFactory作为工厂方法抽象类，定义了创建产品的抽象方法 123public interface URLStreamHandlerFactory &#123; URLStreamHandler createURLStreamHandler(String protocol);&#125; 产品抽象类就是java.net.URLStreamHandler 123public abstract class URLStreamHandler &#123; ...&#125; 产品的工厂方法实现类为sun.misc.Launcher$Factory 123456789101112131415161718192021private static class Factory implements URLStreamHandlerFactory &#123; ... public URLStreamHandler createURLStreamHandler(String var1) &#123; private static String PREFIX = &quot;sun.net.www.protocol&quot;; private Factory() &#123; &#125; public URLStreamHandler createURLStreamHandler(String var1) &#123; String var2 = PREFIX + &quot;.&quot; + var1 + &quot;.Handler&quot;; try &#123; // 通过反射创建指定类型的产品 Class var3 = Class.forName(var2); return (URLStreamHandler)var3.newInstance(); &#125; catch (ReflectiveOperationException var4) &#123; throw new InternalError(&quot;could not load &quot; + var1 + &quot;system protocol handler&quot;， var4); &#125; &#125; &#125;&#125; 可以发现，工厂实现类通过反射类创建具体的产品实现类，而产品实现类非常多 这样满足了开闭原则，也没有过多的增加类的数量，值得我们学习。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://gcdd1993.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Java设计模式（三）简单工厂模式","date":"2019-07-28T15:14:04.000Z","path":"p/21710/","text":"定义与类型 定义:由一个工厂对象决定创建出哪一种产品类的实例 类型:创建型，但不属于GOF23种设计模式 适用场景 工厂类负责创建的对象比较少 客户端(应用层)只知道传入工厂类的参数，对于如何创建对象(逻辑)不关心 优点只需要传入一个正确的参数，就可以获取你所需要的对象，而无须知道其创建细节 缺点工厂类的职责相对过重，增加新的产品，需要修改工厂类的判断逻辑，违背开闭原则 Coding创建一个抽象产品类 123public abstract class Video &#123; public abstract void produce();&#125; 产品实现类 123456789101112public class JavaVideo extends Video &#123; @Override public void produce() &#123; System.out.println(&quot;录制Java课程&quot;); &#125;&#125;public class PythonVideo extends Video &#123; @Override public void produce() &#123; System.out.println(&quot;录制Python视频&quot;); &#125;&#125; 创建产品对应的简单工厂，通过产品类型来创建产品，应用方无需知道创建产品的细节 123456789101112public class VideoFactory &#123; public Video getVideo(String type) &#123; if (&quot;java&quot;.equalsIgnoreCase(type)) &#123; return new JavaVideo(); &#125; else if (&quot;python&quot;.equalsIgnoreCase(type)) &#123; return new PythonVideo(); &#125; else &#123; return null; &#125; &#125;&#125; 测试类 1234567public class Test &#123; public static void main(String[] args) &#123; VideoFactory videoFactory = new VideoFactory(); Video video = videoFactory.getVideo(&quot;Java&quot;); video.produce(); &#125;&#125; 控制台输出 1录制Java课程 如果增加产品，我们不仅需要修改产品对应的产品类，还需要修改工厂类，违反了开闭原则。 我们可以通过反射来优化下我们的工厂类 1234567891011public class VideoFactory &#123; public Video getVideo(Class&lt;? extends Video&gt; clazz) &#123; try &#123; return clazz.newInstance(); &#125; catch (InstantiationException | IllegalAccessException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 这样一来，添加产品的时候不用再修改我们的工厂类，而是直接添加产品即可。 最终的UML类图 源码解析JDK源码在JDK中，使用简单工厂模式的例子如java.util.Calendar，一组getInstance的重载方法，提供了创建Calendar产品的简单工厂方法。 1234public static Calendar getInstance()public static Calendar getInstance(TimeZone zone)public static Calendar getInstance(Locale aLocale)public static Calendar getInstance(TimeZone zone，Locale aLocale) 核心方法为 1private static Calendar createCalendar(TimeZone zone，Locale aLocale) 源码较长，不贴了，有兴趣的可以去看下源码。 Calendar的UML类图如下 Logback源码logback类中的简单工厂模式主要体现在ch.qos.logback.classic.LoggerContext#getLogger(String) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Overridepublic final Logger getLogger(final String name) &#123; if (name == null) &#123; throw new IllegalArgumentException(&quot;name argument cannot be null&quot;); &#125; // 判断log类型返回root节点的logger if (Logger.ROOT_LOGGER_NAME.equalsIgnoreCase(name)) &#123; return root; &#125; int i = 0; Logger logger = root; // 如果缓存中已经存在的指定的logger，直接返回childLogger Logger childLogger = (Logger) loggerCache.get(name); // if we have the child， then let us return it without wasting time if (childLogger != null) &#123; return childLogger; &#125; // 以下是创建logger的逻辑 String childName; while (true) &#123; int h = LoggerNameUtil.getSeparatorIndexOf(name， i); if (h == -1) &#123; childName = name; &#125; else &#123; childName = name.substring(0， h); &#125; // move i left of the last point i = h + 1; synchronized (logger) &#123; childLogger = logger.getChildByName(childName); if (childLogger == null) &#123; childLogger = logger.createChildByName(childName); loggerCache.put(childName， childLogger); incSize(); &#125; &#125; logger = childLogger; if (h == -1) &#123; return childLogger; &#125; &#125;&#125; 是一个典型的简单工厂方法","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://gcdd1993.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Java设计模式（二）设计模式原则","date":"2019-07-27T17:34:12.000Z","path":"p/64818/","text":"学习Java设计模式之前，有必要先了解设计模式原则。 开闭原则定义 一个软件实体如类、模块和函数应该对扩展开放，对修改关闭 用抽象构建框架，用实现扩展细节 优点：提高软件系统的可复用性及可维护性 Coding创建接口 1234567public interface ICourse &#123; Integer getId(); String getName(); Double getPrice();&#125; 创建实现类 12345678910111213@ToString@AllArgsConstructorpublic class JavaCourse implements ICourse &#123; @Getter private Integer id; @Getter private String name; @Getter private Double price;&#125; 测试类 1234567public class Test &#123; public static void main(String[] args) &#123; ICourse iCourse = new JavaCourse(96， &quot;我的Java课程&quot;， 348d); System.out.println(&quot;课程ID: &quot; + iCourse.getId() + &quot; 课程名称： &quot; + iCourse.getName() + &quot;课程价格： &quot; + iCourse.getPrice()); &#125;&#125; 控制台输出 1课程ID: 96 课程名称： 我的Java课程课程价格： 348.0 如果现在要打折出售课程，按照开闭原则来设计，对扩展开放，对修改关闭。 创建打折类 1234567891011121314public class JavaDiscountCourse extends JavaCourse &#123; public JavaDiscountCourse(Integer id， String name， Double price) &#123; super(id， name， price); &#125; public Double getOriginPrice() &#123; return super.getPrice(); &#125; @Override public Double getPrice() &#123; return super.getPrice() * 0.8; &#125;&#125; 修改应用类 123456789public class Test &#123; public static void main(String[] args) &#123; ICourse javaCourse = new JavaDiscountCourse(96， &quot;我的Java课程&quot;， 348d); JavaDiscountCourse iCourse = (JavaDiscountCourse) javaCourse; System.out.println(&quot;课程ID: &quot; + iCourse.getId() + &quot; 课程名称： &quot; + iCourse.getName() + &quot;课程原价： &quot; + iCourse.getOriginPrice() + &quot; 课程折后价格： &quot; + iCourse.getPrice()); &#125;&#125; 控制台输出 1课程ID: 96 课程名称： 我的Java课程课程原价： 348.0 课程折后价格： 278.40000000000003 这里有个要注意的地方，Double * 0.8后输出的浮点数精度有丢失的情况，可以使用BigDecimal的String构造器public BigDecimal(String val)来解决。 修改JavaDiscountCourse 1234567891011121314public class JavaDiscountCourse extends JavaCourse &#123; public JavaDiscountCourse(Integer id， String name， Double price) &#123; super(id， name， price); &#125; public Double getOriginPrice() &#123; return super.getPrice(); &#125; @Override public Double getPrice() &#123; return new BigDecimal(super.getPrice().toString()).multiply(new BigDecimal(&quot;0.8&quot;)).doubleValue(); &#125;&#125; 控制台输出 1课程ID: 96 课程名称： 我的Java课程课程原价： 348.0 课程折后价格： 278.4 依赖倒置原则定义 高层模块不应该依赖低层模块，二者都应该依赖其抽象 抽象不应该依赖细节;细节应该依赖抽象 针对接口编程，不要针对实现编程 优点:可以减少类间的耦合性、提高系统稳定性，提高代码可读性和可维护性，可降低修改程序所造成的风险 Coding反例创建类 12345678910111213public class Geely &#123; public void studyJavaCourse() &#123; System.out.println(&quot;Geely在学习Java课程&quot;); &#125; public void studyFECourse() &#123; System.out.println(&quot;Geely在学习FE课程&quot;); &#125; public void studyPythonCourse() &#123; System.out.println(&quot;Geely在学习Python课程&quot;); &#125;&#125; 测试类 12345678public class Test &#123; // v1 public static void main(String[] args) &#123; Geely geely = new Geely(); geely.studyFECourse(); geely.studyJavaCourse(); &#125;&#125; 控制台输出 12Geely在学习FE课程Geely在学习Java课程 这时候，如果我们要让Geely学习Ruby课程，我们只能在Geely类中添加 123public void studyRubyCourse() &#123; System.out.println(&quot;Geely在学习Ruby课程&quot;);&#125; 然后，在Test类中添加 1geely.studyRubyCourse(); 不符合依赖倒置原则 正例创建接口 123public interface ICourse &#123; void studyCourse();&#125; 创建类，带有成员变量ICourse course 1234567891011@AllArgsConstructorpublic class Geely &#123; @Setter private ICourse course; public void studyImoocCourse() &#123; course.studyCourse(); &#125;&#125; 创建实现类 123456789101112131415161718public class FECourse implements ICourse &#123; @Override public void studyCourse() &#123; System.out.println(&quot;Geely在学习FE课程&quot;); &#125;&#125;public class JavaCourse implements ICourse &#123; @Override public void studyCourse() &#123; System.out.println(&quot;Geely在学习Java课程&quot;); &#125;&#125;public class PythonCourse implements ICourse &#123; @Override public void studyCourse() &#123; System.out.println(&quot;Geely在学习Python课程&quot;); &#125;&#125; 测试类 123456789public class Test &#123; public static void main(String[] args) &#123; Geely geely = new Geely(new JavaCourse()); geely.studyImoocCourse(); geely.setCourse(new FECourse()); geely.studyImoocCourse(); &#125;&#125; 控制台输出 12Geely在学习Java课程Geely在学习FE课程 这样一来，如果要添加新的课程，只需要创建实现类即可。然后应用类设置实现类，无需改动其他代码，符合依赖倒置原则。 单一职责原则定义 不要存在多于一个导致类变更的原因 一个类/接口/方法只负责一项职责 优点:降低类的复杂度、提高类的可读性、提高系统的可维护性、降低变更引起的风险 Coding反例创建类 12345public class Bird &#123; public void mainMoveMode(String birdName) &#123; System.out.println(birdName + &quot; 用翅膀飞&quot;); &#125;&#125; 测试类 12345678public class Test &#123; public static void main(String[] args) &#123; Bird bird = new Bird(); bird.mainMoveMode(&quot;大雁&quot;); bird.mainMoveMode(&quot;鸵鸟&quot;); &#125;&#125; 控制台输出 12大雁 用翅膀飞鸵鸟 用翅膀飞 鸵鸟是用脚走的，所以我们更改Bird类 123456789public class Bird &#123; public void mainMoveMode(String birdName) &#123; if (&quot;鸵鸟&quot;.equals(birdName)) &#123; System.out.println(birdName + &quot; 用脚走&quot;); &#125; else &#123; System.out.println(birdName + &quot; 用翅膀飞&quot;); &#125; &#125;&#125; 如果有更多的鸟类，我们还要写更多的else代码。 正例我们修改下反例中的例子 12345678910public class FlyBird &#123; public void mainMoveMode(String birdName) &#123; System.out.println(birdName + &quot; 用翅膀飞&quot;); &#125;&#125;public class WalkBird &#123; public void mainMoveMode(String birdName) &#123; System.out.println(birdName + &quot; 用脚走&quot;); &#125;&#125; 添加测试类 12345678910public class Test &#123; public static void main(String[] args) &#123; FlyBird flyBird = new FlyBird(); flyBird.mainMoveMode(&quot;大雁&quot;); WalkBird walkBird = new WalkBird(); walkBird.mainMoveMode(&quot;鸵鸟&quot;); &#125;&#125; 控制台输出 12大雁 用翅膀飞鸵鸟 用脚走 再举一个例子 创建接口 123456789101112131415161718192021222324/** * 课程内容 * * @author gaochen * Created on 2019/7/27. */public interface ICourseContent &#123; String getCoursName(); byte[] getCourseVideo();&#125;/** * 课程管理 * * @author gaochen * Created on 2019/7/27. */public interface ICourseManager &#123; void studyCourse(); void refundCourse();&#125; 创建实现类，有着课程内容和课程管理两种职能 123456789101112131415161718192021public class CourseImpl implements ICourseContent， ICourseManager &#123; @Override public String getCoursName() &#123; return null; &#125; @Override public byte[] getCourseVideo() &#123; return new byte[0]; &#125; @Override public void studyCourse() &#123; &#125; @Override public void refundCourse() &#123; &#125;&#125; 接口隔离原则定义 用多个专门的接口，而不使用单一的总接口，客户端不应该依赖它不需要的接口 一个类对一个类的依赖应该建立在最小的接口上 建立单一接口，不要建立庞大臃肿的接口 尽量细化接口，接口中的方法尽量少 优点:符合我们常说的高内聚低耦合的设计思想，从而使得类具有很好的可读性、可扩展性和可维护性。 注意适度原则，一定要适度 Coding反例创建接口 12345678public interface IAnimalAction &#123; void eat(); void fly(); void swim();&#125; 创建实现类 1234567891011121314151617181920212223242526272829303132public class Bird implements IAnimalAction &#123; @Override public void eat() &#123; System.out.println(&quot;鸟 吃饭&quot;); &#125; @Override public void fly() &#123; System.out.println(&quot;鸟 飞&quot;); &#125; @Override public void swim() &#123; // 鸟不会游泳，空实现 &#125;&#125;public class Dog implements IAnimalAction &#123; @Override public void eat() &#123; System.out.println(&quot;狗 吃饭&quot;); &#125; @Override public void fly() &#123; // 狗不会飞，空实现 &#125; @Override public void swim() &#123; System.out.println(&quot;狗 游泳&quot;); &#125;&#125; 我们可以看出，鸟和狗实现了接口后，各自都有无用的接口，所以违反了接口隔离原则，只能采取空实现的方式。但是对于使用方来说，还是可以调用狗的fly方法，得到空的实现。 正例将反例中的接口接口拆分为三个独立的接口 123456789public interface IEatAnimalAction &#123; void eat();&#125;public interface IFlyAnimalAction &#123; void fly();&#125;public interface ISwimAnimalAction &#123; void swim();&#125; Dog改为 1234567891011public class Dog implements IEatAnimalAction，ISwimAnimalAction &#123; @Override public void eat() &#123; System.out.println(&quot;狗 吃饭&quot;); &#125; @Override public void swim() &#123; System.out.println(&quot;狗 游泳&quot;); &#125;&#125; Bird改为 1234567891011public class Bird implements IEatAnimalAction，IFlyAnimalAction &#123; @Override public void eat() &#123; System.out.println(&quot;鸟 吃饭&quot;); &#125; @Override public void fly() &#123; System.out.println(&quot;鸟 飞&quot;); &#125;&#125; 这样就成功的将一个大接口，优化为分摊职责的小接口，实现类可以根据需要实现多个职能接口。 迪米特原则定义 一个对象应该对其他对象保持最少的了解。又叫最少知道原则 尽量降低类与类之间的耦合 优点:降低类之间的耦合 Coding反例创建课程类 12public class Course &#123;&#125; 创建项目经理类 123456public class TeamLeader &#123; public void checkNumberOfCourse(List&lt;Course&gt; courseList) &#123; System.out.println(&quot;在线课程的数量是 ：&quot; + courseList.size()); &#125;&#125; 创建老板类 12345678910public class Boss &#123; public void commandCheckNumber(TeamLeader teamLeader) &#123; List&lt;Course&gt; courseList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) &#123; courseList.add(new Course()); &#125; teamLeader.checkNumberOfCourse(courseList); &#125;&#125; 测试类 1在线课程的数量是 ：20 我们仔细分析一下，其实老板并不需要知道课程的细节，只需要问一下项目经理，有多少课程，项目经理直接告诉老板有20节在线课程。而不是老板将课程列出，让项目经理统计。 我们看下UML类图 正例项目经理类修改为 12345678910public class TeamLeader &#123; public void checkNumberOfCourse() &#123; List&lt;Course&gt; courseList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) &#123; courseList.add(new Course()); &#125; System.out.println(&quot;在线课程的数量是 ：&quot; + courseList.size()); &#125;&#125; 老板类 123456public class Boss &#123; public void commandCheckNumber(TeamLeader teamLeader) &#123; teamLeader.checkNumberOfCourse(); &#125;&#125; 这时候运行一下，结果一样。但是从UML类图上来看，是有很大的优化的。","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://gcdd1993.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"UML","slug":"UML","permalink":"https://gcdd1993.github.io/tags/UML/"}]},{"title":"Java设计模式（一）UML总结","date":"2019-07-27T16:02:06.000Z","path":"p/63230/","text":"定义统一建模语言(英语: Unified Modeling Language ，缩写UML)是非专利的第三代建模和规约语言。 UML特点 UML是一种开放的方法 用于说明、可视化、构建和编写一个正在开发的面向对象的、软件密集系统的制品的开放方法。 UML展现了一系列最佳工程实践，这些最佳实践在对大规模，复杂系统进行建模方面，特别是在软件架构层次已经被验证有效。 UML2.2分类UML2.2中一共定义了14种图示，分类如下: 结构式图形：强调的是系统式的建模 静态图(类图，对象图，包图) 实现图(组件图，部署图) 剖面图 复合结构图 行为式图形：强调系统模型中触发的事件 活动图 状态图 用例图 交互式图形：属于行为式图形子集合，强调系统模型中的资料流程 通信图 交互概述图(UML2.0) 时序图(UML2.0) 时间图(UML2.0) UML类图定义Class Diagram:用于表示类、接口、实例等之间相互的静态关系。虽然名字叫类图，但类图中并不只有类（也包括权限，属性，方法等）。 记忆技巧箭头方向 定义子类时需要通过extends关键字指定父类 子类一定是知道父类定义的，但父类并不知道子类的定义 只有知道对方信息时才能指向对方 所以箭头方向是从子类指向父类 实线-继承|虚线-实现 空心三角箭头:继承或实现 实线继承， is a关系，扩展目的，不虚，很结实 虚线-实现，虚线代表“虚”，无实体 实线-关联|虚线-依赖 实线-关联关系:关系稳定，实打实的关系，铁哥们 表示一个类对象和另一个类对象有关联 通常是一个类中有另一个类对象做为属性 虚线-依赖关系:临时用一下，若即若离，虚无缥缈，若有若无 表示一种使用关系，一个类需要借助另一个类来实现功能 一般是一个类使用另一个类做为参数使用，或作为返回值 空心菱形-聚合|实心菱形-组合 菱形就是一个盛东西的器皿(例如盘子) 聚合:代表空器皿里可以放很多相同东西，聚在一起(箭头方向所指的类) 组合:代表满器皿里已经有实体结构的存在，生死与共 整体和局部的关系，两者有着独立的生命周期，是has a的关系 弱关系 消极的词:弱-空 整体与局部的关系，和聚合的关系相比，关系更加强烈 两者有相同的生命周期， contains-a的关系 强关系 积极的词:强-满 常见数字表达及含义，假设有A类和B类，数字标记在A类侧 0..1:0或1个实例. 0..*:0或多个实例. 1..1:1个实例. 1:只能有一个实例. 1..*:至少有一个实例. 类图详解 类图从上到下包含： 类名：抽象类使用斜体表示，接口用&lt;&gt;表示 属性：访问权限+属性名：属性类型 +：public -：private #：protected ~：default 下横线表示static 方法: 访问权限+方法名：返回值类型 +：public -：private #：protected ~：default 下横线表示static 斜体表示抽象方法 典型的类图表示： UML时序图Sequence Diagram :是显示对象之间交互的图，这些对象是按时间顺序排列的。 时序图中包括的建模元素主要有: 对象(Actor) 生命线(Lifeline) 控制焦点(Focus of control) 消息(Message)等 典型的一个时序图如下：","tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://gcdd1993.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"UML","slug":"UML","permalink":"https://gcdd1993.github.io/tags/UML/"}]},{"title":"Spring Mvc Http 400 Bad Request问题排查","date":"2019-07-26T10:32:46.000Z","path":"p/6604/","text":"如果遇到了Spring MVC报错400，而且没有返回任何信息的情况下该如何排查问题？ 问题描述一直都没毛病的接口，今天测试的时候突然报错400 Bad Request，而且Response没有返回任何信息。 解决方案尝试了一下午，终于找到了排查这类问题的办法。 我们知道，在Spring MVC里面，org.springframework.web.servlet.mvc.method.annotation.ResponseEntityExceptionHandler负责所有异常的统一处理。我们只要在方法handleException打上断点即可。 点开发现，原来是Lombok的问题。报错如下 12Could not read JSON document: Can not construct instance of xxx: no suitable constructor found, can not deserialize from Object value (missing default constructor or creator, or perhaps need to add/enable type information?) at [Source: java.io.PushbackInputStream@12544acd; line: 2, column: 5] Lombok没有为我们自动生成类的构造函数。我们在目标类加上@NoArgsConstructor即可解决。 刨根问底为什么Lombok自动生成的类，没有可供Jackson反序列化的构造函数呢？我看了一下生成的字节码文件，里面确实不存在无参构造和全参构造函数，唯一的构造函数是带一个参数的。 目标类使用了@Data注解，而@Data注解的声明如下 1234567891011121314151617181920/** * Generates getters for all fields, a useful toString method, and hashCode and equals implementations that check * all non-transient fields. Will also generate setters for all non-final fields, as well as a constructor. * &lt;p&gt; * Equivalent to &#123;@code @Getter @Setter @RequiredArgsConstructor @ToString @EqualsAndHashCode&#125;. * &lt;p&gt; * Complete documentation is found at &lt;a href=&quot;https://projectlombok.org/features/Data&quot;&gt;the project lombok features page for &amp;#64;Data&lt;/a&gt;. * * @see Getter * @see Setter * @see RequiredArgsConstructor * @see ToString * @see EqualsAndHashCode * @see lombok.Value */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface Data &#123; String staticConstructor() default &quot;&quot;;&#125; 简单来说，@Data包含了以下注解的功能 @Getter @Setter @RequiredArgsConstructor @ToString @EqualsAndHashCode 而“罪魁祸首”就是@RequiredArgsConstructor了，它的作用是 为每个需要特殊处理的字段（final修饰的或者是@NotNull注释的字段）生成一个带有1个参数的构造函数。 而目标类恰巧有一个字段就是@NotNull注解修饰的，所以生成了单参构造函数。 参考 Lombok Features Spring MVC自定义全局异常处理","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Nginx配置Https指南","date":"2019-07-24T08:42:05.000Z","path":"p/17023/","text":"前言本文是对Nginx配置SSL证书的总结。 申请SSL证书 你可以从任何证书提供商处申请证书，这里以阿里云为例。 打开阿里云SSL证书控制台，点击购买证书 选择免费型一年期的证书，点击立即购买注意，1年到期后别忘记重新申请证书！ 支付放心大胆的支付吧，不用钱！ 验证SSL证书购买完成之后，返回SSL证书控制台，你应该会看到刚才购买的证书。我们点击申请 填写域名（必须是你自己的或者有管理权的域名）和相关信息，完成后点击下一步。注意，免费型证书只支持单个域名！例如你要为www.example.com申请证书，你必须填写www.example.com，而不能是example.com。 在DNS服务商处配置阿里云提供的验证信息。例如DNSPod，填写主机记录，记录值和记录类型，然后点击保存。 耐心等待TTL刷新（一般为10分钟，也可能花不了10分钟）。 回到阿里云SSL证书申请页面，点击验证。 签发域名验证通过后，证书提供商将会为你的域名颁发证书。在阿里云SSL证书控制台的已签发列表下可以找到你的域名对应的SSL证书。 下载证书下载Nginx对应的SSL证书xx_nginx.zip，准备配置Nginx。 配置Nginx如果你还没有安装Nginx，可以参考部署Nginx 上传证书1234567891011$ sudo mkdir /etc/nginx/certs$ sudo cd /etc/nginx/certs## 上传你的证书至此目录$ sudo ls -ldrwxr-xr-x 2 root root 4096 Jul 24 17:15 ./drwxr-xr-x 7 root root 4096 Jul 24 17:15 ../-rw-r--r-- 1 root root 4053 Jul 24 16:49 xx_nginx.zip$ sudo unzip xx_nginx.zip$ sudo ls -l-rw-r--r-- 1 root root 1679 Jul 24 16:48 xx.key ## ssl cert key-rw-r--r-- 1 root root 3667 Jul 24 16:48 xx.pem ## ssl cert 一切准备就绪后，可以开始修改我们的Nginx配置文件了。 修改Nginx配置文件将Http修改为Https非常简单，只需要修改一处内容，并添加若干代码。 将listen 80;修改为listen 443; 在server块中添加以下代码 12345ssl on;ssl_certificate certs/xx.pem;ssl_certificate_key certs/xx.key;ssl_session_timeout 5m; 修改完成后，重启Nginx 12$ sudo service nginx reload$ sudo service nginx restart 好了，使用Https访问你的网站吧。 Http强制转向Https注意，以上修改完成后，只能使用Https访问了，但是往往我们不希望用户使用Http访问的时候出现404的情况。那么，我们可以简单的将80端口的用户转发到443端口，来达到Http和Https共存的状态。 在Nginx配置文件中添加 123456server &#123; listen 80; server_name xx.xx.com; return 301 https://$server_name$request_uri;&#125; 重启Nginx","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://gcdd1993.github.io/tags/Nginx/"}]},{"title":"使用Hyper-V替代VMware","date":"2019-07-19T05:52:46.000Z","path":"p/14834/","text":"Hyper-V是什么 Hyper-V硬件要求为Windows 10 企业版、专业版或教育版，如果你使用的是Mac或者Linux的电脑，可以不往下看了。 虚拟机大家都懂吧，简单来说，Hyper-V就是虚拟机管理工具。如果你使用过VMware Workstation Pro或者是VirtualBox，那你一定不陌生了。 具体来说，Hyper-V 提供硬件虚拟化。 这意味着每个虚拟机都在虚拟硬件上运行。 Hyper-V 允许你创建虚拟硬盘驱动器、虚拟交换机以及许多其他虚拟设备，所有这些都可以添加到虚拟机中。 为什么要使用Hyper-V而不是VMware？首先为什么要使用虚拟机？ 运行需要早期版本的Windows 操作系统或非Windows 操作系统的软件。 实验其他操作系统。 通过虚拟机，可轻松创建和删除不同的操作系统。 使用多个虚拟机在多个操作系统上测试软件。 通过虚拟机，可以在一部台式机或便携式计算机上运行所有内容。 那么，为什么要使用Hyper-V？ 首先，Hyper-V是Windows 10 专业版自带的功能，无需安装其他任何工具 Docker for Windows推荐使用Hyper-V作为虚拟化方案 免费 所以，在Hyper-V能胜任的场景下，我们应该使用Hyper-V。 如何使用Hyper-V检查系统要求 Windows 10 企业版、专业版或教育版。 具有二级地址转换 (SLAT) 的 64 位处理器。 虚拟机监视器模式扩展的 CPU 支持 (Intel Cpu 上的 VT-c)。 最小 4 GB 内存。 注意：系统必须是Windows 10企业版、专业版或教育版。 开启Hyper-V使用 PowerShell 启用 Hyper-V 以管理员身份打开 PowerShell 控制台。 运行以下命令： 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All 如果无法找到此命令，请确保你以管理员身份运行 PowerShell。 安装完成后，请重启。 使用 CMD 和 DISM 启用 Hyper-V部署映像服务和管理工具 (DISM) 可帮助配置 Windows 和 Windows 映像。 在众多应用程序中，DISM 可以在操作系统运行时启用 Windows 功能。 使用 DISM 启用 Hyper-V 角色： 以管理员身份打开 PowerShell 或 CMD 会话。 键入下列命令： 1DISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V 通过“设置”启用 Hyper-V 角色推荐使用这种方式 右键单击 Windows 按钮并选择“应用和功能”。 在 “相关设置” 下的右侧选择 “程序和功能“。 选择“打开或关闭 Windows 功能”。 选择 Hyper-V，然后单击确定。 同样的，安装完成后，请重启。 创建虚拟机在开始菜单找到并打开Hyper-V管理器，它应该位于Windows管理工具文件夹下面。 或者直接搜索Hyper-V 打开后界面如下，我觉得比VMware界面好看点。 快速创建点击快速创建，你将会看到 类似于在线安装，比较简单。 我尝试了导入本地安装源安装Ubuntu 16.04，但是启动报错，找不到Boot信息。 可能原因是：我的电脑不支持第二代虚拟机世代（是一种较新的虚拟化功能） 新建虚拟机点击新建虚拟机，你将会进入一下界面 跟着一步步来吧，首先你得准备好一个系统镜像（ISO结尾的系统镜像文件） 点击下一步，完成。 接着，就进入了Ubuntu系统安装环节，省略了，大家应该都会装的。 导入虚拟机除了自己创建，我们还可以导入别人创建好的虚拟机 点击导入虚拟机 以下是我创建的Ubuntu 16.04虚拟机，你可以直接导入使用。 https://1drv.ms/f/s!AjfBPvEeW2r2hukqwAdOrPSMPpKZ4A 参考Windows 10 上的 Hyper-V 简介","tags":[{"name":"Hyper-V","slug":"Hyper-V","permalink":"https://gcdd1993.github.io/tags/Hyper-V/"}]},{"title":"Ubuntu 搭建phpcms","date":"2019-06-24T09:50:42.000Z","path":"p/5357/","text":"安装Apache2123$ sudo apt-get update -y$ sudo apt-get install apache2 -y$ sudo systemctl start apache2.service 安装Mysql12345678$ sudo apt-get install mysql-server -y$ sudo /usr/bin/mysql_secure_installation## 都选y就行$ mysql -u root -p mysql&gt; CREATE DATABASE js_website;## 导入数据mysql&gt; source /tmp/jskj.sql;mysql&gt; \\q; 安装PHP12$ sudo apt-get install php -y;$ sudo apt-get install -y php-&#123;bcmath,bz2,intl,gd,mbstring,mcrypt,mysql,zip&#125; &amp;&amp; sudo apt-get install libapache2-mod-php -y; 部署PHP官网123456789101112131415161718192021222324252627$ mkdir /var/www/html/phpcms$ cd /var/www/html/phpcms# 上传phpcms.zip包至此目录$ unzip phpcms.zip$ ls -ldrwxr-xr-x 11 root root 4096 Jun 24 17:21 ./drwxr-xr-x 3 root root 4096 Jun 24 17:21 ../-rw-r--r-- 1 root root 48 Jun 24 15:53 admin.phpdrwxr-xr-x 3 root root 4096 Jun 24 15:53 api/-rw-r--r-- 1 root root 991 Jun 24 15:53 api.phpdrwxr-xr-x 18 root root 4096 Jun 24 15:53 caches/-rw-r--r-- 1 root root 104 Jun 24 15:53 crossdomain.xmldrwxr-xr-x 6 root root 4096 Jun 24 15:53 custom/-rw-r--r-- 1 root root 3158 Jun 24 15:53 favicon.icodrwxr-xr-x 2 root root 4096 Jun 24 15:53 html/-rw-r--r-- 1 root root 4444 Jun 24 15:53 index.htm-rw-r--r-- 1 root root 22758 Jun 24 15:53 index.html-rw-r--r-- 1 root root 318 Jun 24 15:53 index.php-rw-r--r-- 1 root root 523 Jun 24 15:53 js.htmldrwxr-xr-x 8 root root 4096 Jun 24 15:53 mes/drwxr-xr-x 8 root root 4096 Jun 24 15:53 phpcms/-rw-r--r-- 1 root root 168191200 Jun 24 16:38 phpcms.zipdrwxr-xr-x 7 root root 4096 Jun 24 15:53 phpsso_server/-rw-r--r-- 1 root root 3621 Jun 24 15:53 plugin.php-rw-r--r-- 1 root root 170 Jun 24 15:53 robots.txtdrwxr-xr-x 6 root root 4096 Jun 24 15:53 statics/drwxr-xr-x 4 root root 4096 Jun 24 15:53 uploadfile/ 👉这里的zip压缩包，是已经install后的phpcms，因为项目经理给我的就是安装好的，所以就直接用了。 反正原理都一样，配置Apache解析域名指向路径就行。 配置Apache1234567891011121314$ cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/phpcms.conf$ cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/phpcms-mes.conf$ ln -s /etc/apache2/sites-available/phpcms.conf /etc/apache2/sites-enabled/phpcms.conf$ ln -s /etc/apache2/sites-available/phpcms-mes.conf /etc/apache2/sites-enabled/phpcms-mes.conf$ vim /etc/apache2/sites-available/phpcms.confServerName js.dbpe-cps.com# ServerAdmin webmaster@localhostDocumentRoot /var/www/html/phpcms$ vim /etc/apache2/sites-available/phpcms-mes.confServerName mes.js.dbpe-cps.com# ServerAdmin webmaster@localhostDocumentRoot /var/www/html/phpcms/mes$ service apache2 restart 域名解析配置你的域名指向你的服务器就行。这里略过。 其他Apache常用命令1234## 重启Apache2$ service apache2 restart $ service apache2 status$ service apache2 start Apache目录 配置目录：/etc/apache2 默认www目录：/var/www/html 这一点跟其他的不一样，我也是看到配置文件才知道是这个目录的 /etc/apache2/apache2.conf","tags":[{"name":"phpcms","slug":"phpcms","permalink":"https://gcdd1993.github.io/tags/phpcms/"}]},{"title":"发布开源项目到Jcenter","date":"2019-06-10T08:44:17.000Z","path":"p/53809/","text":"前言 为了将阿里云短信开箱即用发布到Jcenter仓库，前前后后花费了1天半的时间，把端午节都搭进去了。终于今天收到了Jcenter的消息，自己发布的包被添加到了Jcenter仓库，也算给开源社区做了次小贡献😁😁😁。 现在记录下踩过的坑。 注册Jcenter账号要注意的地方，Jcenter账号跟国内一样分为社区版和企业版，企业版当然是要付费的，而且很坑的是点进Bintray官网，首先映入眼帘的就是大大的Start Your Free Trial（开始免费试用），一开始我就注册了企业版账号，后来删号重建了😂。我们应该点这里： 填写信息后注册，我是直接使用的Github账号注册。 创建Repository点击右上角View Profile 在账号信息下方，我们点击Add New Repository，创建新的仓库。 在填写信息的时候，选择Public（Private是需要付钱的，大家都懂），如果你是maven项目，仓库名最好填写maven，因为我在申请Add To Jcenter时，第一次失败了，要求我把项目放在maven路径下。 创建Package创建完仓库，就是创建包了，没什么好说的，你的应用叫啥名，包就叫啥名就行。 创建完可以看到包的基本信息： 打包上传这里使用的是开源项目bintray-release，官方文档bintray-release/wiki 主要在build.gradle里添加如下信息 1234567891011121314151617181920212223242526272829303132buildscript &#123; repositories &#123; jcenter() &#125; dependencies &#123; classpath &#x27;com.novoda:bintray-release:0.9.1&#x27; &#125;&#125;apply plugin: &#x27;com.novoda.bintray-release&#x27;publish &#123; userOrg = &#x27;你的Bintray用户名&#x27; groupId = &#x27;应用的groupId，例如：io.github.gcdd1993&#x27; artifactId = &#x27;应用的名称，例如：ali-sms-spring-boot-starter&#x27; publishVersion = &#x27;应用的版本号，例如：1.0.0.RELEASE&#x27; desc = &#x27;一句话概述你的应用干啥的&#x27; website = &#x27;应用链接，一般写github地址就行，例如：https://github.com/gcdd1993/ali-sms-spring-boot-starter&#x27;&#125;/** * 以下是我自己加的 * 第一个解决Gradle Task:jar skipped的问题 * 第二个解决javaDoc &#x27;UTF-8&#x27;乱码问题 */jar &#123; enabled = true&#125;tasks.withType(JavaCompile) &#123; options.encoding = &quot;UTF-8&quot;&#125; 接下来执行gradle命令： 1./gradlew bintrayUpload -PbintrayUser=BINTRAY_USERNAME -PbintrayKey=BINTRAY_KEY -PdryRun=false 本地测试可以把-PdryRun=false改为-PdryRun=true，这样就不会帮你上传到Bintray，其他的都执行。 看到以上信息，证明发布成功了。 Add To Jcenter发布成功后，你应该会在Package的Files标签下看到你上传的文件 我们点击右上角Actions下的Add To Jcenter 填写信息，两个复选框我都勾选了，然后填写Group Id，填上应用说明（最好用英文），然后等着就行了。 一般来说1~3天你将会收到一封邮件，通知你的申请通过没有，如下 👉如果没有通过，也会告诉你怎么改，所以不用担心。 这时候再打开Bintray的Package页面，会发现Included In Jcenter，证明已经被Jcenter收录了，其他人就可以正常使用啦。 Travis CI持续集成Travis CI是什么就不介绍了，不明白的可以看下阮一峰的网络日志-持续集成服务 Travis CI 教程，Github公开仓库免费的持续集成工具。 项目根目录添加.travis.yml，填入以下信息（针对Gradle搭建的Java项目适用） 123456789101112131415161718192021language: javasudo: requireddist: xenialjdk: - openjdk8branches: only: - masterbefore_cache: - rm -f $HOME/.gradle/caches/modules-2/modules-2.lock - rm -fr $HOME/.gradle/caches/*/plugin-resolution/cache: directories: - $HOME/.gradle/caches/ - $HOME/.gradle/wrapper/before_install: - chmod +x gradlewinstall: - ./gradlew jarscript: - ./gradlew bintrayUpload -PbintrayUser=$&#123;bintray_user&#125; -PbintrayKey=$&#123;bintray_key&#125; -PdryRun=false 其中变量$&#123;bintray_user&#125;和$&#123;bintray_key&#125;是Travis CI运行时环境变量，请到Travis CI Settings填写。 参考文档 bintray-release-wiki Maven Publish Plugin 阮一峰的网络日志-持续集成服务 Travis CI 教程","tags":[{"name":"Jcenter","slug":"Jcenter","permalink":"https://gcdd1993.github.io/tags/Jcenter/"}]},{"title":"阿里云短信开箱即用","date":"2019-06-07T15:52:16.000Z","path":"p/28056/","text":"简介 使用SpringBoot自动装配简化对接阿里云短信过程。 小工具一枚，欢迎使用和Star支持，如使用过程中碰到问题，可以提出Issue，我会尽力完善该Starter。 版本基础aliyun-java-sdk-core:4.1.0 如何使用Maven12345&lt;dependency&gt; &lt;groupId&gt;io.github.gcdd1993&lt;/groupId&gt; &lt;artifactId&gt;ali-sms-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; Gradle1compile &#x27;io.github.gcdd1993:ali-sms-spring-boot-starter:1.0.0.RELEASE&#x27; 👉注意：需要引入Jcenter仓库 参数配置以application.yml举例 12345678910ali: sms: domain: &quot;dysmsapi.aliyuncs.com&quot; ## 默认dysmsapi.aliyuncs.com version: &quot;2017-05-25&quot; ## 默认2017-05-25 action: &quot;SendSms&quot; ## 默认SendSms access-key: id: &quot;$&#123;阿里云短信AccessKeyId&#125;&quot; secret: &quot;$&#123;阿里云短信AccessKeySecret&#125;&quot; region-id: &quot;$&#123;阿里云短信地域&#125;&quot; sign-name: &quot;$&#123;阿里云短信签名&#125;&quot; ## 如果不填，必须在发送方法中指定 基本使用同步发送短信为了方便使用，接口上进行了方法的重载，提供5种不同的参数列表供选择，你可以自行选择使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 同步发送短信 * &lt;p&gt; * 参数1：使用的短信模板ID * 参数2：接收者的手机号，如&quot;17602526129,17602923211&quot; * 参数3：Map，key对应模板中的参数名，value对应值（这里是使用Jackson来序列化） * &lt;/p&gt; */@Testpublic void sendSync() &#123; SmsResponse smsResponse = sendService.sendSync(TEMPLATE_ID, PHONE_NUMBER, MAP); Assert.assertTrue(smsResponse.isSuccess());&#125;/** * 同步发送短信 * &lt;p&gt; * 参数1：使用的短信模板ID * 参数2：接收者的手机号，如&quot;17602526129,17602923211&quot; * 参数3：要发送的短信写入值，你可以自己进行json的拼装。注意要进行json的转义，例如：&quot;&#123;\\&quot;code\\&quot;:\\&quot;112233\\&quot;&#125;&quot; * &lt;/p&gt; */@Testpublic void sendSync1() &#123; SmsResponse smsResponse = sendService.sendSync(TEMPLATE_ID, PHONE_NUMBER, &quot;&#123;\\&quot;code\\&quot;:\\&quot;112233\\&quot;&#125;&quot;); Assert.assertTrue(smsResponse.isSuccess());&#125;/** * 同步发送短信 * &lt;p&gt; * 参数1：短信签名，适用于同一模板需要有不同短信签名的 * 参数2：使用的短信模板ID * 参数3：接收者的手机号，如&quot;17602526129,17602923211&quot; * 参数4：Map，key对应模板中的参数名，value对应值（这里是使用Jackson来序列化） * &lt;/p&gt; */@Testpublic void sendSync2() &#123; SmsResponse smsResponse = sendService.sendSync(SIGN_NAME, TEMPLATE_ID, PHONE_NUMBER, MAP); Assert.assertTrue(smsResponse.isSuccess());&#125;/** * 同步发送短信 * &lt;p&gt; * 参数1：短信签名，适用于同一模板需要有不同短信签名的 * 参数2：使用的短信模板ID * 参数3：接收者的手机号，如&quot;17602526129,17602923211&quot; * 参数4：要发送的短信写入值，你可以自己进行json的拼装。注意要进行json的转义，例如：&quot;&#123;\\&quot;code\\&quot;:\\&quot;112233\\&quot;&#125;&quot; * &lt;/p&gt; */@Testpublic void sendSync3() &#123; SmsResponse smsResponse = sendService.sendSync(SIGN_NAME, TEMPLATE_ID, PHONE_NUMBER, &quot;&#123;\\&quot;code\\&quot;:\\&quot;112233\\&quot;&#125;&quot;); Assert.assertTrue(smsResponse.isSuccess());&#125; 最后一个提供了一个参数对象来定义短信发送请求，如果不嫌麻烦，可以使用这个。 12345678910111213141516171819202122232425262728293031323334353637/** * 阿里云短信请求体 * * @author gaochen * @date 2019/6/6 */@Datapublic class SmsRequest &#123; /** * 接收短信的手机号码。以英文逗号（,）分隔。 */ private String phoneNumbers; /** * 短信签名名称。请在控制台签名管理页面签名名称一列查看。 */ private String signName; /** * 短信模板ID，前缀为SMS_ */ private Integer templateId; /** * 阿里云短信内容,key:短信模板中的字段名，value：短信模板字段对应值 * 使用此字段需要&#123;@link com.fasterxml.jackson.databind.ObjectMapper&#125; */ private Map&lt;String, String&gt; params; /** * json str of &#123;@link #getParams()&#125; * 使用此字段请设置params为Null */ private String paramStr;&#125; 使用： 123456789@Testpublic void sendSync4() &#123; SmsRequest smsRequest = new SmsRequest(); smsRequest.setPhoneNumbers(PHONE_NUMBER); smsRequest.setTemplateId(TEMPLATE_ID); smsRequest.setParams(MAP); SmsResponse smsResponse = sendService.sendSync(smsRequest); Assert.assertTrue(smsResponse.isSuccess());&#125; 异步发送短信 考虑到发短信的需求，一般来说都需要异步加持，对以上5种方法分别提供了异步接口sendAsync，使用方法基本一致，唯一不同的是，你可以异步处理短信发送返回值。 12345678CompletableFuture&lt;SmsResponse&gt; smsResponse = sendService.sendAsync(TEMPLATE_ID, PHONE_NUMBER, MAP);smsResponse.thenAcceptAsync(sr -&gt; &#123; if (sr.isSuccess()) &#123; System.out.println(&quot;发短信成功&quot;); &#125; else &#123; System.out.println(&quot;发送到消息队列，准备重试此次短信&quot;); &#125;&#125;); 高级使用除了使用以上方法发送短信外，你还可以使用官方的IAcsClient来发送短信，如 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package io.github.gcdd1993.demo;import com.aliyuncs.CommonRequest;import com.aliyuncs.IAcsClient;import com.aliyuncs.request;import com.aliyuncs.CommonResponse;import com.aliyuncs.exceptions.ClientException;import com.aliyuncs.http.MethodType;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import io.github.gcdd1993.alisms.domain.SmsRequest;import io.github.gcdd1993.alisms.domain.SmsResponse;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;/** * TODO * * @author gaochen * @date 2019/6/8 */@Servicepublic class SendService &#123; @Autowired private IAcsClient acsClient; public SmsResponse sendSync() &#123; try &#123; CommonRequest request = new CommonRequest(); request.setMethod(MethodType.POST); request.setDomain(&quot;dysmsapi.aliyuncs.com&quot;); request.setVersion(&quot;2017-05-25&quot;); request.setAction(&quot;SendSms&quot;); request.putQueryParameter(&quot;RegionId&quot;, &quot;region&quot;); request.putQueryParameter(&quot;PhoneNumbers&quot;, &quot;1771636783&quot;); request.putQueryParameter(&quot;SignName&quot;, &quot;SignName&quot;); request.putQueryParameter(&quot;TemplateCode&quot;, &quot;SMS_12345678&quot;); request.putQueryParameter(&quot;TemplateParam&quot;, &quot;&#123;\\&quot;code\\&quot;:\\&quot;112233\\&quot;&#125;&quot;); CommonResponse commonResponse = acsClient.getCommonResponse(request); return SmsResponse.SmsResponseBuilder.build(commonResponse); &#125; catch (ClientException e) &#123; log.error(&quot;send msg error.&quot;, e); return SmsResponse.SmsResponseBuilder.buildFail(e.getMessage()); &#125; catch (JsonProcessingException e) &#123; log.error(&quot;write json failed.&quot;, e); return SmsResponse.SmsResponseBuilder.buildFail(&quot;短信参数在json序列化时出错&quot;); &#125; &#125;&#125; LicensesThe Apache License, Version 2.0 IssuesIssues Welcome 支持 Click Github to star, Thanks! 更多参考阿里云短信服务API参考","tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://gcdd1993.github.io/tags/Spring-Boot/"}]},{"title":"分布式任务调度XXL-JOB初体验","date":"2019-06-05T05:19:33.000Z","path":"p/29085/","text":"简介XXL-JOB是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现已开放源代码并接入多家公司线上产品线，开箱即用。 官方文档很完善，不多赘述。本文主要是搭建XXL-JOB和简单使用的记录。 搭建xxl-job-admin管理端运行环境 Ubuntu 16.04 64位 Mysql 5.7 安装Mysql1234567891011121314$ sudo apt-get update$ sudo apt-get install mysql-server## 设置mysql，主要是安全方面的，密码策略等$ mysql_secure_installation## 配置远程访问$ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnfbind-address = 0.0.0.0$ sudo service mysql restart$ sudo service mysql status● mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-06-05 13:23:41 HKT; 45s ago... 创建数据库12$ mysql -u root -pmysql&gt; CREATE database if NOT EXISTS `xxl-job` default character set utf8 collate utf8_general_ci; 创建用户123$ mysql -u root -pmysql&gt; CREATE USER &#x27;xxl-job&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;xxlJob2019@&#x27;;mysql&gt; GRANT ALL PRIVILEGES ON `xxl-job`.* TO &#x27;xxl-job&#x27;@&#x27;%&#x27;; 本地测试xxl-job-admin拉取最新源码12$ git clone git@github.com:xuxueli/xxl-job.git$ cd xxl-job 导入项目我比较熟悉Idea开发工具，所以这里使用Idea的Gradle项目进行演示。 打开xxl-job，项目结构如下 测试项目打开xxl-job-admin/resources/application.properties，修改mysql连接信息 1234### xxl-job, datasourcespring.datasource.url=jdbc:mysql://192.168.32.129:3306/xxl-job?Unicode=true&amp;characterEncoding=UTF-8spring.datasource.username=xxl-jobspring.datasource.password=xxlJob2019@ 使用/xxl-job/doc/db/tables_xxl_job.sql初始化数据库，初始化完应该如下图 准备就绪后，就可以启动项目了，然后打开地址http://localhost:8080/xxl-job-admin将会看到首页 部署打包调度中心12345678910111213141516171819$ cd /xxl-job$ mvn install...[INFO] xxl-job ............................................ SUCCESS [ 0.513 s][INFO] xxl-job-core ....................................... SUCCESS [ 4.258 s][INFO] xxl-job-admin ...................................... SUCCESS [ 5.525 s][INFO] xxl-job-executor-samples ........................... SUCCESS [ 0.016 s][INFO] xxl-job-executor-sample-spring ..................... SUCCESS [ 2.188 s][INFO] xxl-job-executor-sample-springboot ................. SUCCESS [ 0.892 s][INFO] xxl-job-executor-sample-jfinal ..................... SUCCESS [ 1.753 s][INFO] xxl-job-executor-sample-nutz ....................... SUCCESS [ 1.316 s][INFO] xxl-job-executor-sample-frameless .................. SUCCESS [ 0.358 s][INFO] xxl-job-executor-sample-jboot ...................... SUCCESS [ 1.279 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 18.549 s[INFO] Finished at: 2019-06-05T14:40:25+08:00[INFO] ------------------------------------------------------------------------ 看到以上信息，说明我们打包成功了，在/xxl-job/xxl-job-admin目录下会存在jar文件：xxl-job-admin-2.1.0-SNAPSHOT.jar 部署到服务器12345678910111213141516171819202122232425262728293031323334$ sudo apt install openjdk-8-jdk$ java -versionopenjdk version &quot;1.8.0_212&quot;OpenJDK Runtime Environment (build 1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03)OpenJDK 64-Bit Server VM (build 25.212-b03, mixed mode)$ sudo mkdir -p /data/xxl-job$ sudo cd /data/xxl-job## 上传我们打包好的jar至此目录，并添加软连接$ sudo ln -s xxl-job-admin-2.1.0-SNAPSHOT.jar current.jar## 注册为system服务，可以达到异常重启，开机自启等目的$ sudo vim /etc/systemd/system/xxl-job.service[Unit]Description=xxl-job Service DaemonAfter=mysql.service[Service]Environment=&quot;JAVA_OPTS= -Xmx1024m -Xms1024m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:NewRatio=3 -Dserver.port=8081&quot;# java要写绝对路径ExecStart=/usr/local/jdk/bin/java -jar /data/xxl-job/current.jarRestart=alwaysWorkingDirectory=/data/xxl-job/[Install]WantedBy=multi-user.target$ sudo systemctl enable xxl-job.service$ sudo service xxl-job start$ sudo service xxl-job status● xxl-job.service - xxl-job Service Daemon Loaded: loaded (/etc/systemd/system/xxl-job.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2019-07-18 18:19:08 CST; 2min 19s ago Main PID: 27572 (java) CGroup: /system.slice/xxl-job.service └─27572 /usr/local/jdk/bin/java -jar /data/xxl-job/current.jar 我们访问一下http://192.168.32.129:8080/xxl-job-admin： 测试任务调度以上，我们的任务调度管理端已经搭建完成，接下来，让我们测试下任务调度。 直接使用自带的SpringBoot测试项目xxl-job-executor-sample-springboot进行测试，修改配置文件 1xxl-job-executor-sample-springboot=http://192.168.32.129:8080/xxl-job-admin 自定义任务编写一个简单的任务，打印100次当前序列 12345678910111213141516171819202122232425262728package com.xxl.job.executor.service.jobhandler;import com.xxl.job.core.biz.model.ReturnT;import com.xxl.job.core.handler.IJobHandler;import com.xxl.job.core.handler.annotation.JobHandler;import com.xxl.job.core.log.XxlJobLogger;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;/** * TODO * * @author gaochen * @date 2019/6/5 */@JobHandler(value=&quot;gcddJobHandler&quot;)@Componentpublic class GcddJobHandler extends IJobHandler &#123; @Override public ReturnT&lt;String&gt; execute(String param) throws Exception &#123; for (int i = 0; i &lt; 100; i++) &#123; XxlJobLogger.log(&quot;XXL-JOB, print &quot; + i); TimeUnit.SECONDS.sleep(1); &#125; return SUCCESS; &#125;&#125; 启动执行器然后启动执行器，启动完成后，我们会发现管理页面的执行器列表会多出我们刚才启动的执行器 添加任务 查看任务执行日志 可以看到，任务已经按照我们的规划执行成功了，非常的方便。 结语想要了解更详细的内容，请访问xxl-job官网","tags":[{"name":"xxl-job","slug":"xxl-job","permalink":"https://gcdd1993.github.io/tags/xxl-job/"},{"name":"分布式","slug":"分布式","permalink":"https://gcdd1993.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"手写IOC容器","date":"2019-06-02T11:59:52.000Z","path":"p/11305/","text":"前言本文是为了学习Spring IOC容器的执行过程而写，不能完全代表Spring IOC容器，只是简单实现了容器的依赖注入和控制反转功能，无法用于生产，只能说对理解Spring容器能够起到一定的作用。 开始创建项目创建Gradle项目，并修改build.gradle 1234567891011121314151617plugins &#123; id &#x27;java&#x27; id &quot;io.franzbecker.gradle-lombok&quot; version &quot;3.1.0&quot;&#125;group &#x27;io.github.gcdd1993&#x27;version &#x27;1.0-SNAPSHOT&#x27;sourceCompatibility = 1.8repositories &#123; mavenCentral()&#125;dependencies &#123; testCompile group: &#x27;junit&#x27;, name: &#x27;junit&#x27;, version: &#x27;4.12&#x27;&#125; 创建BeanFactoryBeanFactory是IOC中用于存放bean实例以及获取bean的核心接口，它的核心方法是getBean以及getBean的重载方法，这里简单实现两个getBean的方法。 12345678910111213141516171819202122232425262728package io.github.gcdd1993.ioc.bean;/** * bean factory interface * * @author gaochen * @date 2019/6/2 */public interface BeanFactory &#123; /** * 通过bean名称获取bean * * @param name bean名称 * @return bean */ Object getBean(String name); /** * 通过bean类型获取bean * * @param tClass bean类型 * @param &lt;T&gt; 泛型T * @return bean */ &lt;T&gt; T getBean(Class&lt;T&gt; tClass);&#125; 创建ApplicationContext上下文ApplicationContext，即我们常说的应用上下文，实际就是Spring容器本身了。 我们创建ApplicationContext类，并实现BeanFactory接口。 12public class ApplicationContext implements BeanFactory &#123;&#125; getBean方法既然说是容器，那肯定要有地方装我们的bean实例吧，使用两个Map作为容器。 123456789/** * 按照beanName分组 */private final Map&lt;String, Object&gt; beanByNameMap = new ConcurrentHashMap&lt;&gt;(256);/** * 按照beanClass分组 */private final Map&lt;Class&lt;?&gt;, Object&gt; beanByClassMap = new ConcurrentHashMap&lt;&gt;(256); 然后，我们可以先完成我们的getBean方法。 123456789@Overridepublic Object getBean(String name) &#123; return beanByNameMap.get(name);&#125;@Overridepublic &lt;T&gt; T getBean(Class&lt;T&gt; tClass) &#123; return tClass.cast(beanByClassMap.get(tClass));&#125; 直接从Map中获取bean实例，是不是很简单？当然了，在真实的Spring容器中，是不会这么简单啦，不过我们这次是要化繁为简，理解IOC容器。 构造器Spring提供了@ComponentScan来扫描包下的Component，我们为了简便，直接在构造器中指定要扫描的包。 123456789101112131415private final Set&lt;String&gt; basePackages;/** * 默认构造器，默认扫描当前所在包 */public ApplicationContext() &#123; this(new HashSet&lt;&gt;(Collections.singletonList(ApplicationContext.class.getPackage().getName())));&#125;/** * 全参构造器 * @param basePackages 扫描的包名列表 */public ApplicationContext(Set&lt;String&gt; basePackages) &#123; this.basePackages = basePackages;&#125; refresh方法refresh的过程基本按照以下流程来走 扫描指定的包下所有带@Bean注解（Spring中是@Component注解）的类。 12345678910List&lt;Class&gt; beanClasses = PackageScanner.findClassesWithAnnotation(packageName, Bean.class);System.out.println(&quot;scan classes with Bean annotation : &quot; + beanClasses.toString());for (Class beanClass : beanClasses) &#123; try &#123; createBean(beanClass); &#125; catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException | InvocationTargetException | InstantiationException e) &#123; e.printStackTrace(); &#125;&#125; 遍历类，获取类的构造器以及所有字段。 123Constructor constructor = beanClass.getDeclaredConstructor();Object object = constructor.newInstance();Field[] fields = beanClass.getDeclaredFields(); 判断字段是依赖注入的还是普通字段。 如果是普通字段，通过字段类型初始化该字段，并尝试从@Value注解获取值塞给字段。 1234567Value value = field.getAnnotation(Value.class);if (value != null) &#123; // 注入 field.setAccessible(true); // 需要做一些类型转换，从String转为对应的类型 field.set(object, value.value());&#125; 如果是依赖注入的字段，尝试从beanByClassMap中获取对应的实例，如果没有，就先要去实例化该字段对应的类型。 123456789101112131415161718192021Autowired autowired = field.getAnnotation(Autowired.class);if (autowired != null) &#123; // 依赖注入 String name = autowired.name(); // 按照名称注入 Object diObj; if (!name.isEmpty()) &#123; diObj = beanByNameMap.get(name) == null ? createBean(name) : beanByNameMap.get(name); &#125; else &#123; // 按照类型注入 Class&lt;?&gt; aClass = field.getType(); diObj = beanByClassMap.get(aClass) == null ? createBean(aClass) : beanByClassMap.get(aClass); &#125; // 注入 field.setAccessible(true); field.set(object, diObj);&#125; 测试我们的IOC容器创建Address 123456789@Data@Beanpublic class Address &#123; @Value(&quot;2222&quot;) private String longitude; @Value(&quot;1111&quot;) private String latitude;&#125; 创建Person并注入Address 123456789101112@Data@Beanpublic class Person &#123; @Autowired private Address address; @Value(&quot;gaochen&quot;) private String name; @Value(&quot;27&quot;) private String age;&#125; 创建测试类ApplicationContextTest 12345678910111213141516public class ApplicationContextTest &#123; @Test public void refresh() &#123; Set&lt;String&gt; basePackages = new HashSet&lt;&gt;(1); basePackages.add(&quot;io.github.gcdd1993.ioc&quot;); ApplicationContext ctx = new ApplicationContext(basePackages); ctx.refresh(); Person person = ctx.getBean(Person.class); System.out.println(person); Object person1 = ctx.getBean(&quot;Person&quot;); System.out.println(person1); &#125;&#125; 控制台将会输出： 1234scan classes with Bean annotation : [class io.github.gcdd1993.ioc.util.Address, class io.github.gcdd1993.ioc.util.Person]scan classes with Bean annotation : [class io.github.gcdd1993.ioc.util.Address, class io.github.gcdd1993.ioc.util.Person]Person(address=Address(longitude=2222, latitude=1111), name=gaochen, age=27)Person(address=Address(longitude=2222, latitude=1111), name=gaochen, age=27) 可以看到，我们成功将Address实例注入到了Person实例中，并且将它们存储在了我们自己的IOC容器中。其实，Spring容器的原理大致就是如此，只不过为了应对企业级开发，提供了很多便捷的功能，例如bean的作用域、bean的自定义方法等等。 获取源码完整源码可以在我的github仓库获取👉Simple-IOC-Container","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"使用Tesseract-Ocr识别数字","date":"2019-06-01T17:59:30.000Z","path":"p/5479/","text":"前言Tesseract-Ocr是我在编写爬虫项目中，用来识别图片（不是验证码）的本地解决方案（因为客户不想使用API识别，太贵），识别率目前达到了100%，可以说是相当了得，当然了，这取决于使用的traineddata。 简介 Tesseract最初是在1985年至1994年间在Hewlett-Packard Laboratories Bristol和Greeley Colorado的Hewlett-Packard Co开发的，1996年进行了一些更改，移植到Windows，并且随着C++在1998年兴起。2005年Tesseract由惠普开源，然后从2006年至今，由谷歌继续开发。 Tesseract-Ocr并不是一个软件，它是一个软件包，包含了一个OCR引擎【libtesseract】和一个命令行程序 【tesseract】。Tesseract 4增加了一个基于OCR引擎的新神经网络（LSTM），该引擎专注于行级识别，但仍然支持Tesseract 3的传统Tesseract OCR引擎，该引擎通过识别字符模式来工作。 要启用与Tesseract 3的兼容性，你需要使用Legacy OCR Engine模式（–oem 0）。它还需要支持传统引擎的traineddata（训练好的数据文件），这些文件可以从tessdata存储库的文件获取。 Tesseract支持识别unicode（UTF-8），可以“开箱即用”识别100多种语言。 Tesseract支持多种输出格式：纯文本，hOCR（HTML），PDF，TSV。主分支还具有ALTO（XML）输出的实验支持。 ⭐️⭐️⭐️ 具体介绍可以上tesseract-wiki查看。 在Java上使用创建项目，并引入Jar包Maven123456&lt;!-- https://mvnrepository.com/artifact/net.sourceforge.tess4j/tess4j --&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.tess4j&lt;/groupId&gt; &lt;artifactId&gt;tess4j&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt;&lt;/dependency&gt; Gradle1compile &#x27;net.sourceforge.tess4j:tess4j:4.3.1&#x27; 导入traineddatatraineddata是使用Tesseract-Ocr训练好的数据文件，可以直接使用。这些文件你可以去tessdata存储库查找，也可以去谷歌搜索，当然了，你也可以自己训练😂。 traineddata通常以*.traineddata命名，其中*指的是支持的语言类型。在这里你可以看到4.0.0版本支持的语言以及traineddata列表。 这次，我们选择eng.traineddata进行测试。下载eng.traineddata放入/resources/traineddata目录，如下图所示。 编写测试代码初始化Tesseract引擎1234567891011public class TesseractTest &#123; private ITesseract tesseract; @Before public void init() &#123; tesseract = new Tesseract(); System.out.println(&quot;tesseract init done...&quot;); &#125;&#125; 实际上，上面的代码是无法正常运行的，因为找不到指定语言版本的traineddata文件。 net.sourceforge.tess4j:tess4j:4.1.1提供的API并不好，在Tesseract构造函数中，没有提供可选参数的构造器。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Tesseract implements ITesseract &#123; // Tesseract使用的语言版本，用以选择traineddata private String language = &quot;eng&quot;; // traineddata目录，里面放*.traineddata数据文件 private String datapath; // 省略其他代码 ... public Tesseract() &#123; try &#123; // 默认从系统环境变量获取traineddata目录 datapath = System.getenv(&quot;TESSDATA_PREFIX&quot;); &#125; catch (Exception e) &#123; // ignore &#125; finally &#123; if (datapath == null) &#123; datapath = &quot;./&quot;; &#125; &#125; &#125; /** * Sets language for OCR. * * @param language the language code, which follows ISO 639-3 standard. */ @Override public void setLanguage(String language) &#123; this.language = language; &#125; /** * Sets path to &lt;code&gt;tessdata&lt;/code&gt;. * * @param datapath the tessdata path to set */ @Override public void setDatapath(String datapath) &#123; this.datapath = datapath; &#125; // 省略其他代码 ...&#125; 所以，我们可以选择设置环境变量TESSDATA_PREFIX为数据目录，或者通过Java编码的方式来设置。 12tesseract.setLanguage(&quot;eng&quot;); // 默认就是eng，你可以选择其他langtesseract.setDatapath(TesseractTest.class.getResource(&quot;/traineddata&quot;).getPath().substring(1)); OCR识别测试tesseract提供了一系列doOcr方法的重载，我们可以方便的进行OCR识别。 123456789101112131415String doOCR(File imageFile) throws TesseractException;String doOCR(File imageFile, Rectangle rect) throws TesseractException;String doOCR(BufferedImage bi) throws TesseractException;String doOCR(BufferedImage bi, Rectangle rect) throws TesseractException;String doOCR(List&lt;IIOImage&gt; imageList, Rectangle rect) throws TesseractException;String doOCR(List&lt;IIOImage&gt; imageList, String filename, Rectangle rect) throws TesseractException;String doOCR(int xsize, int ysize, ByteBuffer buf, Rectangle rect, int bpp) throws TesseractException;String doOCR(int xsize, int ysize, ByteBuffer buf, String filename, Rectangle rect, int bpp) throws TesseractException; 可以看出，doOcr方法支持多种图片识别方式，如图片文件、多个图片文件、图片文件局部处理等等方式。 为了方便测试，我们选取最简单的图片文件方式测试。 图片是个URL链接，如下所示 123456@Testpublic void testOcr() throws IOException, TesseractException &#123; BufferedImage image = ImageIO.read(new URL(&quot;http://static8.ziroom.com/phoenix/pc/images/price/aacd14fbc53a106c7f0f0d667535683as.png&quot;)); String ocr = tesseract.doOCR(image); System.out.println(&quot;ocr result : &quot; + ocr);&#125; 控制台输出： 12tesseract init done...ocr result : 2710386495 识别准确率，主要在于你选择的训练数据文件，我使用的是数据文件是这个，对于数字的准确率基本上是100%。 异常如果你遭遇Invalid memory access异常，这是由于找不到对应lang的*.traineddata文件，请修改language和datapath。 1234567891011121314Invalid memory accessjava.lang.Error: Invalid memory access at com.sun.jna.Native.invokePointer(Native Method) at com.sun.jna.Function.invokePointer(Function.java:470) at com.sun.jna.Function.invoke(Function.java:404) at com.sun.jna.Function.invoke(Function.java:315) at com.sun.jna.Library$Handler.invoke(Library.java:212) at com.sun.proxy.$Proxy9.TessBaseAPIGetUTF8Text(Unknown Source) at net.sourceforge.tess4j.Tesseract.getOCRText(Tesseract.java:495) at net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:321) at net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:293) at net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:274) at net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:258) ... 训练工具https://github.com/tesseract-ocr/tesseract/wiki/AddOns 训练数据仓库 tessdata_best：基于LSTM引擎的训练数据，最佳最准确的 tessdata_fast：基于LSTM引擎的训练数据，快速（精简）版本 tessdata：支持双引擎（LSTM和传统引擎），但LSTM训练数据不是最新的版本 推荐使用tessdata_best，虽然识别速度相对于tessdata_fast稍慢，但是准确率可以保证。 参考tesseract-ocr-wiki","tags":[{"name":"Tesseract-Ocr","slug":"Tesseract-Ocr","permalink":"https://gcdd1993.github.io/tags/Tesseract-Ocr/"}]},{"title":"理解一致性Hash算法","date":"2019-05-29T09:13:47.000Z","path":"p/61241/","text":"简介 一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。 现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是由客户端来提供，具体在计算一致性hash时采用如下步骤： 首先求出memcached服务器（节点）的哈希值，并将其配置到0～2^32的圆（continuum）上。 采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过2^32仍然找不到服务器，就会保存到第一台memcached服务器上。 从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但Consistent Hashing中，只有在圆（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示： 一致性Hash性质 考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的。 尤其是在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要。 良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面： 平衡性(Balance) 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity) 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。 哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。 分散性(Spread) 在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。 当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。 分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load) 负载问题实际上是从另一个角度看待分散性问题。 既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。 与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 平滑性(Smoothness) 平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。 原理基本概念一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。 简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下： 整个空间按顺时针方向组织。0和2^32-1在零点中方向重合。 下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下： 接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。 例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下： 根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 容错性现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。 一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。 可扩展性如果在系统中增加一台服务器Node X，如下图所示： 此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。 一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 数据倾斜问题另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。 为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。 具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。 在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 代码测试一致性Hash模拟类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package com.example.demo.hash;import java.util.*;/** * 一致性Hash * * @author gaochen * @date 2019/5/29 */public class ConsistentHash&lt;T&gt; &#123; /** * 节点的复制因子,实际节点个数 * numberOfReplicas */ private final int numberOfReplicas; /** * 虚拟节点个数,存储虚拟节点的hash值到真实节点的映射 */ private final SortedMap&lt;Integer, T&gt; circle = new TreeMap&lt;&gt;(); public ConsistentHash(int numberOfReplicas, Collection&lt;T&gt; nodes) &#123; this.numberOfReplicas = numberOfReplicas; for (T node : nodes) &#123; add(node); &#125; &#125; /** * 模拟添加一个节点 * &lt;p&gt; * 对于一个实际机器节点 node, 对应 numberOfReplicas 个虚拟节点 * 不同的虚拟节点(i不同)有不同的hash值,但都对应同一个实际机器node * 虚拟node一般是均衡分布在环上的,数据存储在顺时针方向的虚拟node上 * &lt;/P&gt; * * @param node 哈希环节点 */ public void add(T node) &#123; for (int i = 0; i &lt; numberOfReplicas; i++) &#123; String nodestr = node.toString() + i; int hashcode = nodestr.hashCode(); System.out.println(&quot;hashcode:&quot; + hashcode); circle.put(hashcode, node); &#125; &#125; /** * 删除一个节点 * * @param node 待删除节点 */ public void remove(T node) &#123; for (int i = 0; i &lt; numberOfReplicas; i++) &#123; circle.remove((node.toString() + i).hashCode()); &#125; &#125; /** * 获得一个最近的顺时针节点,根据给定的key 取Hash * 然后再取得顺时针方向上最近的一个虚拟节点对应的实际节点 * 再从实际节点中取得 数据 * * @param key 模拟缓存Key */ public T get(Object key) &#123; if (circle.isEmpty()) &#123; return null; &#125; // node 用String来表示,获得node在哈希环中的hashCode int hash = key.hashCode(); System.out.println(&quot;hashcode-----&gt;:&quot; + hash); //数据映射在两台虚拟机器所在环之间,就需要按顺时针方向寻找机器 if (!circle.containsKey(hash)) &#123; SortedMap&lt;Integer, T&gt; tailMap = circle.tailMap(hash); hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey(); &#125; return circle.get(hash); &#125; /** * 获取当前哈希环节点数 * * @return 哈希环节点数 */ public long getSize() &#123; return circle.size(); &#125; /** * 查看表示整个哈希环中各个虚拟节点位置 */ public void showBalance() &#123; //获得TreeMap中所有的Key Set&lt;Integer&gt; sets = circle.keySet(); //将获得的Key集合排序 SortedSet&lt;Integer&gt; sortedSets = new TreeSet&lt;Integer&gt;(sets); for (Integer hashCode : sortedSets) &#123; System.out.println(hashCode); &#125; System.out.println(&quot;----each location &#x27;s distance are follows: ----&quot;); //查看相邻两个hashCode的差值 Iterator&lt;Integer&gt; it = sortedSets.iterator(); Iterator&lt;Integer&gt; it2 = sortedSets.iterator(); if (it2.hasNext()) &#123; it2.next(); &#125; long keyPre, keyAfter; while (it.hasNext() &amp;&amp; it2.hasNext()) &#123; keyPre = it.next(); keyAfter = it2.next(); System.out.println(keyAfter - keyPre); &#125; &#125;&#125; 测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.example.demo.hash;import org.junit.Before;import org.junit.Test;import java.util.Arrays;import java.util.HashSet;import java.util.List;import java.util.Set;/** * TODO * * @author gaochen * @date 2019/5/29 */public class ConsistentHashTest &#123; private static ConsistentHash&lt;String&gt; consistentHash; @Before public void initHash() &#123; Set&lt;String&gt; nodes = new HashSet&lt;&gt;(); consistentHash = new ConsistentHash&lt;&gt;(2, nodes); &#125; @Test public void testBalance() &#123; // 分配三个节点 consistentHash.add(&quot;A1&quot;); consistentHash.add(&quot;C1&quot;); consistentHash.add(&quot;D1&quot;); System.out.println(&quot;hash circle size: &quot; + consistentHash.getSize()); System.out.println(&quot;location of each node are follows: &quot;);// consistentHash.showBalance(); // hash值在当前哈希环内 final String key1 = &quot;A31&quot;; // hash值超出了当前哈希环 final String key2 = &quot;Apple&quot;; final List&lt;String&gt; keys = Arrays.asList(key1, key2); // 模拟节点分配 showAllocate(keys); // 模拟增加节点, A31被分配到更近的B1节点 consistentHash.add(&quot;B1&quot;); System.out.println(&quot;增加节点B1&quot;); showAllocate(keys); System.out.println(&quot;-------------------------------------&quot;); // 模拟删除节点, A31被分配到更近的C1节点 consistentHash.remove(&quot;B1&quot;); System.out.println(&quot;删除节点B1&quot;); showAllocate(keys); &#125; /** * 模拟缓存分配 * * @param keys 缓存键 */ private void showAllocate(List&lt;String&gt; keys) &#123; keys.forEach(key -&gt; &#123; String node = consistentHash.get(key); // A31被分配到更近的C1节点 System.out.println(String.format(&quot;key %s is allocated to node %s&quot;, key, node)); &#125;); &#125;&#125; 控制台输出： 12345678910111213141516171819202122232425hashcode:64032hashcode:64033hashcode:65954hashcode:65955hashcode:66915hashcode:66916hash circle size: 6location of each node are follows: hashcode-----&gt;:64095key A31 is allocated to node C1hashcode-----&gt;:63476538key Apple is allocated to node A1hashcode:64993hashcode:64994增加节点B1hashcode-----&gt;:64095key A31 is allocated to node B1hashcode-----&gt;:63476538key Apple is allocated to node A1-------------------------------------删除节点B1hashcode-----&gt;:64095key A31 is allocated to node C1hashcode-----&gt;:63476538key Apple is allocated to node A1 可以看出，增加或删除节点，只会影响到节点与上一个节点之间的元素，所以一致性Hash算法在容错性和可扩展性上面较普通Hash是有巨大提升的。 参考资料五分钟看懂一致性哈希算法 维基百科-散列函数 一致性哈希算法及其在分布式系统中的应用","tags":[{"name":"一致性Hash","slug":"一致性Hash","permalink":"https://gcdd1993.github.io/tags/%E4%B8%80%E8%87%B4%E6%80%A7Hash/"}]},{"title":"理解Nginx负载均衡","date":"2019-05-29T02:30:45.000Z","path":"p/52703/","text":"前言工作以来，一直都在使用Nginx作为负载均衡服务器，但是关于Nginx的负载均衡算法一直没有深入理解过，这次好好的整理下。 准备服务器 搭建三台用于测试的虚拟机 名称 IP 服务 node01 192.168.198.131 Nginx、模拟业务（8080） node02 192.168.198.130 模拟业务（8080） node03 192.168.198.132 模拟业务（8080） 修改hostname和hosts 123456$ vim /etc/hosts192.168.198.131 node01$ vim /etc/hostnamenode01$ reboot## 其余两台也改下，并重启使配置生效 在node01上安装Nginx服务 1234$ echo -e &quot;deb http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\\ndeb-src http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx&quot; | sudo tee /etc/apt/sources.list.d/nginx.list$ wget -O- http://nginx.org/keys/nginx_signing.key | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install nginx 模拟业务使用https://start.spring.io快速新建Spring Boot项目，添加Web模块，并编写以下代码： 123456789@RestController@RequestMapping(&quot;/test&quot;)public class DemoController &#123; @GetMapping public String test() throws UnknownHostException &#123; return &quot;this is : &quot; + Inet4Address.getLocalHost(); &#125;&#125; 打包并部署到服务器，我使用的是The Application Plugin，部署完毕启动 测试下： 123456789101112## node01$ curl 192.168.198.131:8080/test...this is : node01/192.168.198.131## node02$ curl 192.168.198.130:8080/test...this is : node02/192.168.198.130## node03$ curl 192.168.198.132:8080/test...this is : node03/192.168.198.132 Nginx负载均衡Round Robin（轮询） 请求在服务器之间均匀分布，可以设置服务器权重。 12345678910111213141516$ vim /etc/nginx/conf/demo.confupstream backend &#123; server 192.168.198.131:8080; server 192.168.198.132:8080; server 192.168.198.130:8080;&#125;server &#123; listen 80; server_name 192.168.198.131; location / &#123; proxy_pass http://backend; &#125;&#125;$ service nginx restart 测试下 123456789101112$ curl 192.168.198.131/test...this is : node01/192.168.198.131 # node01$ curl 192.168.198.131/test...this is : node03/192.168.198.132 # node03$ curl 192.168.198.131/test...this is : node03/192.168.198.130 # node02$ curl 192.168.198.131/test...this is : node01/192.168.198.131 # node01 可以看到，每台服务器访问到的次数是相等的。 Least Connections 请求分配到连接数最少的服务器，可以设置服务器权重。 12345678$ vim /etc/nginx/conf/demo.confupstream backend &#123; least_conn; server 192.168.198.131:8080; server 192.168.198.132:8080; server 192.168.198.130:8080;&#125;$ service nginx restart 这个不知道怎么模拟出连接数最少场景。 IP Hash 从客户端的IP地址来确定请求应该发送给哪台服务器。在这种情况下，使用IPv4地址的前三个八位字节或整个IPv6地址来计算散列值。该方法能保证来自同一地址的请求分配到同一台服务器，除非该服务器不可用。 12345678$ vim /etc/nginx/conf/demo.confupstream backend &#123; ip_hash; server 192.168.198.131:8080; server 192.168.198.132:8080; server 192.168.198.130:8080;&#125;$ service nginx restart 测试下 123456789$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node01/192.168.198.131 可以看到，请求都被分配到node01节点。 接下来，将node01节点关闭，看看会发生什么： 12345678910111213141516$ ps -ef | grep demoroot 3343 1764 0 11:52 pts/0 00:00:23 java -jar /home/demo/demo-boot-0.0.1-SNAPSHOT/lib/demo-0.0.1-SNAPSHOT.jarroot 4529 1764 0 13:11 pts/0 00:00:00 grep --color=auto demo$ kill -9 3343$ ps -ef | grep demoroot 4529 1764 0 13:11 pts/0 00:00:00 grep --color=auto demo$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132 由于node01节点不可用，请求都被分配到node03节点。 Generic Hash 与上面的IP_HASH类似，通用HASH按照用户定义的参数来计算散列值，参数可以是文本字符串，变量或组合。例如，参数可以是远端地址： 12345678$ vim /etc/nginx/conf/demo.confupstream backend &#123; hash $remote_addr consistent; server 192.168.198.131:8080; server 192.168.198.132:8080; server 192.168.198.130:8080;&#125;$ service nginx restart 测试下 123456789$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node02/192.168.198.130 可以看到，请求都被分配到了node02节点。 👉上面的consistent是可选参数，如果设置了，将采用Ketama一致性hash算法计算散列值。 关于一致性Hash，可以查看我的另一篇博客：理解一致性Hash算法 Random 请求会被随机分配到一台服务器，可以设置服务器权重。 12345678$ vim /etc/nginx/conf/demo.confupstream backend &#123; random; server 192.168.198.131:8080; server 192.168.198.132:8080; server 192.168.198.130:8080;&#125;$ service nginx restart 测试下 123456789101112$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node01/192.168.198.130 可以看到，请求是被随机分配到三台服务器的。 Weights 除了设置负载均衡算法，我们还可以为服务器设置权重，权重默认值是1 1234567$ vim /etc/nginx/conf/demo.confupstream backend &#123; server 192.168.198.131:8080 weight=5; server 192.168.198.132:8080 weight=10; server 192.168.198.130:8080;&#125;$ service nginx restart 测试下 123456789101112131415$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131 可以看到，5次请求中，node03(weight=10)占了3次，node01(weight=5)占了2次，node02(weight=1)1次都没有。 理论上来说，上面的配置，访问16次，node03应被分配10次，node01应被分配5次，node02应被分配1次。 参考资料http-load-balancer","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://gcdd1993.github.io/tags/Nginx/"}]},{"title":"Redis 常用命令","date":"2019-05-26T03:41:40.000Z","path":"p/1091/","text":"前言Redis提供了丰富的命令（command）对数据库和各种数据类型进行操作，这些command可以在Linux终端使用。在编程时，比如各类语言包，这些命令都有对应的方法。下面将Redis提供的命令做一总结。 键值相关命令keys 返回满足给定pattern的所有key 12345678910111213141516171819202122232425127.0.0.1:6379&gt; keys * 1) &quot;mylist4&quot; 2) &quot;myset7&quot; 3) &quot;name1&quot; 4) &quot;myset3&quot; 5) &quot;myset2&quot; 6) &quot;mylist2&quot; 7) &quot;mylist6&quot; 8) &quot;name&quot; 9) &quot;myhash&quot;10) &quot;mylist7&quot;11) &quot;key1&quot;12) &quot;mylist5&quot;13) &quot;mylist8&quot;14) &quot;myzset2&quot;15) &quot;myzset3&quot;16) &quot;myzset&quot;17) &quot;myset5&quot;18) &quot;myset4&quot;19) &quot;mylist3&quot;20) &quot;myset&quot;21) &quot;myset6&quot;22) &quot;age&quot;23) &quot;mylist&quot;24) &quot;key2&quot; 用表达式*，代表取出所有的key。 123456789127.0.0.1:6379&gt; keys mylist*1) &quot;mylist4&quot;2) &quot;mylist2&quot;3) &quot;mylist6&quot;4) &quot;mylist7&quot;5) &quot;mylist5&quot;6) &quot;mylist8&quot;7) &quot;mylist3&quot;8) &quot;mylist&quot; 用表达式mylist*，代表取出所有以mylist开头的key。 exists 确认一个key是否存在 1234127.0.0.1:6379&gt; exists HongWan(integer) 0127.0.0.1:6379&gt; exists age(integer) 1 从结果来数据库中不存在HongWan这个key，但是age这个key是存在的。 del 删除一个key 1234127.0.0.1:6379&gt; del age(integer) 1127.0.0.1:6379&gt; exists age(integer) 0 expire 设置一个key的过期时间(单位:秒) 12345678910111213141516127.0.0.1:6379&gt; exists addr(integer) 1127.0.0.1:6379&gt; ttl addr(integer) -1127.0.0.1:6379&gt; expire addr 10(integer) 1127.0.0.1:6379&gt; ttl addr(integer) 6127.0.0.1:6379&gt; ttl addr(integer) 5127.0.0.1:6379&gt; ttl addr(integer) 4127.0.0.1:6379&gt; ttl addr(integer) -2127.0.0.1:6379&gt; exists addr(integer) 0 可以看到，未设置过期时间时，ttl值为-1，设置10s过期后，不断地使用ttl获取key的有效时长，当值为-2时，表示已过期并被删除。 move 将当前数据库中的key转移到其它数据库中 1234567891011121314127.0.0.1:6379&gt; select 0OK127.0.0.1:6379&gt; set age 30OK127.0.0.1:6379&gt; get age&quot;30&quot;127.0.0.1:6379&gt; move age 1(integer) 1127.0.0.1:6379&gt; get age(nil)127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; get age&quot;30&quot; 在本例中，我先显式的选择了数据库0，然后在这个库中设置一个key，接下来我们将这个key从数据库0移到数据库1，之后我们确认在数据库0中无此key了, 但在数据库1中存在这个key，说明我们转移成功了 。 persist 移除给定key的过期时间 12345678127.0.0.1:6379[1]&gt; expire age 300(integer) 1127.0.0.1:6379[1]&gt; ttl age(integer) 296127.0.0.1:6379[1]&gt; persist age(integer) 1127.0.0.1:6379[1]&gt; ttl age(integer) -1 在这个例子中，我们手动的将未到过期时间的key，成功设置为过期。 randomkey 随机返回key空间的一个key 1234127.0.0.1:6379&gt; randomkey&quot;mylist5&quot;127.0.0.1:6379&gt; randomkey&quot;myzset2&quot; 通过结果可以看到取key的规则是随机的。 rename 重命名key 123456127.0.0.1:6379[1]&gt; keys *1) &quot;age&quot;127.0.0.1:6379[1]&gt; rename age age_newOK127.0.0.1:6379[1]&gt; keys *1) &quot;age_new&quot; age成功的被我们改名为age_new了。 type 返回值的类型 123456127.0.0.1:6379&gt; type namestring127.0.0.1:6379&gt; type mysetset127.0.0.1:6379&gt; type myzsetzset 服务器相关命令ping 测试连接是否存活 12345678127.0.0.1:6379&gt; pingPONG// 执行下面命令之前，我们停止redis服务器127.0.0.1:6379&gt; pingCould not connect to Redis at 127.0.0.1:6379: Connection refused// 执行下面命令之前，我们启动redis服务器not connected&gt; pingPONG echo 在命令行打印一些内容 12127.0.0.1:6379&gt; echo HongWan&quot;HongWan&quot; select 选择数据库。Redis数据库编号从0~15，我们可以选择任意一个数据库来进行数据的存取 1234127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; select 16(error) ERR invalid DB index quit 退出连接 12127.0.0.1:6379&gt; quitroot@test01:~# dbsize 返回当前数据库中key的数目 12127.0.0.1:6379&gt; dbsize(integer) 23 结果说明此库中有23个key。 info 获取服务器的信息和统计 123456789101112127.0.0.1:6379&gt; info# Serverredis_version:3.0.6redis_git_sha1:00000000redis_git_dirty:0redis_build_id:28b6715d3583bf8eredis_mode:standaloneos:Linux 4.4.0-148-generic x86_64arch_bits:64multiplexing_api:epollgcc_version:5.4.0... 此结果用于说明服务器的基础信息，包括版本、启动时间等。 monitor 实时转储收到的请求 先在终端1输入monitor命令，将会进入等待状态 12127.0.0.1:6379&gt; monitorOK 新建一个终端，输入一些redis命令 1234567891011121314151617181920212223242526127.0.0.1:6379&gt; keys * 1) &quot;myset3&quot; 2) &quot;myset2&quot; 3) &quot;mylist7&quot; 4) &quot;mylist4&quot; 5) &quot;key1&quot; 6) &quot;myset7&quot; 7) &quot;name1&quot; 8) &quot;mylist6&quot; 9) &quot;myzset&quot;10) &quot;mylist2&quot;11) &quot;myset&quot;12) &quot;mylist&quot;13) &quot;myhash&quot;14) &quot;myset4&quot;15) &quot;name&quot;16) &quot;myset5&quot;17) &quot;myzset3&quot;18) &quot;mylist3&quot;19) &quot;mylist5&quot;20) &quot;myzset2&quot;21) &quot;mylist8&quot;22) &quot;key2&quot;23) &quot;myset6&quot;127.0.0.1:6379&gt; get addr(nil) 回到终端1中，我们将会看到打印出了刚才我们在终端2中敲入的redis命令 1234127.0.0.1:6379&gt; monitorOK1558844434.297954 [0 127.0.0.1:34926] &quot;keys&quot; &quot;*&quot;1558844444.673315 [0 127.0.0.1:34926] &quot;get&quot; &quot;addr&quot; config get 获取服务器配置信息 123127.0.0.1:6379&gt; config get dir1) &quot;dir&quot;2) &quot;/var/lib/redis&quot; 本例中我们获取了dir这个参数配置的值，如果想获取全部参数据的配置值也很简单，只需执行”config get *”即可将全部的值都显示出来。 flushdb 删除当前选择数据库中的所有key 123456127.0.0.1:6379&gt; dbsize(integer) 23127.0.0.1:6379&gt; flushdbOK127.0.0.1:6379&gt; dbsize(integer) 0 在本例中我们将0号数据库中的key都清除了。 flushall 删除所有数据库中的所有key 123456789101112127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; dbsize(integer) 1127.0.0.1:6379[1]&gt; select 0OK127.0.0.1:6379&gt; flushallOK127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; dbsize(integer) 0 在本例中我们先查看了一个1号数据库中有一个key，然后我切换到0号库执行flushall命令，结果1号库中的key也被清除了，说明此命令工作正常。 数据相关命令👉Redis-数据类型及操作","tags":[{"name":"Redis","slug":"Redis","permalink":"https://gcdd1993.github.io/tags/Redis/"},{"name":"NoSql","slug":"NoSql","permalink":"https://gcdd1993.github.io/tags/NoSql/"}]},{"title":"Redis 数据类型及操作","date":"2019-05-25T15:59:44.000Z","path":"p/38807/","text":"前言作为Key-value型数据库，Redis也提供了键（Key）和键值（Value）的映射关系。但是，除了常规的数值或字符串，Redis的键值还可以是以下形式之一： [Lists （可重复列表） ](#Lists （可重复列表） ) [Sets （不可重复集合） ](#Sets （不可重复集合）) [Sorted sets （不可重复有序集合） ](#Sorted sets （不可重复有序集合）) [Hashes （哈希表）](#Hashes （哈希表）) 键值的数据类型决定了该键值支持的操作。Redis支持诸如列表、集合或有序集合的交集、并集、查集等高级原子操作；同时，如果键值的类型是普通数字，Redis则提供自增等原子操作。 strings（字符串） string类型是二进制安全的。意思是redis的string可以包含任何数据，比如jpg图片或者序列化的对象。 set 设置key对应的值为string类型的value。 12127.0.0.1:6379&gt; set name wwlOK setnx 设置key对应的值为string类型的value。如果key已经存在，返回0，nx是not exist的意思。 123456127.0.0.1:6379&gt; get name&quot;wwl&quot;127.0.0.1:6379&gt; setnx name HongWan_new(integer) 0127.0.0.1:6379&gt; get name &quot;HongWan&quot; 由于原来name有一个对应的值，所以本次的修改不生效，且返回码是0。 setex 设置key对应的值为string类型的value，并指定此键值对应的有效期。 12345678127.0.0.1:6379&gt; setex haircolor 10 red OK127.0.0.1:6379&gt; get haircolor&quot;red&quot;127.0.0.1:6379&gt; get haircolor&quot;red&quot;127.0.0.1:6379&gt; get haircolor(nil) 可见由于最后一次的调用是10秒以后了，所以取不到haicolor这个键对应的值。 setrange 设置指定key的value值的子字符串。 12345678127.0.0.1:6379&gt; set name &#x27;HongWan@126.com&#x27;OK127.0.0.1:6379&gt; get name &quot;HongWan@126.com&quot;127.0.0.1:6379&gt; setrange name 8 gmail.com (integer) 17127.0.0.1:6379&gt; get name &quot;HongWan@gmail.com&quot; 其中的8是指从下标为8（包含8）的字符开始替换。 mset 一次设置多个key的值，成功返回ok表示所有的值都设置了，失败返回0表示没有任何值被设置。 123456127.0.0.1:6379&gt; mset key1 HongWan1 key2 HongWan2 OK127.0.0.1:6379&gt; get key1 &quot;HongWan1&quot;127.0.0.1:6379&gt; get key2&quot;HongWan2&quot; msetnx 一次设置多个key的值，成功返回ok表示所有的值都设置了，失败返回0表示没有任何值被设置，但是不会覆盖已经存在的key。 12345678910127.0.0.1:6379&gt; get key1&quot;HongWan1&quot;127.0.0.1:6379&gt; get key2&quot;HongWan2&quot;127.0.0.1:6379&gt; msetnx key2 HongWan2_new key3 HongWan3(integer) 0127.0.0.1:6379&gt; get key2 &quot;HongWan2&quot;127.0.0.1:6379&gt; get key3(nil) 可以看出如果这条命令返回0，那么里面操作都会回滚，都不会被执行。 get 获取key对应的string值,如果key不存在返回nil。 12345127.0.0.1:6379&gt; get name &quot;HongWan_new&quot;## 我们获取一个库中不存在的键name1，那么它会返回一个nil以表时无此键值对 redis 127.0.0.1:6379&gt; get name1 (nil) getset 设置key的值，并返回key的旧值。 123456127.0.0.1:6379&gt; get name &quot;HongWan&quot;127.0.0.1:6379&gt; getset name HongWan_new &quot;HongWan&quot;127.0.0.1:6379&gt; get name&quot;HongWan_new&quot; 如果key不存在，将返回nil，并会设置新值。 1234redis 127.0.0.1:6379&gt; getset name1 aaa(nil) 127.0.0.1:6379&gt; get name1&quot;aaa&quot; getrange 获取指定key的value值的子字符串。 1234567891011127.0.0.1:6379&gt; get name &quot;HongWan_new&quot;127.0.0.1:6379&gt; getrange name 0 6 &quot;HongWan&quot;## 字符串左面下标是从0开始的127.0.0.1:6379&gt; getrange name -7 -1&quot;Wan_new&quot;## 字符串右面下标是从-1开始的127.0.0.1:6379&gt; getrange name 7 100 &quot;_new&quot;## 当下标超出字符串长度时，将默认为是同方向的最大下标 mget 一次获取多个key的值，如果对应key不存在，则对应返回nil。 12345127.0.0.1:6379&gt; mget key1 key2 key3 1) &quot;HongWan1&quot;2) &quot;HongWan2&quot;3) (nil)## key3由于没有这个键定义，所以返回nil。 incr 对key的值做加加操作,并返回新的值。注意incr一个不是int的value会返回错误，incr一个不存在的key，则设置key为1 。 123456127.0.0.1:6379&gt; set age 20OK127.0.0.1:6379&gt; incr age (integer) 21127.0.0.1:6379&gt; get age&quot;21&quot; incrby 同incr类似，加指定值 ，key不存在时候会设置key，并认为原来的value是 0 。 12345678127.0.0.1:6379&gt; get age &quot;21&quot;127.0.0.1:6379&gt; incrby age 5(integer) 26127.0.0.1:6379&gt; get name&quot;HongWan_new&quot;127.0.0.1:6379&gt; get age&quot;26&quot; decr 对key的值做的是减减操作，decr一个不存在key，则设置key为-1。 123456127.0.0.1:6379&gt; get age&quot;26&quot;127.0.0.1:6379&gt; decr age(integer) 25127.0.0.1:6379&gt; get age&quot;25&quot; decrby 同decr，减指定值。 123456127.0.0.1:6379&gt; get age&quot;25&quot;127.0.0.1:6379&gt; decrby age 5(integer) 20127.0.0.1:6379&gt; get age&quot;20&quot; decrby完全是为了可读性，我们完全可以通过incrby一个负值来实现同样效果，反之一样。 123456127.0.0.1:6379&gt; get age&quot;20&quot;127.0.0.1:6379&gt; incrby age -5(integer) 15127.0.0.1:6379&gt; get age&quot;15&quot; append 给指定key的字符串值追加value,返回新字符串值的长度。 123456127.0.0.1:6379&gt; get name&quot;HongWan_new&quot;127.0.0.1:6379&gt; append name @126.com(integer) 19127.0.0.1:6379&gt; get name&quot;HongWan_new@126.com&quot; strlen 取指定key的value值的长度。 12345678127.0.0.1:6379&gt; get name&quot;HongWan_new@126.com&quot;127.0.0.1:6379&gt; strlen name(integer) 19127.0.0.1:6379&gt; get age&quot;15&quot;127.0.0.1:6379&gt; strlen age(integer) 2 Lists （可重复列表）list是一个链表结构，主要功能是push、pop、获取一个范围的所有值等等，操作中key理解为链表的名字。 Redis的list类型其实就是一个每个子元素都是string类型的双向链表。链表的最大长度是(2^32)。我们可以通过push,pop操作从链表的头部或者尾部添加删除元素。这使得list既可以用作栈，也可以用作队列。 lpush 在key对应list的头部添加字符串元素。 1234567127.0.0.1:6379&gt; lpush mylist &quot;world&quot;(integer) 1127.0.0.1:6379&gt; lpush mylist &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; lrange mylist 0 -11) &quot;hello&quot;2) &quot;world&quot; 在此处我们先插入了一个world，然后在world的头部插入了一个hello。其中lrange是用于获取mylist的内容。 rpush 在key对应list的尾部添加字符串元素。 1234567127.0.0.1:6379&gt; rpush mylist2 &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist2 &quot;world&quot;(integer) 2127.0.0.1:6379&gt; lrange mylist2 0 -11) &quot;hello&quot;2) &quot;world&quot; 在此处我们先插入了一个hello，然后在hello的尾部插入了一个world。 linsert 在key对应list的特定位置之前或之后添加字符串元素。 12345678910127.0.0.1:6379&gt; rpush mylist3 &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist3 &quot;world&quot;(integer) 2127.0.0.1:6379&gt; linsert mylist3 before &quot;world&quot; &quot;there&quot;(integer) 3127.0.0.1:6379&gt; lrange mylist3 0 -11) &quot;hello&quot;2) &quot;there&quot;3) &quot;world&quot; 在此处我们先插入了一个hello，然后在hello的尾部插入了一个world，然后又在world的前面插入了there。 lset 设置list中指定下标的元素值(下标从0开始) 。 1234567891011121314127.0.0.1:6379&gt; rpush mylist4 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist4 &quot;two&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist4 &quot;three&quot;(integer) 3127.0.0.1:6379&gt; lset mylist4 0 &quot;four&quot;OK127.0.0.1:6379&gt; lset mylist4 -2 &quot;five&quot;OK127.0.0.1:6379&gt; lrange mylist4 0 -11) &quot;four&quot;2) &quot;five&quot;3) &quot;three&quot; 在此处我们依次插入了one,two,three，然后将标是0的值设置为four，再将下标是-2的值设置为five。 lrem 从key对应list中删除count个和value相同的元素。 count&gt;0时，按从头到尾的顺序删除。 12345678910111213127.0.0.1:6379&gt; rpush mylist5 &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist5 &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist5 &quot;foo&quot;(integer) 3127.0.0.1:6379&gt; rpush mylist5 &quot;hello&quot;(integer) 4127.0.0.1:6379&gt; lrem mylist5 2 &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; lrange mylist5 0 -11) &quot;foo&quot;2) &quot;hello&quot; count&lt;0时，按从尾到头的顺序删除。 12345678910111213127.0.0.1:6379&gt; rpush mylist6 &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist6 &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist6 &quot;foo&quot;(integer) 3127.0.0.1:6379&gt; rpush mylist6 &quot;hello&quot;(integer) 4127.0.0.1:6379&gt; lrem mylist6 -2 &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; lrange mylist6 0 -11) &quot;hello&quot;2) &quot;foo&quot; count=0时，删除全部。 123456789101112127.0.0.1:6379&gt; rpush mylist7 &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist7 &quot;hello&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist7 &quot;foo&quot;(integer) 3127.0.0.1:6379&gt; rpush mylist7 &quot;hello&quot;(integer) 4127.0.0.1:6379&gt; lrem mylist7 0 &quot;hello&quot;(integer) 3127.0.0.1:6379&gt; lrange mylist7 0 -11) &quot;foo&quot; ltrim 保留指定key 的值范围内的数据。 1234567891011121314127.0.0.1:6379&gt; rpush mylist8 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist8 &quot;two&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist8 &quot;three&quot;(integer) 3127.0.0.1:6379&gt; rpush mylist8 &quot;four&quot;(integer) 4127.0.0.1:6379&gt; ltrim mylist8 1 -1OK127.0.0.1:6379&gt; lrange mylist8 0 -11) &quot;two&quot;2) &quot;three&quot;3) &quot;four&quot; lpop 从list的头部删除元素，并返回删除元素。 1234567127.0.0.1:6379&gt; lrange mylist 0 -11) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; lpop mylist&quot;hello&quot;127.0.0.1:6379&gt; lrange mylist 0 -11) &quot;world&quot; rpop 从list的尾部删除元素，并返回删除元素。 1234567127.0.0.1:6379&gt; lrange mylist2 0 -11) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; rpop mylist2&quot;world&quot;127.0.0.1:6379&gt; lrange mylist2 0 -11) &quot;hello&quot; rpoplpush 从第一个list的尾部移除元素并添加到第二个list的头部,最后返回被移除的元素值，整个操作是原子的。如果第一个list是空或者不存在返回nil。 1234567891011121314127.0.0.1:6379&gt; lrange mylist5 0 -11) &quot;foo&quot;2) &quot;hello&quot;127.0.0.1:6379&gt; lrange mylist6 0 -11) &quot;hello&quot;2) &quot;foo&quot;127.0.0.1:6379&gt; rpoplpush mylist5 mylist6&quot;hello&quot;127.0.0.1:6379&gt; lrange mylist5 0 -11) &quot;foo&quot;127.0.0.1:6379&gt; lrange mylist6 0 -11) &quot;hello&quot;2) &quot;hello&quot;3) &quot;foo&quot; lindex 返回名称为key的list中index位置的元素。 1234567127.0.0.1:6379&gt; lrange mylist5 0 -11) &quot;three&quot;2) &quot;foo&quot;127.0.0.1:6379&gt; lindex mylist5 0&quot;three&quot;127.0.0.1:6379&gt; lindex mylist5 1&quot;foo&quot; llen 返回key对应list的长度。 12127.0.0.1:6379&gt; llen mylist5(integer) 2 Sets （不可重复集合）Redis的set是string类型的无序集合。set元素最大可以包含(2^32)个元素。 set的是通过hash table实现的，所以添加、删除和查找的复杂度都是O(1)。hash table会随着添加或者删除自动的调整大小。 sadd 向名称为key的set中添加元素。 123456789127.0.0.1:6379&gt; sadd myset &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;world&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;world&quot;(integer) 0127.0.0.1:6379&gt; smembers myset1) &quot;world&quot;2) &quot;hello&quot; 本例中，我们向myset中添加了三个元素，但由于第三个元素跟第二个元素是相同的，所以第三个元素没有添加成功，最后我们用smembers来查看myset中的所有元素。 srem 删除名称为key的set中的元素member。 12345678910111213127.0.0.1:6379&gt; sadd myset2 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; sadd myset2 &quot;two&quot;(integer) 1127.0.0.1:6379&gt; sadd myset2 &quot;three&quot;(integer) 1127.0.0.1:6379&gt; srem myset2 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; srem myset2 &quot;four&quot;(integer) 0127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot; 本例中，我们向myset2中添加了三个元素后，再调用srem来删除one和four，但由于元素中没有four所以，此条srem命令执行失败。 spop 随机返回并删除名称为key的set中一个元素。 1234567891011127.0.0.1:6379&gt; sadd myset3 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; sadd myset3 &quot;two&quot;(integer) 1127.0.0.1:6379&gt; sadd myset3 &quot;three&quot;(integer) 1127.0.0.1:6379&gt; spop myset3&quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot; 本例中，我们向myset3中添加了三个元素后，再调用spop来随机删除一个元素，可以看到three元素被删除了。 sdiff 返回所有给定key与第一个key的差集。 12345678127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sdiff myset2 myset31) &quot;two&quot; 本例中，我们可以看到myset2中的元素与myset3中不同的只是three，所以只有three被查出来了，而不是three和one，因为one是myset3的元素。 我们也可以将myset2和myset3换个顺序来看一下结果： 12127.0.0.1:6379&gt; sdiff myset3 myset21) &quot;one&quot; 这个结果中只显示了，myset3中的元素与myset2中不同的元素。 sdiffstore返回所有给定key与第一个key的差集，并将结果存为另一个key。 12345678910127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sdiffstore myset4 myset2 myset3(integer) 1127.0.0.1:6379&gt; smembers myset41) &quot;two&quot; sinter 返回所有给定key的交集。 12345678127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sinter myset2 myset31) &quot;three&quot; 通过本例的结果可以看出, myset2和myset3的交集two被查出来了。 sinterstore 返回所有给定key的交集，并将结果存为另一个key。 12345678910127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sinterstore myset5 myset2 myset3(integer) 1127.0.0.1:6379&gt; smembers myset51) &quot;three&quot; 通过本例的结果可以看出, myset2和myset3的交集被保存到myset5中了。 sunion 返回所有给定key的并集。 12345678910127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sunion myset2 myset31) &quot;three&quot;2) &quot;one&quot;3) &quot;two&quot; 通过本例的结果可以看出, myset2和myset3的并集被查出来了。 sunionstore 返回所有给定key的并集，并将结果存为另一个key。 123456789101112127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; sunionstore myset6 myset2 myset3(integer) 3127.0.0.1:6379&gt; smembers myset61) &quot;three&quot;2) &quot;one&quot;3) &quot;two&quot; 通过本例的结果可以看出, myset2和myset3的并集被保存到myset6中了。 smove 从第一个key对应的set中移除member并添加到第二个对应set中。 1234567127.0.0.1:6379&gt; smembers myset21) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; smove myset2 myset7 three(integer) 1127.0.0.1:6379&gt; smembers myset71) &quot;three&quot; 通过本例可以看到，myset2的three被移到myset7中了。 scard 返回名称为key的set的元素个数。 12127.0.0.1:6379&gt; scard myset2(integer) 1 sismember 测试member是否是名称为key的set的元素。 123456127.0.0.1:6379&gt; smembers myset21) &quot;two&quot;127.0.0.1:6379&gt; sismember myset2 two(integer) 1127.0.0.1:6379&gt; sismember myset2 one(integer) 0 通过本例可以看到，two是myset2的成员，而one不是。 srandmember 随机返回名称为key的set的一个元素，但是不删除元素。 123456789127.0.0.1:6379&gt; smembers myset31) &quot;three&quot;2) &quot;one&quot;127.0.0.1:6379&gt; srandmember myset3&quot;three&quot;127.0.0.1:6379&gt; srandmember myset3&quot;one&quot;127.0.0.1:6379&gt; srandmember myset3&quot;one&quot; 通过本例可以看到，第二次返回了元素”one”，但是并没有删除”one”元素。 Sorted sets （不可重复有序集合）sorted set是set的一个升级版本，它在set的基础上增加了一个顺序属性，这一属性在添加修改元素的时候可以指定，每次指定后，zset会自动重新按新的值调整顺序。可以理解为有两列的mysql表，一列存value，一列存顺序。 和set一样sorted set也是string类型元素的集合，不同的是每个元素都会关联一个double类型的score。sorted set的实现是skip list和hash table的混合体。 zadd 向名称为key的zset中添加元素member，score用于排序。如果该元素已经存在，则根据score更新该元素的顺序。 1234567891011127.0.0.1:6379&gt; zadd myzset 1 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset 2 &quot;two&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset 3 &quot;two&quot;(integer) 0127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;3&quot; zrem 删除名称为key的zset中的元素member。 12345678910127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;3&quot;127.0.0.1:6379&gt; zrem myzset two(integer) 1127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot; 可以看到two被删除了。 zincrby 如果在名称为key的zset中已经存在元素member，则该元素的score增加increment；否则向集合中添加该元素，其score的值为increment。 1234567891011127.0.0.1:6379&gt; zadd myzset2 1 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset2 2 &quot;two&quot;(integer) 1127.0.0.1:6379&gt; zincrby myzset2 2 &quot;one&quot;&quot;3&quot;127.0.0.1:6379&gt; zrange myzset2 0 -1 withscores1) &quot;two&quot;2) &quot;2&quot;3) &quot;one&quot;4) &quot;3&quot; 本例中将one的score从1增加了2，增加到了3。 zrank 返回名称为key的zset中member元素的排名(按score从小到大排序)即下标。 12345678910111213141516171819127.0.0.1:6379&gt; zadd myzset3 1 &quot;one&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset3 2 &quot;two&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset3 3 &quot;three&quot;(integer) 1127.0.0.1:6379&gt; zadd myzset3 5 &quot;five&quot;(integer) 1127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;2&quot;5) &quot;three&quot;6) &quot;3&quot;7) &quot;five&quot;8) &quot;5&quot;127.0.0.1:6379&gt; zrank myzset3 two(integer) 1 zrevrank 返回名称为key的zset中member元素的排名(按score从大到小排序)即下标。 1234567891011127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;2&quot;5) &quot;three&quot;6) &quot;3&quot;7) &quot;five&quot;8) &quot;5&quot;127.0.0.1:6379&gt; zrevrank myzset3 two(integer) 2 按从大到小排序的话two是第三个元素，下标是2。 zrevrange 返回名称为key的zset（按score从大到小排序）中的index从start到end的所有元素。 123456789127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;five&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;3&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;one&quot;8) &quot;1&quot; zrangebyscore 返回集合中score在给定区间的元素。 1234567891011121314127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;2&quot;5) &quot;three&quot;6) &quot;3&quot;7) &quot;five&quot;8) &quot;5&quot;127.0.0.1:6379&gt; zrangebyscore myzset3 2 3 withscores1) &quot;two&quot;2) &quot;2&quot;3) &quot;three&quot;4) &quot;3&quot; zcount 返回集合中score在给定区间的数量。 1234567891011127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;five&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;3&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;one&quot;8) &quot;1&quot;127.0.0.1:6379&gt; zcount myzset3 2 3(integer) 2 zcard 返回集合中元素个数。 1234567891011127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;five&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;3&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;one&quot;8) &quot;1&quot;127.0.0.1:6379&gt; zcard myzset3(integer) 4 zscore 返回给定元素对应的score。 1234567891011127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;five&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;3&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;one&quot;8) &quot;1&quot;127.0.0.1:6379&gt; zscore myzset3 two&quot;2&quot; zremrangebyrank 删除集合中排名在给定区间的元素。 123456789101112131415161718127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;five&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;3&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;one&quot;8) &quot;1&quot;127.0.0.1:6379&gt; zremrangebyrank myzset3 3 3(integer) 1127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;three&quot;2) &quot;3&quot;3) &quot;two&quot;4) &quot;2&quot;5) &quot;one&quot;6) &quot;1&quot; 在本例中我们将myzset3中按从小到大排序结果的下标为3的元素删除了。 zremrangebyscore 删除集合中score在给定区间的元素。 123456789101112127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;three&quot;2) &quot;3&quot;3) &quot;two&quot;4) &quot;2&quot;5) &quot;one&quot;6) &quot;1&quot;127.0.0.1:6379&gt; zremrangebyscore myzset3 1 2(integer) 2127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) &quot;three&quot;2) &quot;3&quot; 在本例中我们将myzset3中按从小到大排序结果的score在1~2之间的元素删除了。 Hashes （哈希表）Redis hash是一个string类型的field和value的映射表。它的添加、删除操作都是O(1)（平均），hash特别适合用于存储对象。 相较于将对象的每个字段存成单个string类型，将一个对象存储在hash类型中会占用更少的内存，并且可以更方便的存取整个对象。 hset 设置hash field为指定值，如果key不存在，则先创建。 12127.0.0.1:6379&gt; hset myhash field1 Hello(integer) 1 hsetnx 设置hash field为指定值，如果key不存在，则先创建。如果field已经存在，返回0，nx是not exist的意思。 1234127.0.0.1:6379&gt; hsetnx myhash field &quot;Hello&quot;(integer) 1127.0.0.1:6379&gt; hsetnx myhash field &quot;Hello&quot;(integer) 0 第一次执行是成功的，但第二次执行相同的命令失败，原因是field已经存在了。 hmset 同时设置hash的多个field。 12127.0.0.1:6379&gt; hmset myhash field1 Hello field2 WorldOK hget 获取指定的hash field。 123456127.0.0.1:6379&gt; hget myhash field1&quot;Hello&quot;127.0.0.1:6379&gt; hget myhash field2&quot;World&quot;127.0.0.1:6379&gt; hget myhash field3(nil) 由于数据库没有field3，所以取到的是一个空值nil。 hmget 获取全部指定的hash filed。 1234127.0.0.1:6379&gt; hmget myhash field1 field2 field31) &quot;Hello&quot;2) &quot;World&quot;3) (nil) 由于数据库没有field3，所以取到的是一个空值nil。 hincrby 给指定的hash filed 加上给定值。 12345678127.0.0.1:6379&gt; hset myhash field3 20(integer) 1127.0.0.1:6379&gt; hget myhash field3 &quot;20&quot;127.0.0.1:6379&gt; hincrby myhash field3 -8(integer) 12127.0.0.1:6379&gt; hget myhash field3&quot;12&quot; 在本例中我们将field3的值从20降到了12，即做了一个减8的操作。 hexists 测试指定field是否存在。 1234127.0.0.1:6379&gt; hexists myhash field1(integer) 1127.0.0.1:6379&gt; hexists myhash field9(integer) 0 通过上例可以说明field1存在，但field9是不存在的。 hlen 返回指定hash的field数量。 12127.0.0.1:6379&gt; hlen myhash(integer) 4 通过上例可以看到myhash中有4个field。 hdel 删除指定hash的指定field。 123456127.0.0.1:6379&gt; hlen myhash(integer) 4127.0.0.1:6379&gt; hdel myhash field1(integer) 1127.0.0.1:6379&gt; hlen myhash(integer) 3 hkeys 返回hash的所有field。 1234127.0.0.1:6379&gt; hkeys myhash1) &quot;field&quot;2) &quot;field2&quot;3) &quot;field3&quot; hvals 返回hash的所有value。 1234127.0.0.1:6379&gt; hvals myhash1) &quot;Hello&quot;2) &quot;World&quot;3) &quot;12&quot; hgetall 获取某个hash中全部的filed及value。 1234567127.0.0.1:6379&gt; hgetall myhash1) &quot;field&quot;2) &quot;Hello&quot;3) &quot;field2&quot;4) &quot;World&quot;5) &quot;field3&quot;6) &quot;12&quot; 一下子将myhash中所有的field及对应的value都取出来了。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://gcdd1993.github.io/tags/Redis/"},{"name":"NoSql","slug":"NoSql","permalink":"https://gcdd1993.github.io/tags/NoSql/"}]},{"title":"Ubuntu切换为阿里镜像源","date":"2019-05-25T15:45:14.000Z","path":"p/12805/","text":"前言在VM虚拟机搭建Ubuntu系统学习或者测试时，常常要使用apt安装测试，但是由于系统自带的下载源在国外服务器上，下载速度慢的无法忍受。所以我们需要切换为国内镜像源，能显著加快安装包下载速度。 步骤123456$ cd /etc/apt/$ cp sources.list sources.list.bak ## 备份系统自带的source列表## 选择合适的镜像源，如阿里云的镜像 http://mirrors.aliyun.com/ubuntu$ sed -i &#x27;s/^\\(deb\\|deb-src\\) \\([^ ]*\\) \\(.*\\)/\\1 http:\\/\\/mirrors.aliyun.com\\/ubuntu \\3/&#x27; sources.list## 更新apt$ apt-get update 国内镜像源 名称 地址 阿里镜像源 http://mirrors.aliyun.com/ubuntu 清华大学镜像源 https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ 网易镜像源 https://mirrors.163.com/ubuntu/ 东北大学镜像源 http://mirror.neu.edu.cn/ubuntu/ 至于哪个源比较快，看个人的网络吧，可以自行测试下。本人使用的是阿里镜像源。","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://gcdd1993.github.io/tags/Ubuntu/"}]},{"title":"Cassandra学习笔记","date":"2019-05-22T07:42:04.000Z","path":"p/60138/","text":"准备按照Cassandra集群部署搭建两台测试机，环境信息如下： 名称 IP 数据中心名称 node-01 192.168.198.130 datacenter1 node-02 192.168.198.131 datacenter1 Keyspace创建Keyspace1create_keyspace_statement ::= CREATE KEYSPACE [ IF NOT EXISTS ] keyspace_name WITH options 示例： 1234567891011121314## 使用SimpleStrategy复制策略CREATE KEYSPACE excelsior WITH replication = &#123;&#x27;class&#x27;: &#x27;SimpleStrategy&#x27;, &#x27;replication_factor&#x27; : 3&#125;;## 使用NetworkTopologyStrategy复制策略# 1. 确认分区名称$ nodetool statusDatacenter: datacenter1...# 2. 使用NetworkTopologyStrategy复制策略创建keyspaceCREATE KEYSPACE excalibur WITH replication = &#123;&#x27;class&#x27;: &#x27;NetworkTopologyStrategy&#x27;, &#x27;DC1&#x27; : 1, &#x27;DC2&#x27; : 3&#125; AND durable_writes = false; 使用Keyspace1use_statement ::= USE keyspace_name 修改Keyspace（replication factor）1alter_keyspace_statement ::= ALTER KEYSPACE keyspace_name WITH options 示例： 12ALTER KEYSPACE excelsior WITH replication = &#123;&#x27;class&#x27;: &#x27;SimpleStrategy&#x27;, &#x27;replication_factor&#x27; : 4&#125;; 查看Keyspace1DESCRIBE KEYSPACE &lt;keyspace name&gt;; 使用该语句查看创建的键空间是否正确： 123DESCRIBE KEYSPACE excelsior;CREATE KEYSPACE excelsior WITH replication = &#123;&#x27;class&#x27;: &#x27;SimpleStrategy&#x27;, &#x27;replication_factor&#x27; : 3&#125; AND durable_writes = true; 删除Keyspace1drop_keyspace_statement ::= DROP KEYSPACE [ IF EXISTS ] keyspace_name 1234DROP KEYSPACE excelsior;DESCRIBE excelsior;&#x27;excelsior&#x27; not found in keyspaces Table创建Table123456789101112131415create_table_statement ::= CREATE TABLE [ IF NOT EXISTS ] table_name &#x27;(&#x27; column_definition ( &#x27;,&#x27; column_definition )* [ &#x27;,&#x27; PRIMARY KEY &#x27;(&#x27; primary_key &#x27;)&#x27; ] &#x27;)&#x27; [ WITH table_options ]column_definition ::= column_name cql_type [ STATIC ] [ PRIMARY KEY]primary_key ::= partition_key [ &#x27;,&#x27; clustering_columns ]partition_key ::= column_name | &#x27;(&#x27; column_name ( &#x27;,&#x27; column_name )* &#x27;)&#x27;clustering_columns ::= column_name ( &#x27;,&#x27; column_name )*table_options ::= COMPACT STORAGE [ AND table_options ] | CLUSTERING ORDER BY &#x27;(&#x27; clustering_order &#x27;)&#x27; [ AND table_options ] | optionsclustering_order ::= column_name (ASC | DESC) ( &#x27;,&#x27; column_name (ASC | DESC) )* 创建Table必须指定主键，主键是用于在表中唯一标识某一行，可以是一列或多列。 示例，在excelsior键空间创建一张名为excelsior_alt_stats 的表： 12345678CREATE TABLE excelsior.excelsior_alt_stats ( id UUID PRIMARY KEY, lastname text, birthday timestamp, nationality text, weight text, height text); cassandra还支持collection（map, set, 或者 list）类型作为列： 1234567CREATE TABLE excelsior.whimsey ( id UUID PRIMARY KEY, lastname text, excelsior_teams set&lt;text&gt;, events list&lt;text&gt;, teams map&lt;int,text&gt; ); 甚至是嵌套的元组类型（tuple）： 1234567CREATE TABLE excelsior.route ( race_id int, race_name text, point_id int, lat_long tuple&lt;text, tuple&lt;float,float&gt;&gt;, PRIMARY KEY (race_id, point_id)); 更多数据类型请参阅下一节Cassandra数据结构 静态列某些列可以在表定义中声明为STATIC。静态的列将由属于同一分区（具有相同分区键）的所有行“共享”。例如： 123456789101112131415161718CREATE TABLE t ( pk int, t int, v text, s text static, PRIMARY KEY (pk, t));INSERT INTO t (pk, t, v, s) VALUES (0, 0, &#x27;val0&#x27;, &#x27;static0&#x27;);INSERT INTO t (pk, t, v, s) VALUES (0, 1, &#x27;val1&#x27;, &#x27;static1&#x27;);SELECT * FROM t; pk | t | v | s ----+---+--------+----------- 0 | 0 | &#x27;val0&#x27; | &#x27;static1&#x27; 0 | 1 | &#x27;val1&#x27; | &#x27;static1&#x27; ## 所有记录中的静态列将永远展示最后一次更新的值 修改Table1234alter_table_statement ::= ALTER TABLE table_name alter_table_instructionalter_table_instruction ::= ADD column_name cql_type ( &#x27;,&#x27; column_name cql_type )* | DROP column_name ( column_name )* | WITH options 示例： 1234ALTER TABLE addamsFamily ADD gravesite varchar;ALTER TABLE addamsFamily WITH comment = &#x27;A most excellent and useful table&#x27;; 修改Table可以： 向表中添加新列（通过ADD指令）。请注意，无法更改表的主键，因此新添加的列将不会成为主键的一部分。 从表中删除列。这会丢弃列及其所有内容。 更改一些表选项（通过WITH指令）。支持的选项与创建表时相同（在创建后无法更改的COMPACT STORAGE和CLUSTERING ORDER之外）。 删除Table1drop_table_statement ::= DROP TABLE [ IF EXISTS ] table_name 截断Table（清空表数据）1truncate_statement ::= TRUNCATE [ TABLE ] table_name 由于表是唯一可以在当前截断的对象，因此可以省略TABLE关键字。 截断表会永久删除表中的所有现有数据，但不会删除表本身。 Cassandra数据结构 CQL是一种类型化语言，支持丰富的数据类型集，包括本地类型，集合类型，用户定义类型，元组类型和自定义类型： 1cql_type ::= native_type | collection_type | user_defined_type | tuple_type | custom_type 本地类型（Native Types） 类型 常量支持 说明 ascii string ASCII字符串 bigint integer 64位无符号整数 blob blob 任意字节（无验证） boolean boolean true或false counter integer 计数器列（64位有符号值） date integer， string 日期（没有相应的时间值） decimal integer， float 十进制可变精度 double integer float 64位IEEE-754浮点 duration duration 持续时间（纳秒精度） float integer， float 32位IEEE-754浮点 inet string IP地址，IPv4（4字节长）或IPv6（16字节长） int integer 32位无符号整数 smallint integer 16位有符号整数 text string UTF8编码的字符串 time integer， string 具有纳秒精度的时间（没有相应的日期值） timestamp integer， string 时间戳（日期和时间），精度为毫秒 timeuuid uuid UUID（版本1），通常用作“无冲突”时间戳 tinyint integer 8位有符号整数 uuid uuid 一个UUID（任何版本） varchar string UTF8编码的字符串 varint integer 任意精度整数 其中需要注意的是时间类型： timestamps 时间戳类型的值被编码为64位有符号整数，表示自标准基准时间（称为纪元：1970年1月1日格林威治标准时间00:00:00）以来的毫秒数。 1299038700000 &#39;2011-02-03 04:05+0000&#39; &#39;2011-02-03 04:05:00+0000&#39; &#39;2011-02-03 04:05:00.000+0000&#39; &#39;2011-02-03T04:05+0000&#39; &#39;2011-02-03T04:05:00+0000&#39; &#39;2011-02-03T04:05:00.000+0000&#39; 例如： 123SELECT *FROM pointWHERE ts = &#x27;2018-11-15 00:00:30.557+0000&#x27;; 或者 123SELECT *FROM pointWHERE ts = 1542211230557; 其中，+0000是RFC 822 4-digit时区规范，+0000指GMT。美国太平洋标准时间为-0800，中国北京标准时间为+8000，官方建议每次插入查询都带上时区，不加的话，默认是使用Cassandra节点配置的时区，可能会出现时区不一致导致的查询失败问题。 dates 日期类型的值被编码为32位无符号整数，表示在该范围的中心处具有“纪元”的天数（2^31）。大纪元是1970年1月1日。 至于时间戳，日期可以作为整数或使用日期字符串输入。在后一种情况下，格式应为yyyy-mm-dd（例如’2011-02-03’）。 times 时间类型的值被编码为64位有符号整数，表示自午夜以来的纳秒数。 对于时间戳，可以以整数或表示时间的字符串的形式输入时间。在后一种情况下，格式应为hh:mm:ss [.fffffffff]（其中亚秒精度是可选的，如果提供，则可以小于纳秒）。例如，以下是一段时间内的有效输入： &#39;08:12:54&#39; &#39;08:12:54.123&#39; &#39;08:12:54.123456&#39; &#39;08:12:54.123456789&#39; durations 持续时间类型的值被编码为3个有符号整数的可变长度。这是因为一个月的天数可以改变，一天可以有23或25小时，具体取决于夏令时。 第一个整数表示月数（32位整数） 第二个表示天数（32位整数） 第三个表示纳秒数（64位整数） 支持的单位： y: 年(12 月) mo: 月 (1 月) w: 周(7 天) d: 天(1 天) h: 小时(3,600,000,000,000 纳秒) m: 分钟(60,000,000,000 纳) s: 秒(1,000,000,000 纳) ms: 毫秒(1,000,000 纳) us or µs : 微妙(1000 纳) ns: 纳秒(1 纳) ISO 8601格式：P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W ISO 8601替代格式：P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss] 插入示例： 123INSERT INTO RiderResults (rider, race, result) VALUES (&#x27;Christopher Froome&#x27;, &#x27;Tour de France&#x27;, 89h4m48s);INSERT INTO RiderResults (rider, race, result) VALUES (&#x27;BARDET Romain&#x27;, &#x27;Tour de France&#x27;, PT89H8M53S);INSERT INTO RiderResults (rider, race, result) VALUES (&#x27;QUINTANA Nairo&#x27;, &#x27;Tour de France&#x27;, P0000-00-00T89:09:09); 持续时间列不能作为主键。这是由于无法精确确认持续时间。如果没有日期上下文，实际上不可能知道1个月是否大于29天。 1天的持续时间也不等于24h，因为持续时间类型需要支持夏令时。 集合类型（Collections）cassandra支持三种类型的集合：Maps, Sets and Lists 123collection_type ::= MAP &#x27;&lt;&#x27; cql_type &#x27;,&#x27; cql_type &#x27;&gt;&#x27; | SET &#x27;&lt;&#x27; cql_type &#x27;&gt;&#x27; | LIST &#x27;&lt;&#x27; cql_type &#x27;&gt;&#x27; 可以这样输入集合类型的数据： 1234collection_literal ::= map_literal | set_literal | list_literalmap_literal ::= &#x27;&#123;&#x27; [ term &#x27;:&#x27; term (&#x27;,&#x27; term : term)* ] &#x27;&#125;&#x27;set_literal ::= &#x27;&#123;&#x27; [ term (&#x27;,&#x27; term)* ] &#x27;&#125;&#x27;list_literal ::= &#x27;[&#x27; [ term (&#x27;,&#x27; term)* ] &#x27;]&#x27; Maps Maps是一组（有序）键值对，其中键是唯一的，并且按其键排序。 1234567891011CREATE TABLE users ( id text PRIMARY KEY, name text, favs map&lt;text, text&gt; // A map of text keys, and text values);INSERT INTO users (id, name, favs) VALUES (&#x27;jsmith&#x27;, &#x27;John Smith&#x27;, &#123; &#x27;fruit&#x27; : &#x27;Apple&#x27;, &#x27;band&#x27; : &#x27;Beatles&#x27; &#125;);// Replace the existing map entirely.UPDATE users SET favs = &#123; &#x27;fruit&#x27; : &#x27;Banana&#x27; &#125; WHERE id = &#x27;jsmith&#x27;; 另外，Maps还具有一些高级特性： 更新或插入一个或多个元素 12UPDATE users SET favs[&#x27;author&#x27;] = &#x27;Ed Poe&#x27; WHERE id = &#x27;jsmith&#x27;;UPDATE users SET favs = favs + &#123; &#x27;movie&#x27; : &#x27;Cassablanca&#x27;, &#x27;band&#x27; : &#x27;ZZ Top&#x27; &#125; WHERE id = &#x27;jsmith&#x27;; 删除一个或多个元素（如果一个元素不存在，删除它是一个无效操作但不会抛出错误） 12DELETE favs[&#x27;author&#x27;] FROM users WHERE id = &#x27;jsmith&#x27;;UPDATE users SET favs = favs - &#123; &#x27;movie&#x27;, &#x27;band&#x27;&#125; WHERE id = &#x27;jsmith&#x27;; Sets Sets是唯一值的（已排序）集合。 1234567891011CREATE TABLE users ( id text PRIMARY KEY, name text, favs map&lt;text, text&gt; // A map of text keys, and text values);INSERT INTO users (id, name, favs) VALUES (&#x27;jsmith&#x27;, &#x27;John Smith&#x27;, &#123; &#x27;fruit&#x27; : &#x27;Apple&#x27;, &#x27;band&#x27; : &#x27;Beatles&#x27; &#125;);// Replace the existing map entirely.UPDATE users SET favs = &#123; &#x27;fruit&#x27; : &#x27;Banana&#x27; &#125; WHERE id = &#x27;jsmith&#x27;; 另外，Sets也具有一些高级特性： 添加一个或多个元素（因为这是一个集合，插入一个已存在的元素是一个无效操作） 1UPDATE images SET tags = tags + &#123; &#x27;gray&#x27;, &#x27;cuddly&#x27; &#125; WHERE name = &#x27;cat.jpg&#x27;; 删除一个或多个元素（如果一个元素不存在，删除它是一个无效操作但不会抛出错误） 1UPDATE images SET tags = tags - &#123; &#x27;cat&#x27; &#125; WHERE name = &#x27;cat.jpg&#x27;; Lists Lists是非唯一值的（已排序）集合，其中元素按列表中的位置排序。它与Sets的区别就在于是否是唯一值。 123456789101112CREATE TABLE plays ( id text PRIMARY KEY, game text, players int, scores list&lt;int&gt; // A list of integers)INSERT INTO plays (id, game, players, scores) VALUES (&#x27;123-afde&#x27;, &#x27;quake&#x27;, 3, [17, 4, 2]);// Replace the existing list entirelyUPDATE plays SET scores = [ 3, 9, 4] WHERE id = &#x27;123-afde&#x27;; 另外，Lists同样也具有一些高级特性： 在列表头或尾添加元素 12UPDATE plays SET players = 5, scores = scores + [ 14, 21 ] WHERE id = &#x27;123-afde&#x27;;UPDATE plays SET players = 6, scores = [ 3 ] + scores WHERE id = &#x27;123-afde&#x27;; 💡该操作不是幂等的，特别是在其中一个操作超时时，重试操作是不安全的，可能会导致同一数据插入两次。 在列表中指定下标处设置值。该列表必须长度大于此下标，否则将抛出列表太小的错误 1UPDATE plays SET scores[1] = 7 WHERE id = &#x27;123-afde&#x27;; 通过列表指定下标删除元素。该列表必须长度大于此下标，否则将抛出列表太小的错误。此外，当操作从列表中删除元素时，列表大小将减1，从而改变此下标之后所有元素的位置 1DELETE scores[1] FROM plays WHERE id = &#x27;123-afde&#x27;; 删除列表中指定下标之间的所有元素 1UPDATE plays SET scores = scores - [ 12, 21 ] WHERE id = &#x27;123-afde&#x27;; 💡以上2,3,4操作会出现内部的 read-before-write，会比通常的更新消耗更多的资源，所以尽量使用Sets代替Lists。 用户自定义类型（User-Defined Types） CQL支持用户定义类型（以下简称UDT）。可以使用下面create_type_statement，alter_type_statement和drop_type_statement创建，修改和删除此类型。 12user_defined_type ::= udt_nameudt_name ::= [ keyspace_name &#x27;.&#x27; ] identifier 创建123create_type_statement ::= CREATE TYPE [ IF NOT EXISTS ] udt_name &#x27;(&#x27; field_definition ( &#x27;,&#x27; field_definition )* &#x27;)&#x27;field_definition ::= identifier cql_type UDT有一个名称（用于声明该类型的列），是一组命名和类型字段。字段名称可以是任何类型，包括集合或其他UDT。例如： 12345678910111213141516CREATE TYPE phone ( country_code int, number text,)CREATE TYPE address ( street text, city text, zip text, phones map&lt;text, phone&gt;)CREATE TABLE user ( name text PRIMARY KEY, addresses map&lt;text, frozen&lt;address&gt;&gt;) 💡注意 尝试创建现有类型时请使用IF NOT EXISTS选项，否则将会抛出错误。 UDT本质上绑定到创建它的键空间，并且只能在该键空间中使用。在创建时，如果类型名称以键空间名称为前缀，则在该键空间中创建它。否则，它将在当前键空间中创建。 从Cassandra 4.0开始，在大多数情况下必须冻结UDT，因此在上面的表定义中冻结了&lt;address&gt;。有关详细信息，请参阅冻结部分。 修改123alter_type_statement ::= ALTER TYPE udt_name alter_type_modificationalter_type_modification ::= ADD field_definition | RENAME identifier TO identifier ( identifier TO identifier )* 修改一个UDT，可以： 在类型中添加一个新字段 1ALTER TYPE address ADD country text 请注意：新添加的字段在之前的记录中，都将被置为NULL。 重命名该类型的字段 1ALTER TYPE address RENAME zip TO zipcode 删除1drop_type_statement ::= DROP TYPE [ IF EXISTS ] udt_name 使用1udt_literal ::= &#x27;&#123;&#x27; identifier &#x27;:&#x27; term ( &#x27;,&#x27; identifier &#x27;:&#x27; term )* &#x27;&#125;&#x27; 使用UDT有点像Maps，例如 12345678910111213141516INSERT INTO user (name, addresses) VALUES (&#x27;z3 Pr3z1den7&#x27;, &#123; &#x27;home&#x27; : &#123; street: &#x27;1600 Pennsylvania Ave NW&#x27;, city: &#x27;Washington&#x27;, zip: &#x27;20500&#x27;, phones: &#123; &#x27;cell&#x27; : &#123; country_code: 1, number: &#x27;202 456-1111&#x27; &#125;, &#x27;landline&#x27; : &#123; country_code: 1, number: &#x27;...&#x27; &#125; &#125; &#125;, &#x27;work&#x27; : &#123; street: &#x27;1600 Pennsylvania Ave NW&#x27;, city: &#x27;Washington&#x27;, zip: &#x27;20500&#x27;, phones: &#123; &#x27;fax&#x27; : &#123; country_code: 1, number: &#x27;...&#x27; &#125; &#125; &#125; &#125;) 元组（Tuples） CQL还支持元组和元组类型（元素可以是不同类型），类似于匿名的UDT或者是Scala的Tuple类型。 12tuple_type ::= TUPLE &#x27;&lt;&#x27; cql_type ( &#x27;,&#x27; cql_type )* &#x27;&gt;&#x27;tuple_literal ::= &#x27;(&#x27; term ( &#x27;,&#x27; term )* &#x27;)&#x27; 例如： 123456CREATE TABLE durations ( event text, duration tuple&lt;int, text&gt;,)INSERT INTO durations (event, duration) VALUES (&#x27;ev1&#x27;, (3, &#x27;hours&#x27;)); 自定义类型（Custom Types） 自定义类型主要是为了兼容老项目，不建议使用。使用已有的类型加上用户自定义类型（UDT）就够了。 1custom_type ::= string 数据增删改查（CRUD）SELECT123456789101112131415161718192021select_statement ::= SELECT [ JSON | DISTINCT ] ( select_clause | &#x27;*&#x27; ) FROM table_name [ WHERE where_clause ] [ GROUP BY group_by_clause ] [ ORDER BY ordering_clause ] [ PER PARTITION LIMIT (integer | bind_marker) ] [ LIMIT (integer | bind_marker) ] [ ALLOW FILTERING ]select_clause ::= selector [ AS identifier ] ( &#x27;,&#x27; selector [ AS identifier ] )selector ::= column_name | term | CAST &#x27;(&#x27; selector AS cql_type &#x27;)&#x27; | function_name &#x27;(&#x27; [ selector ( &#x27;,&#x27; selector )* ] &#x27;)&#x27; | COUNT &#x27;(&#x27; &#x27;*&#x27; &#x27;)&#x27;where_clause ::= relation ( AND relation )*relation ::= column_name operator term &#x27;(&#x27; column_name ( &#x27;,&#x27; column_name )* &#x27;)&#x27; operator tuple_literal TOKEN &#x27;(&#x27; column_name ( &#x27;,&#x27; column_name )* &#x27;)&#x27; operator termoperator ::= &#x27;=&#x27; | &#x27;&lt;&#x27; | &#x27;&gt;&#x27; | &#x27;&lt;=&#x27; | &#x27;&gt;=&#x27; | &#x27;!=&#x27; | IN | CONTAINS | CONTAINS KEYgroup_by_clause ::= column_name ( &#x27;,&#x27; column_name )*ordering_clause ::= column_name [ ASC | DESC ] ( &#x27;,&#x27; column_name [ ASC | DESC ] )* 示例： 1234567891011SELECT name, occupation FROM users WHERE userid IN (199, 200, 207);SELECT JSON name, occupation FROM users WHERE userid = 199;SELECT name AS user_name, occupation AS user_occupation FROM users;SELECT time, valueFROM eventsWHERE event_type = &#x27;myEvent&#x27; AND time &gt; &#x27;2011-02-03&#x27; AND time &lt;= &#x27;2012-01-01&#x27;SELECT COUNT (*) AS user_count FROM users; Allowing filtering 默认情况下，CQL仅允许不涉及“过滤”服务器端的选择查询，原因是那些“非过滤”查询具有可预测的性能，因为它们的查询性能与Limit成比例。 举个例子： 123456789CREATE TABLE users ( username text PRIMARY KEY, firstname text, lastname text, birth_year int, country text)CREATE INDEX ON users(birth_year); 以下两种查询是不需要添加ALLOW FILTERING的： 12SELECT * FROM users;SELECT * FROM users WHERE birth_year = 1981; 因为在这两种情况下，Cassandra都保证这些查询性能与返回的数据量成正比。 而下面的这个查询，则需要强制添加： 1SELECT * FROM users WHERE birth_year = 1981 AND country = &#x27;FR&#x27; ALLOW FILTERING; 👉🏼 关于如何定义可预测的列，可参考Cassandra中的索引 INSERT123456insert_statement ::= INSERT INTO table_name ( names_values | json_clause ) [ IF NOT EXISTS ] [ USING update_parameter ( AND update_parameter )* ]names_values ::= names VALUES tuple_literaljson_clause ::= JSON string [ DEFAULT ( NULL | UNSET ) ]names ::= &#x27;(&#x27; column_name ( &#x27;,&#x27; column_name )* &#x27;)&#x27; 示例： 1234567INSERT INTO NerdMovies (movie, director, main_actor, year) VALUES (&#x27;Serenity&#x27;, &#x27;Joss Whedon&#x27;, &#x27;Nathan Fillion&#x27;, 2005) USING TTL 86400;INSERT INTO NerdMovies JSON &#x27;&#123;&quot;movie&quot;: &quot;Serenity&quot;, &quot;director&quot;: &quot;Joss Whedon&quot;, &quot;year&quot;: 2005&#125;&#x27;; 💡请注意 与SQL不同，INSERT默认情况下不检查行的先前存在：如果之前不存在，则创建行，否则更新。此外，没有办法知道发生了哪些创建或更新。 如果要做到存在则不更新，可以使用IF NOT EXISTS条件。但请注意，使用IF NOT EXISTS将导致不可忽略的性能成本（内部使用Paxos），因此应谨慎使用。 INSERT的所有更新都以原子方式单独应用。 UPDATE12345678910111213update_statement ::= UPDATE table_name [ USING update_parameter ( AND update_parameter )* ] SET assignment ( &#x27;,&#x27; assignment )* WHERE where_clause [ IF ( EXISTS | condition ( AND condition )*) ]update_parameter ::= ( TIMESTAMP | TTL ) ( integer | bind_marker )assignment ::= simple_selection &#x27;=&#x27; term | column_name &#x27;=&#x27; column_name ( &#x27;+&#x27; | &#x27;-&#x27; ) term | column_name &#x27;=&#x27; list_literal &#x27;+&#x27; column_namesimple_selection ::= column_name | column_name &#x27;[&#x27; term &#x27;]&#x27; | column_name &#x27;.&#x27; `field_namecondition ::= simple_selection operator term 示例： 12345678910UPDATE NerdMovies USING TTL 400 SET director = &#x27;Joss Whedon&#x27;, main_actor = &#x27;Nathan Fillion&#x27;, year = 2005 WHERE movie = &#x27;Serenity&#x27;;UPDATE UserActions SET total = total + 2 WHERE user = B70DE1D0-9908-4AE3-BE34-5573E5B09F14 AND action = &#x27;click&#x27;; 💡请注意 与SQL不同，UPDATE默认情况下不检查行的先前存在（除非通过IF）：如果之前不存在，则创建行，否则更新。此外，没有办法知道是否发生了创建或更新。 可以通过IF在某些列上使用条件，在这种情况下，除非满足条件，否则不会更新行。但请注意，使用IF条件会产生不可忽视的性能成本（内部使用Paxos），因此应谨慎使用。 在UPDATE语句中，同一分区键中的所有更新都以原子方式单独应用。 此外，UPDATE操作针对某些数据类型有强制性要求： c = c + 3 用于递增/递减计数器。 ‘=’符号后面的列名称必须与’=’符号前面的列名相同。请注意，仅在计数器上允许递增/递减，并且是计数器上允许的唯一更新操作。 id = id + &lt;some-collection&gt; 和id[value1] = value2 用于集合。 id.field = 3 在非冻结的用户定义类型上设置字段的值。 DELETE12345delete_statement ::= DELETE [ simple_selection ( &#x27;,&#x27; simple_selection ) ] FROM table_name [ USING update_parameter ( AND update_parameter )* ] WHERE where_clause [ IF ( EXISTS | condition ( AND condition )*) ] 示例： 12345DELETE FROM NerdMovies USING TIMESTAMP 1240003134 WHERE movie = &#x27;Serenity&#x27;;DELETE phone FROM Users WHERE userid IN (C73DE1D3-AF08-40F3-B124-3FF3E5109F22, B70DE1D0-9908-4AE3-BE34-5573E5B09F14); 💡请注意 WHERE子句指定要删除的行。使用IN运算符可以使用一个语句删除多行。可以使用不等运算符（例如&gt;=）删除一系列行。 在DELETE语句中，同一分区键中的所有删除都以原子方式单独应用。 DELETE操作可以通过使用IF子句来条件化，类似于UPDATE和INSERT语句。但是，与INSERT和UPDATE语句一样，这将导致不可忽略的性能成本（内部，将使用Paxos），因此应谨慎使用。 批处理 批处理只允许包含UPDATE，INSERT和DELETE语句。 批处理节省客户端和服务器之间的网络资源消耗。 12345batch_statement ::= BEGIN [ UNLOGGED | COUNTER ] BATCH [ USING update_parameter ( AND update_parameter )* ] modification_statement ( &#x27;;&#x27; modification_statement )* APPLY BATCHmodification_statement ::= insert_statement | update_statement | delete_statement 示例 123456BEGIN BATCH INSERT INTO users (userid, password, name) VALUES (&#x27;user2&#x27;, &#x27;ch@ngem3b&#x27;, &#x27;second user&#x27;); UPDATE users SET password = &#x27;ps22dhds&#x27; WHERE userid = &#x27;user3&#x27;; INSERT INTO users (userid, password) VALUES (&#x27;user4&#x27;, &#x27;ch@ngem3c&#x27;); DELETE name FROM users WHERE userid = &#x27;user1&#x27;;APPLY BATCH; 💡请注意 属于给定分区键的BATCH中的所有更新都是单独执行的。 默认情况下，批处理中的所有操作都按记录执行，以确保所有变更都最终完成（或不执行任何操作）。类似于SQL事务，但不完全等同于SQL事务。 UNLOGGED batches默认情况下，Cassandra使用批处理日志来确保所有变更都最终完成（或不执行任何操作）【请注意，操作仅在单个分区中隔离】。 批处理跨越多个分区时，批处理在性能上会有所损失。可以使用UNLOGGED选项来跳过批处理日志，不过，如果批处理失败，可能会造成批处理中的任务部分成功部分失败，请谨慎选择。 COUNTER batches使用COUNTER选项进行批量计数器更新。 与Cassandra中的其他更新不同，计数器更新不是幂等的。 参考资源Apache Cassandra Documentation","tags":[{"name":"Cassandra","slug":"Cassandra","permalink":"https://gcdd1993.github.io/tags/Cassandra/"}]},{"title":"记一次Postgres CPU爆满故障","date":"2019-05-10T05:19:22.000Z","path":"p/7877/","text":"问题描述公司项目测试环境调用某些接口的时候，服务器立即崩溃，并一定时间内无法提供服务。 问题排查服务器配置不够第一反应是服务器需要升配啦，花钱解决一切！毕竟测试服务器配置确实不高，2CPU + 4Gib，能干啥？不过问题是今天突然发生的，而且说崩就崩。凭着严谨的态度，还是要刨根问底地找下问题。 查看服务器负载 free -m 内存占用并不大，忘记截图了，反正看下来不是内存过高导致的崩溃 top 数据库占用CPU过高连接数过多 业务高峰活跃连接陡增，活跃的连接数是否比平时多很多 123456SELECT COUNT(*) FROM pg_stat_activity WHERE STATE NOT LIKE &#x27;%idle&#x27;; 查询下来只有3个连接，所以不是连接数导致的CPU过高 慢SQL 如果活跃连接数的变化处于正常范围，则可能是当时有性能很差的SQL被大量执行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849select datname, usename, client_addr, application_name, state, backend_start, xact_start, xact_stay, query_start, query_stay, replace( query, chr(10), &#x27; &#x27; ) as query from ( select pgsa.datname as datname, pgsa.usename as usename, pgsa.client_addr client_addr, pgsa.application_name as application_name, pgsa.state as state, pgsa.backend_start as backend_start, pgsa.xact_start as xact_start, extract( epoch from (now() - pgsa.xact_start) ) as xact_stay, pgsa.query_start as query_start, extract( epoch from (now() - pgsa.query_start) ) as query_stay, pgsa.query as query from pg_stat_activity as pgsa where pgsa.state != &#x27;idle&#x27; and pgsa.state != &#x27;idle in transaction&#x27; and pgsa.state != &#x27;idle in transaction (aborted)&#x27; ) idleconnections order by query_stay desc limit 5; 可以看到，确实有一条慢SQL，而且属于奇慢无比，执行了接近1分钟还没执行完毕，基本可以定位，是慢SQL导致的CPU占用陡增。 问题解决对于上面的方法查出来的慢SQL，首先需要做的是Kill掉他们，使业务先恢复。 12select pg_cancel_backend(pid) from pg_stat_activity where query like &#x27;%&lt;query text&gt;%&#x27; and pid != pg_backend_pid();select pg_terminate_backend(pid) from pg_stat_activity where query like &#x27;%&lt;query text&gt;%&#x27; and pid != pg_backend_pid(); 如果这些SQL确实是业务上必需的，则需要对他们做如下优化： 对查询涉及的表，执行ANALYZE &lt;table&gt;或VACUUM ANZLYZE &lt;table&gt;，更新表的统计信息，使查询计划更准确。为避免对业务影响，最好在业务低峰执行。 执行explain &lt;query text&gt;或explain (buffers true, analyze true, verbose true) &lt;query text&gt;命令，查看SQL的执行计划（前者不会实际执行SQL，后者会实际执行而且能得到详细的执行信息），对其中的Table Scan涉及的表，建立索引。 重新编写SQL，去除掉不必要的子查询、改写UNION ALL、使用JOIN CLAUSE固定连接顺序等，都是进一步深度优化SQL的手段，这里不再深入说明。 总结在查询语句中，尽量减少不必要的子查询，公司使用的ORM框架是Spring JPA，针对一些特别慢的HQL，可以采用直接执行SQL的方式来优化查询效率。 12@Query(value = &quot;select count(*) from example_table where example_id = :exampleId&quot;, nativeQuery = true)int exampleNativeQuery(@Param(&quot;exampleId&quot;) Long exampleId); 参考PostgreSQL/PPAS CPU使用率高的原因及解决办法","tags":[{"name":"数据库","slug":"数据库","permalink":"https://gcdd1993.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"记一次生产事故--磁盘被占满","date":"2019-04-18T14:08:20.000Z","path":"p/58700/","text":"写在前面今天，跑在阿里云ECS上的生产环境，突然间访问异常，接口各种报错，无奈公司没有专业的运维人员，只能硬着头皮解决一下。 问题排查先从表面看起，数据库首先报错 12Caused by: org.postgresql.util.PSQLException: ERROR: could not extend file &quot;base/16385/16587_fsm&quot;: No space left on device 建议：Check free disk space. 直观上看，设备没有可用空间，也就是磁盘满了。 进入服务器后台，执行 12345678910$ df -hFilesystem Size Used Avail Use% Mounted onudev 7.9G 0 7.9G 0% /devtmpfs 1.6G 3.5M 1.6G 1% /run/dev/vda1 59G 56G 0 100% /tmpfs 7.9G 4.0K 7.9G 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G 14G 937G 2% /datatmpfs 1.6G 0 1.6G 0% /run/user/0 发现确实磁盘满了，而且满的很彻底。系统盘占用100%，估计什么服务都跑不动了。/dev/vda1 59G 56G 0 100% / 不过发现/dev/mapper/vg0-vol0 1000G 14G 937G 2% /data，1000G只用了2% 阿里云ECS分为系统盘和数据盘，1000G的是数据盘 第一反应，应该是搭建的PG数据库的数据没有移到数据盘上。 将Postgres数据库数据目录移动到系统盘 参考如何将PostgreSQL数据目录移动到Ubuntu 16.04上的新位置 123456789101112131415161718192021222324$ sudo -u postgres psqlpostgres# SHOW data_directory; # 查看当前数据目录 data_directory ------------------------------ /var/lib/postgresql/9.5/main(1 row)postgres# \\q; # 退出# 为了确保数据的完整性，我们将在实际更改数据目录之前关闭PostgreSQL$ sudo systemctl stop postgresql# 确保关闭完成$ sudo systemctl status postgresql. . .Jul 22 16:22:44 ubuntu-512mb-nyc1-01 systemd[1]: Stopped PostgreSQL RDBMS.$ sudo rsync -av /var/lib/postgresql /data # /data为要迁移到的新目录$ cd /data$ ls... postgresql# 删除原数据目录$ sudo rm -rf /var/lib/postgresql# 将新数据目录链接到原数据目录$ sudo ln -s /data/postgresql /var/lib/postgresql# 重启Postgres数据库$ sudo systemctl start postgresql$ sudo systemctl status postgresql 完成以上步骤，即将postgre数据库数据目录移到了阿里云数据盘 以为OK了，执行 12345678910$ df -hFilesystem Size Used Avail Use% Mounted onudev 7.9G 0 7.9G 0% /devtmpfs 1.6G 3.5M 1.6G 1% /run/dev/vda1 59G 56G 51M 100% /tmpfs 7.9G 4.0K 7.9G 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G 14G 937G 2% /datatmpfs 1.6G 0 1.6G 0% /run/user/0 纹丝未动。。。 Ubuntu查询大文件猜测是存在大文件导致磁盘被占满 123456$ cd /$ find . -type f -size +800M -print0 | xargs -0 du -h5.6G ./var/log/syslog.16.7G ./var/log/syslog...$ rm ... 如果发现是log字眼的大文件，我们可以毫不留情的删掉，要是遇见一些不认识的，不要贸然删掉，一定要查清楚文件的作用，能删则删，千万不要不小心删库跑路。。。 删除完毕后，再次查看 12345678910$ df -hFilesystem Size Used Avail Use% Mounted onudev 7.9G 0 7.9G 0% /devtmpfs 1.6G 3.4M 1.6G 1% /run/dev/vda1 59G 45G 12G 80% /tmpfs 7.9G 4.0K 7.9G 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G 14G 936G 2% /datatmpfs 1.6G 0 1.6G 0% /run/user/0 多出了12G。 查看已删除空间却没有释放的进程这时候，服务应该可以恢复成功。但你马上会发现，磁盘又被占满，而这次，日志文件却不算大。 查看已经删除的文件，空间有没有释放，没有的话kill掉pid 使用rm删除文件的时候，虽然文件已经被删除，但是由于文件被其他进程占用，空间却没有释放 1234$ sudo lsof -n |grep deletedjava 17866 root 237r REG 253,1 163541 1709285 /tmp/tomcat.8250394289784312179.8080/work/Tomcat/localhost/ROOT/upload_c6db0c17_6e6a_4141_bfb6_ac1b2d8a3b0b_00000000.tmp (deleted)...$ sudo kill -9 17866 再次使用df -h命令，磁盘使用率一下子减少了好多。 总结 服务器系统盘被占满是非常可怕的！届时，一切服务都将变得不可用，业务系统也会莫名其妙多出奇怪的问题。所以，运维需要经常性的查看服务器磁盘占用情况，阿里云ECS用户，可以开启报警，及时发现问题，解决问题！ 阿里云ECS提供了系统盘和数据盘，记住，例如Pg、Redis、Cassandra等容易占磁盘的服务，一定要将数据目录放在阿里云ECS提供的数据盘上。 /var/log是系统日志目录，可以经常性的关注下，大容量日志尽早删除。 对待进程不停对文件写日志的操作，要释放文件占用的磁盘空间，最好的方法是在线清空这个文件，可以通过如下命令完成： 1[root@localhost ~]# echo &quot;&quot; &gt;/var/log/syslog 通过这种方法，磁盘空间不但可以马上释放，也可保障进程继续向文件写入日志，这种方法经常用于在线清理Apache、Tomcat、Nginx等Web服务产生的日志文件。 最后，有一个专业的运维是多么重要！","tags":[{"name":"运维","slug":"运维","permalink":"https://gcdd1993.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"运维笔记（部署篇）","date":"2019-04-16T07:43:58.000Z","path":"p/8505/","text":"前言针对Ubuntu 16.04，汇总常用服务的搭建指南。 系统初始化 新买的ECS需要执行系统初始化 12345678910111213141516171819202122232425262728293031323334$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean$ cat /etc/hosts # 修改hosts，一般将本机需要使用的外部内网服务设置映射为名称172.16.0.192 kftest-config01$ cat /etc/hostname # 修改hostname，便于辨认pg_1$ reboot # 修改hostname需要重启生效# 挂载数据盘，例如阿里云数据盘 https://help.aliyun.com/document_detail/25446.html$ sudo fdisk -l # 查看实例上的数据盘Disk /dev/vdb: 1000 GiB, 1073741824000 bytes, 2097152000 sectors$ sudo fdisk -u /dev/vdbCommand (m for help): n... 一路enterCommand (m for help): w## 更多参考 https://help.aliyun.com/document_detail/108501.html$ sudo fdisk -lu /dev/vdbDevice Boot Start End Sectors Size Id Type/dev/vdb1 2048 2097151999 2097149952 1000G 83 Linux$ sudo mkfs.ext4 /dev/vdb1 # 在新分区上创建一个文件系统$ cp /etc/fstab /etc/fstab.bak # 备份 etc/fstab$ echo /dev/vdb1 /data ext4 defaults 0 0 &gt;&gt; /etc/fstab # 向 /etc/fstab 写入新分区信息$ sudo mkdir /data$ sudo mount /dev/vdb1 /data # 挂载文件系统$ df -h/dev/vdb1 985G 72M 935G 1% /data Postgresql安装Postgresql12345$ echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main&quot; | sudo tee /etc/apt/sources.list.d/pgdg.list$ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install postgresql-9.6 # 自行选择合适版本## 更多参考 https://www.postgresql.org/download/linux/ubuntu/ 修改配置文件1234567891011$ sudo vim /etc/postgresql/9.6/main/postgresql.conflisten_addresses = &#x27;*&#x27;max_connections = 1000logging_collector = on## 更多参考 https://www.postgresql.org/docs/current/static/runtime-config.html $ sudo vim /etc/postgresql/9.6/main/pg_hba.confhost all all 0.0.0.0/0 md5## 更多参考 https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html $ sudo service postgresql restart 修改默认用户Postgres的密码1234$ sudo -u postgres psql# ALTER USER postgres WITH PASSWORD &#x27;postgres&#x27;;# \\q$ exit 搭建集群（可选） 主机 ip Master节点 10.10.10.10 Slave节点 10.10.10.9 Master节点和Slave节点分别按照上述步骤安装完成postgres后，开始搭建集群。 master节点： 修改配置 1234567891011121314151617181920$ sudo vi /etc/postgresql/9.6/main/postgresql.conflisten_addresses = &#x27;*&#x27;wal_level = hot_standbyarchive_mode = onarchive_command = &#x27;test ! -f /var/lib/postgresql/9.6/archive/%f &amp;&amp; cp %p /var/lib/postgresql/9.6/archive/%f&#x27;max_wal_senders = 16wal_keep_segments = 100hot_standby = onlogging_collector = on## 更多参考 https://www.postgresql.org/docs/current/static/runtime-config.html$ sudo vi /etc/postgresql/9.6/main/pg_hba.confhost all all 10.0.0.0/8 md5host replication repuser 10.0.0.0/8 md5## 更多参考 https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html $ sudo -upostgres mkdir /var/lib/postgresql/9.6/archive$ sudo chmod 0700 /var/lib/postgresql/9.6/archive $ sudo service postgresql restart 创建工作账户 repuser 12345$ sudo -upostgres createuser --replication repuser$ sudo -upostgres psqlpostgres=# \\password repuser&lt;password&gt;## 更多参考 https://www.postgresql.org/docs/current/static/user-manag.html slave节点： 先停止服务 1$ sudo service postgresql stop 由master节点导入数据（postgres 免密码登录 repuser role） 1234567$ sudo -upostgres vi /var/lib/postgresql/.pgpass10.10.10.10:5432:*:repuser:&lt;password&gt;127.0.0.1:5432:*:repuser:&lt;password&gt; $ sudo chmod 0600 /var/lib/postgresql/.pgpass$ sudo mv /var/lib/postgresql/9.6/main /var/lib/postgresql/9.6/main.bak$ sudo -upostgres pg_basebackup -D /var/lib/postgresql/9.6/main -F p -X stream -v -R -h 10.10.10.10 -p 5432 -U repuser 修改配置 123456789$ sudo vi /var/lib/postgresql/9.6/main/recovery.confstandby_mode = &#x27;on&#x27;primary_conninfo = &#x27;user=repuser host=10.10.10.10 port=5432&#x27;trigger_file = &#x27;failover.now&#x27;## 更多参考 https://www.postgresql.org/docs/current/static/recovery-config.html $ sudo vi /etc/postgresql/9.6/main/postgresql.confhot_standby = on 重启并检查服务 123456789$ sudo service postgresql start $ sudo service postgresql status...Active: active (exited)$ sudo -upostgres psqlpsql (9.6.12)... 测试集群在master节点进行增删改操作，对照看slave节点是否能够从master节点复制操作 常用命令123$ sudo service postgresql start$ sudo service postgresql status$ sudo service postgresql restart 👉 PG数据库常用命令 Redis安装Redis（单机）1234$ sudo apt-get install redis-server$ sudo vim /etc/redis/redis.conf# bind 127.0.0.1$ sudo systemctl restart redis-server 安装Redis（集群） 主机 ip redis-server sentinel node01 10.10.10.5 主 √ node02 10.10.10.4 从 √ node03 10.10.10.6 从 √ 安装 Redis-Server12345678910111213141516node01:$ sudo apt-get install redis-server$ sudo vi /etc/redis/redis.confbind: 10.10.10.5$ sudo service redis-server restartnode02:$ sudo apt-get install redis-server$ sudo vi /etc/redis/redis.confbind: 10.10.10.4slaveof 10.10.10.5 $ sudo service redis-server restartnode03 同node02 测试主从同步12345678910111213141516171819202122232425262728node01:$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt;info....# Replicationrole:masterconnected_slaves:2slave0:ip=10.10.10.4,port=6379,state=online,offset=99,lag=0slave1:ip=10.10.10.6,port=6379,state=online,offset=99,lag=1master_repl_offset:99....10.10.10.5:6379&gt;set testkey testvalueOK10.10.10.5:6379&gt;get testkey&quot;testvalue&quot; node02:$ redis-cli -h 10.10.10.4 -p 637910.9.8.203:6379&gt;info...# Replicationrole:slavemaster_host:10.10.10.5master_port:6379master_link_status:up...10.10.10.4:6379&gt;get testkey&quot;testvalue&quot; 配置 Sentinel（可选） 一个稳健的 Redis Sentinel 集群，应该使用至少 三个 Sentinel 实例，并且保证将这些实例放到 不同的机器 上，甚至不同的 物理区域。 123456789101112131415161718192021222324$ sudo wget http://download.redis.io/redis-stable/sentinel.conf -O /etc/redis/sentinel.conf$ sudo chown redis:redis /etc/redis/sentinel.conf$ sudo vi /etc/redis/sentinel.confsentinel monitor mymaster 10.10.10.5 6379 2sentinel down-after-milliseconds mymaster 60000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000## 自启动配置$ sudo vi /etc/redis/sentinel.service[Unit]Documentation=http://redis.io/topics/sentinel[Service]ExecStart=/usr/bin/redis-server /etc/redis/sentinel.conf --sentinelUser=redisGroup=redis[Install]WantedBy=multi-user.target $ sudo ln -s /etc/redis/sentinel.service /lib/systemd/system/sentinel.service$ sudo systemctl enable sentinel.service$ sudo service sentinel startnode02 node03 sentinel 配置同node01，所有节点配置完成，再继续下一步 配置好sentinel之后，redis.conf和sentinel.conf都由sentinel接管；sentinel监控主节点发生改变的话，会更改对应的配置文件sentinel.conf和redis.conf。 测试Sentinel监控、通知、自动故障转移12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# 查看所有节点哨兵配置node01,node02,node03:$ redis-cli -h 10.10.10.5 -p 2637910.10.10.5:26379&gt; info# Serverredis_version:3.0.6...config_file:/etc/redis/sentinel.conf# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.5:6379,slaves=2,sentinels=1# 在从节点查看哨兵详情，关注主节点$ redis-cli -h 10.10.10.4 -p 2637910.10.10.5:26379&gt; sentinel master mymaster 1) &quot;name&quot; 2) &quot;mymaster&quot; 3) &quot;ip&quot; 4) &quot;10.10.10.5&quot; 5) &quot;port&quot; 6) &quot;6379&quot;...# 停止主节点所在redis-servernode01:$ systemctl stop redis-server.service# 查看从节点的哨兵详情，一般来说，过1分钟~2分钟，会自动选举出新的主节点，例如node03被推举为主节点node02:$ redis-cli -h 10.10.10.4 -p 2637910.10.10.4:26379&gt; info...# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.6:6379,slaves=2,sentinels=3$ redis-cli -h 10.10.10.6 -p 637910.10.10.6:6379&gt; info# Replicationrole:masterconnected_slaves:1slave0:ip=10.10.10.4,port=6379,state=online,offset=19874,lag=0master_repl_offset:19874...# 启动刚才被停止的原主节点redis-server，将作为从节点加入到redis集群node01:$ systemctl start redis-server$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt; info...# Replicationrole:slavemaster_host:10.10.10.6master_port:6379master_link_status:up...$ redis-cli -h 10.10.10.5 -p 2637910.10.10.5:26379&gt; info...# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.6:6379,slaves=2,sentinels=3 客户端连接Sentinel配置完sentinel，客户端连接方式就改变了，拿Redisson举例，需要增加以下配置，并删除单机模式下spring.redis.host配置，端口号改成哨兵的端口号 12spring.redis.sentinel.master=mymasterspring.redis.sentinel.nodes=10.10.10.4:26379,10.10.10.5:26379,10.10.10.6:26379 引入的jar是 1compile &quot;org.redisson:redisson-spring-boot-starter:3.9.1&quot; 配置类所在位置： 1org.springframework.boot.autoconfigure.data.redis.RedisProperties.Sentinel 常用命令1234$ sudo systemctl start redis$ sudo systemctl enable redis$ sudo systemctl restart$ sudo systemctl stop redis 常见问题 有时可能会遇到关闭或重启不了，这时候可以使用redis-cli提供的命令行来强制关闭 123$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt; shutdown nosave## 更多参考 https://redis.io/commands/SHUTDOWN Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Redis被配置为保存数据库快照，但它目前不能持久化到硬盘。 12345$ vim /etc/sysctl.conf## 添加一行vm.overcommit_memory=1$ sudo sysctl -p /etc/sysctl.conf## 重启所有节点redis-server和sentinel 如果改好后，还不行，就需要查看下Redis的dump文件配置是不是被更改了 1234567$ redis-cli -h 10.10.10.510.10.10.5:6379&gt; CONFIG GET dbfilename1) &quot;dbfilename&quot;2) &quot;.rdb&quot; ## 默认是dump.rdb10.10.10.5:6379&gt; CONFIG GET dir1) &quot;dir&quot;2) &quot;/var/spool/cron&quot; ## 默认是dump.rdb 以上配置，如果不是自己更改的，则可怀疑是被黑客篡改了 检查Redis端口是否在公网开放，如果是，立马关闭 设置Redis访问密码 恢复Redis默认配置 1234567$ vim /etc/redis/redis.confdbfilename &quot;dump.rdb&quot;dir &quot;/var/lib/redis&quot;$ service redis-server restartnode01 node02 node03均按此修改并重启## 了解更多 https://serverfault.com/questions/800295/redis-spontaneously-failed-failed-opening-rdb-for-saving-permission-denied Consul安装Consul（单机）12345678910111213141516171819202122232425262728293031$ sudo mkdir -p /data/consul/&#123;current/&#123;bin,etc&#125;,data&#125;$ sudo wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip -O /data/consul/consul_1.5.3_linux_amd64.zip$ sudo apt-get install unzip$ sudo unzip /data/consul/consul_1.5.3_linux_amd64.zip -d /data/consul/current/bin$ sudo vi /data/consul/current/etc/consul.json&#123; &quot;bootstrap&quot;: true, &quot;datacenter&quot;: &quot;test-datacenter&quot;, &quot;data_dir&quot;: &quot;/data/consul/data&quot;, &quot;log_level&quot;: &quot;INFO&quot;, &quot;server&quot;: true, &quot;client_addr&quot;: &quot;0.0.0.0&quot;, &quot;ui&quot;: true, &quot;start_join&quot;: [&quot;ip:8301&quot;], &quot;enable_syslog&quot;: true&#125;## 更多参考：https://www.consul.io/docs/agent/options.html#configuration_files$ sudo ln -s /data/consul/current/etc /data/consul/etc$ sudo vi /etc/systemd/system/consul.service[Unit]Description=consul service[Service]ExecStart=/data/consul/current/bin/consul agent -bind=&#123;ip&#125; -config-dir /data/consul/etc/consul.jsonUser=root[Install]WantedBy=multi-user.target$ sudo systemctl enable consul.service$ sudo systemctl start consul.service 安装Consul（集群） 主机 ip node01 10.10.10.5 node02 10.10.10.4 node03 10.10.10.6 1234567891011121314151617181920212223242526272829303132node01 node02 node03$ sudo mkdir -p /data/consul/&#123;current/&#123;bin,etc&#125;,data&#125;$ sudo wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip -O /data/consul/consul_1.5.3_linux_amd64.zip$ sudo apt-get install unzip$ sudo unzip /data/consul/consul_1.5.3_linux_amd64.zip -d /data/consul/current/bin$ sudo vi /data/consul/current/etc/consul.json&#123; &quot;datacenter&quot;: &quot;roc-datacenter&quot;, &quot;data_dir&quot;: &quot;/data/consul/data&quot;, &quot;log_level&quot;: &quot;INFO&quot;, &quot;server&quot;: true, &quot;bootstrap_expect&quot;: 3, &quot;client_addr&quot;: &quot;10.10.10.4&quot;, &quot;ui&quot;: true, &quot;start_join&quot;: [&quot;10.10.10.4:8301&quot;,&quot;10.10.10.5:8301&quot;,&quot;10.10.10.6:8301&quot;], &quot;enable_syslog&quot;: true&#125;## 更多参考：https://www.consul.io/docs/agent/options.html#configuration_files$ sudo ln -s /data/consul/current/etc /data/consul/etc$ sudo vi /etc/systemd/system/consul.service[Unit]Description=consul service[Service]ExecStart=/data/consul/current/bin/consul agent -config-dir /data/consul/etc/consul.jsonUser=root[Install]WantedBy=multi-user.target$ sudo systemctl enable consul.service$ sudo systemctl start consul.service 需要开放的端口：8300, 8301, 8500，如果网络不通，则子节点将无法join到主节点，可能会出现 1failed to sync remote state: No cluster leader 无法选举出leader，其实是节点之间无法通信，如果通信正常，启动之时所有节点会自动推举出leader。 常用命令12345$ sudo systemctl start consul.service$ sudo systemctl stop consul.service$ sudo systemctl restart consul.service## 更多参考：https://www.consul.io/docs/commands/index.html Nginx安装Nginx12345$ echo -e &quot;deb http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\\ndeb-src http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx&quot; | sudo tee /etc/apt/sources.list.d/nginx.list$ wget -O- http://nginx.org/keys/nginx_signing.key | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install nginx## 更多参考：http://nginx.org/en/linux_packages.html#stable 常用命令12345$ sudo service nginx start$ sudo service nginx stop$ sudo service nginx restart$ sudo service nginx reload # 重新加载配置 Cassandra集群 主机 IP cassandra-1 192.168.0.1 cassandra-2 192.168.0.2 安装Cassandra123456$ echo &quot;deb http://www.apache.org/dist/cassandra/debian 39x main&quot; | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list$ curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -$ sudo apt update$ sudo apt -y install cassandra$ sudo apt install openjdk-8-jdk-headless## 更多参考：http://cassandra.apache.org/download/#installation-from-debian-packages 修改配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657$ sudo vi /etc/cassandra/cassandra.yamlseed_provider: - seeds: &quot;192.168.0.1,192.168.0.2&quot; concurrent_writes: 64concurrent_counter_writes: 64concurrent_counter_writes: 64concurrent_materialized_view_writes: 64compaction_throughput_mb_per_sec: 128file_cache_size_in_mb: 1024buffer_pool_use_heap_if_exhausted: truedisk_optimization_strategy: spinning#listen_address: localhostlisten_interface: eth0#rpc_address: localhostrpc_interface: eth0enable_user_defined_functions: trueauto_bootstrap: false## 优化cassandra jvm配置$ sudo vi /etc/cassandra/jvm.options#-XX:+UseParNewGC#-XX:+UseConcMarkSweepGC#-XX:+CMSParallelRemarkEnabled#-XX:SurvivorRatio=8#-XX:MaxTenuringThreshold=1#-XX:CMSInitiatingOccupancyFraction=75#-XX:+UseCMSInitiatingOccupancyOnly#-XX:CMSWaitDuration=10000#-XX:+CMSParallelInitialMarkEnabled#-XX:+CMSEdenChunksRecordAlways-XX:+UseG1GC-XX:G1RSetUpdatingPauseTimePercent=5-XX:MaxGCPauseMillis=500-XX:InitiatingHeapOccupancyPercent=70-XX:ParallelGCThreads=16-XX:ConcGCThreads=16$ sudo vi /etc/cassandra/cassandra-env.sh## 配置为主机内网地址JVM_OPTS=&quot;$JVM_OPTS -Djava.rmi.server.hostname=192.168.0.1&quot;#if [ &quot;x$LOCAL_JMX&quot; = &quot;x&quot; ]; then# LOCAL_JMX=yes# fi if [ &quot;x$LOCAL_JMX&quot; = &quot;x&quot; ]; then LOCAL_JMX=no fi#JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=true&quot;JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;#JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password&quot;$ sudo systemctl stop cassandra 迁移配置导数据盘（可选）1234$ sudo mv /var/lib/cassandra /data/cassandra$ sudo ln -s /data/cassandra /var/lib/cassandra$ sudo systemctl start cassandra 集群内其余机器，重复上述步骤，修改对应IP Zookeeper集群 主机 IP zk-01 192.168.0.1 zk-02 192.168.0.2 zk-03 192.168.0.3 安装Zookeeper1$ sudo apt install zookeeperd 修改配置文件12345678$ sudo vim /etc/zookeeper/conf/zoo.cfgserver.1=192.168.0.1:2888:3888server.2=192.168.0.2:2888:3888server.3=192.168.0.3:2888:3888$ sudo vim /etc/zookeeper/conf/myid1# 每台主机id各不相同，比如zk-01=1,zk-02=2,zk-03=3$ sudo systemctl restart zookeeper 安装ZK-UI（可选）123456789# 安装zkui$ cd /data &amp;&amp; wget https://github.com/zifangsky/zkui/releases/download/v2.0/zkui-2.0.zip$ sudo unzip zkui-2.0.zip$ sudo vi /data/zkui/config.cfg zkServer=192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181userSet = &#123;&quot;users&quot;: [&#123; &quot;username&quot;:&quot;&lt;username&gt;&quot; , &quot;password&quot;:&quot;&lt;password&gt;&quot;,&quot;role&quot;: &quot;ADMIN&quot; &#125;,&#123; &quot;username&quot;:&quot;appconfig&quot; , &quot;password&quot;:&quot;appconfig&quot;,&quot;role&quot;: &quot;USER&quot; &#125;]&#125; $ cd /data/zkui &amp;&amp; sudo bash start.sh 集群内其余机器，重复上述步骤 Kafka集群 主机 IP zk-01 192.168.0.1 zk-02 192.168.0.2 zk-03 192.168.0.3 安装Kafka12345678$ sudo mkdir /data/kafka &amp;&amp; cd ~$ wget &quot;http://www-eu.apache.org/dist/kafka/1.0.1/kafka_2.12-1.0.1.tgz&quot;$ curl http://kafka.apache.org/KEYS | gpg --import$ wget https://dist.apache.org/repos/dist/release/kafka/1.0.1/kafka_2.12-1.0.1.tgz.asc$ gpg --verify kafka_2.12-1.0.1.tgz.asc kafka_2.12-1.0.1.tgz$ sudo tar -xvzf kafka_2.12-1.0.1.tgz --directory /data/kafka --strip-components 1$ sudo rm -rf kafka_2.12-1.0.1.tgz kafka_2.12-1.0.1.tgz.asc## 更多参考 https://tecadmin.net/install-apache-kafka-ubuntu/ 修改配置文件123456789101112131415161718$ sudo mkdir /data/kafka-logs$ sudo cp /data/kafka/config/server.properties&#123;,.bak&#125;$ sudo vim /data/kafka/config/server.properties broker.id=0 # 每台主机各不相同listeners=PLAINTEXT://0.0.0.0:9092advertised.listeners=PLAINTEXT://&lt;ip&gt;:9092delete.topic.enable = trueleader.imbalance.check.interval.seconds=5 # leader不平衡检查间隔leader.imbalance.per.broker.percentage=1log.dirs=/data/kafka-logsoffsets.topic.replication.factor=3log.retention.hours=72log.segment.bytes=1073741824zookeeper.connect=192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181 $ sudo vim /data/kafka/bin/kafka-server-start.shexport JMX_PORT=12345 # 暴露jmx端口，留待监控使用 注册为Systemd服务12345678910111213141516171819$ sudo adduser --system --no-create-home --disabled-password --disabled-login kafka$ sudo chown -R kafka:nogroup /data/kafka$ sudo chown -R kafka:nogroup /data/kafka-logs $ sudo vim /etc/systemd/system/kafka.service[Unit]Description=High-available, distributed message brokerAfter=network.target[Service]User=kafkaExecStart=/data/kafka/bin/kafka-server-start.sh /data/kafka/config/server.properties[Install]WantedBy=multi-user.target## 启用服务$ sudo systemctl enable kafka.service$ sudo systemctl start kafka.service## 更多参考 https://kafka.apache.org/quickstart 测试Kafka的使用（可选）123456789$ /data/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test$ /data/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181 $ /data/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt; Hello World # 另外一个terminal$ /data/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningHello World 部署Kafka-manager1234567891011121314151617181920212223$ cd /data &amp; sudo wget https://github.com/yahoo/kafka-manager/archive/1.3.3.17.zip$ sudo unzip kafka-manager-1.3.3.17.zip$ sudo mv kafka-manager-1.3.3.17 kafka-manager$ sudo chown -R kafka:nogroup /data/kafka-manager$ sudo vim /data/kafka-manager/conf/application.confkafka-manager.zkhosts=&quot;192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181&quot;basicAuthentication.enabled=truebasicAuthentication.username=&quot;&lt;username&gt;&quot;basicAuthentication.password=&quot;&lt;password&gt;&quot; $ sudo vim /etc/systemd/system/kafka-manager.service[Unit]Description=High-available, distributed message broker managerAfter=network.target[Service]User=kafkaExecStart=/data/kafka-manager/bin/kafka-manager[Install]WantedBy=multi-user.target## 启用服务$ sudo systemctl enable kafka-manager.service$ sudo systemctl start kafka-manager.service Mysql安装Mysql12$ sudo apt-get update$ sudo apt-get install mysql-server 在安装过程中，系统将提示您创建root密码。请务必记住root密码 配置Mysql运行安全脚本 1$ mysql_secure_installation 值得一提的是，Disallow root login remotely?，如果你需要使用root账号进行远程连接，请选择No 验证接下来测试下是否安装成功了 运行状态 12345678910$ systemctl status mysql.service● mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2019-07-18 23:38:43 PDT; 11min ago Main PID: 2948 (mysqld) Tasks: 28 Memory: 142.6M CPU: 545ms CGroup: /system.slice/mysql.service └─2948 /usr/sbin/mysqld 登录查看版本 123456789101112131415$ mysqladmin -p -u root versionmysqladmin Ver 8.42 Distrib 5.7.26, for Linux on x86_64Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Server version 5.7.26-0ubuntu0.16.04.1Protocol version 10Connection Localhost via UNIX socketUNIX socket /var/run/mysqld/mysqld.sockUptime: 12 min 18 secThreads: 1 Questions: 36 Slow queries: 0 Opens: 121 Flush tables: 1 Open tables: 40 Queries per second avg: 0.048 到这里，Mysql安装完成！ 参考 Systemd 入门教程：命令篇","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://gcdd1993.github.io/tags/Ubuntu/"},{"name":"运维","slug":"运维","permalink":"https://gcdd1993.github.io/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Java安全笔记","date":"2019-04-10T12:13:10.000Z","path":"p/10700/","text":"前言后端接口开发中，涉及到用户私密信息（用户名、密码）等，我们不能传输明文，必须使用加密方式传输。这次政府项目中，安全测试组提出了明文传输漏洞，抽空研究了下Java加解密相关知识，记录下。 散列函数Java提供了一个名为MessageDigest的类，它属于java.security包。 此类支持诸如SHA-1，SHA 256，MD5之类的算法，以将任意长度的消息转换为信息摘要。 散列函数返回的值称为信息摘要或简称散列值。 下图说明了散列函数。 要使用散列函数加密数据，我们通常按照以下步骤执行： 创建MessageDigest对象1MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;); MessageDigest提供了getInstance静态方法来获得MessageDigest实例，支持的类型可参考Wiki-SHA家族 将数据传递给创建的MessageDigest对象1md.update(&quot;gcdd1993&quot;.getBytes()); 生成消息摘要1byte[] digest = md.digest(); 通常我们会将其转换为Hex字符串123456StringBuffer hexString = new StringBuffer();for (byte aDigest : digest) &#123; hexString.append(Integer.toHexString(0xFF &amp; aDigest));&#125;System.out.println(&quot;Hex format : &quot; + hexString.toString()); 消息认证码 MAC(消息认证码)算法是一种对称密钥加密技术，用于提供消息认证。要建立MAC过程，发送方和接收方共享对称密钥K。 实质上，MAC是在基础消息上生成的加密校验和，它与消息一起发送以确保消息验证。 使用MAC进行身份验证的过程如下图所示 在Java中，javax.crypto包的Mac类提供了消息认证代码的功能。按照以下步骤使用此类创建消息身份验证代码。 创建KeyGenerator对象1KeyGenerator keyGen = KeyGenerator.getInstance(&quot;DES&quot;); KeyGenerator支持以下类型： AES (128) DES (56) DESede (168) HmacSHA1 HmacSHA256 创建SecureRandom对象1SecureRandom secureRandom = new SecureRandom(); 初始化KeyGenerator1keyGen.init(secureRandom); 生成密钥1Key key = keyGen.generateKey(); 使用密钥初始化Mac对象12Mac mac = Mac.getInstance(&quot;HmacMD5&quot;);mac.init(key); Mac支持以下类型： HmacMD5 HmacSHA1 HmacSHA256 完成mac操作123String msg = &quot;gcdd1993&quot;;byte[] bytes = msg.getBytes();byte[] macResult = mac.doFinal(bytes); 数字签名 数字签名允许验证签名的作者，日期和时间，验证消息内容。 它还包括用于其他功能的身份验证功能。 优点 认证 数字签名有助于验证消息来源。 完整性 邮件签名后，邮件中的任何更改都将使签名无效。 不可否认 通过此属性，任何已签署某些信息的实体都不能在以后拒绝签名。 创建数字签名创建KeyPairGenerator对象 KeyPairGenerator类提供getInstance()方法，该方法接受表示所需密钥生成算法的String变量，并返回生成密钥的KeyPairGenerator对象。 1KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(&quot;DSA&quot;); 初始化KeyPairGenerator对象 KeyPairGenerator类提供了一个名为initialize()的方法，该方法用于初始化密钥对生成器。 此方法接受表示密钥大小的整数值。 1keyPairGen.initialize(2048); 生成KeyPair 使用generateKeyPair()方法生成密钥对 1KeyPair pair = keyPairGen.generateKeyPair(); 从密钥对中获取私钥1PrivateKey privateKey = pair.getPrivate(); 创建签名对象 Signature类的getInstance()方法接受表示所需签名算法的字符串参数，并返回相应的Signature对象。 Signature支持以下类型： SHA1withDSA SHA1withRSA SHA256withRSA 1Signature sign = Signature.getInstance(&quot;SHA256withDSA&quot;); 初始化签名对象1sign.initSign(privateKey); 将数据添加到Signature对象12String msg = &quot;gcdd1993&quot;;sign.update(msg.getBytes()); 计算签名1byte[] signature = sign.sign(); 验证签名 我们创建签名后，通常可以将私钥发送到客户端，以进行签名操作。服务端保存公钥，以进行签名验证 初始化签名对象以进行验证 使用公钥初始化签名对象 1sign.initVerify(pair.getPublic()); 更新要验证的数据1sign.update(msg.getBytes()); 验证签名12boolean verify = sign.verify(signature);Assert.assertTrue(verify); 公私钥加解密数据 可以使用javax.crypto包的Cipher类加密给定数据。 获取公私钥的步骤，与签名类似 1234KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(&quot;RSA&quot;);keyPairGen.initialize(2048);KeyPair pair = keyPairGen.generateKeyPair();PublicKey publicKey = pair.getPublic(); 加密数据创建一个Cipher对象 Cipher类的getInstance()方法接受表示所需转换的String变量，并返回实现给定转换的Cipher对象。 Cipher支持以下类型： AES/CBC/NoPadding (128) AES/CBC/PKCS5Padding (128) AES/ECB/NoPadding (128) AES/ECB/PKCS5Padding (128) DES/CBC/NoPadding (56) DES/CBC/PKCS5Padding (56) DES/ECB/NoPadding (56) DES/ECB/PKCS5Padding (56) DESede/CBC/NoPadding (168) DESede/CBC/PKCS5Padding (168) DESede/ECB/NoPadding (168) DESede/ECB/PKCS5Padding (168) RSA/ECB/PKCS1Padding (1024, 2048) RSA/ECB/OAEPWithSHA-1AndMGF1Padding (1024, 2048) RSA/ECB/OAEPWithSHA-256AndMGF1Padding (1024, 2048) 1Cipher cipher = Cipher.getInstance(&quot;RSA/ECB/PKCS1Padding&quot;); 使用公钥初始化Cipher对象 Cipher类的init()方法接受两个参数，一个表示操作模式的整数参数(加密/解密)和一个表示公钥的Key对象。 1cipher.init(Cipher.ENCRYPT_MODE, publicKey); 将数据添加到Cipher对象 Cipher类的update()方法接受表示要加密的数据的字节数组，并使用给定的数据更新当前对象。 12String msg = &quot;gcdd1993&quot;;cipher.update(msg.getBytes()); 加密数据1byte[] cipherText = cipher.doFinal(); 解密数据使用私钥初始化Cipher对象1cipher.init(Cipher.DECRYPT_MODE, pair.getPrivate()); 解密数据12byte[] decipheredText = cipher.doFinal(cipherText);Assert.assertEquals(msg, new String(decipheredText)); 第三方类库 前后端适用且应用广泛的是Crypto-JS,使用 Crypto-JS 可以非常方便地在 JavaScript 进行 MD5、SHA1、SHA2、SHA3、RIPEMD-160 哈希散列，进行 AES、DES、Rabbit、RC4、Triple DES 加解密。 AES加密 高级加密标准（英语：Advanced Encryption Standard，缩写：AES），在密码学中又称Rijndael加密法，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。 一般来说，我们可以在服务端随机生成密钥，然后将密钥发送给客户端进行加密，上传密文到服务端，服务端进行解密。 本文只讨论Java的AES加解密方式。 引入Jar包1compile group: &#x27;org.webjars.npm&#x27;, name: &#x27;crypto-js&#x27;, version: &#x27;3.1.8&#x27; 生成密钥1234Random random = new Random();byte[] key = new byte[16];random.nextBytes(key);SecretKeySpec keySpec = new SecretKeySpec(key, &quot;AES&quot;); 生成偏移量123byte[] iv = new byte[16];random.nextBytes(iv);IvParameterSpec ivSpec = new IvParameterSpec(iv); 创建Cipher对象1Cipher cipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;); 初始化Cipher为加密工作过程1cipher.init(Cipher.ENCRYPT_MODE, keySpec, ivSpec); 加密1byte[] original = cipher.doFinal(encrypted1); AES解密初始化Cipher为解密工作过程1cipher.init(Cipher.DECRYPT_MODE, keySpec, ivSpec); 解密12byte[] bytes = cipher.doFinal(original);Assert.assertEquals(data, new String(bytes, StandardCharsets.UTF_8)); AES加解密总结实际项目中，可以按照以下方式实现对称加密 服务端提供一个接口，该接口负责随机生成key（密码）和iv（偏移量），并将其存入redis（设置超时时间） 客户端调用接口，获得key和iv以及一个redis_key，进行数据加密，将加密后的数据以及redis_key传到服务端 服务端使用redis_key获得key和iv，进行解密 总结在Java EE安全里，主要是进行客户端加密，以及服务端解密的过程来实现数据安全传输的目的。在这个过程中，特别要注意以下几点： 随机性：加密方式不可单一，可通过更换Cipher.getInstance()的String值来随机生成加密工人进行加密。 保密性：加密使用的密钥或者偏移量等，需要使用超时、模糊目的等手段进行隐藏，加大破解成本。 没有完全有效的加密，但是只要做到破解成本大于加密成本，就是有效的加密。这样，我们可以不断地更换加密方式达到我们想要的效果。 👉 代码仓库","tags":[{"name":"Java","slug":"Java","permalink":"https://gcdd1993.github.io/tags/Java/"},{"name":"安全","slug":"安全","permalink":"https://gcdd1993.github.io/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"Lombok 详解","date":"2019-04-05T13:19:26.000Z","path":"p/12232/","text":"简介lombok是一个编译级别的插件，它可以在项目编译的时候生成一些代码。通俗的说，lombok可以通过注解来标示生成getter settter等代码。 引入创建gradle项目 1compile group: &#x27;org.projectlombok&#x27;, name: &#x27;lombok&#x27;, version: &#x27;1.16.20&#x27; 注解@NonNull 标记字段不可为null 1234567@Setterpublic class Person &#123; @NonNull private String name; @NonNull private Integer age;&#125; 对应的字节码文件： 12345678910111213141516171819202122232425public class Person &#123; @NonNull private String name; @NonNull private Integer age; public Person() &#123; &#125; public void setName(@NonNull String name) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name&quot;); &#125; else &#123; this.name = name; &#125; &#125; public void setAge(@NonNull Integer age) &#123; if (age == null) &#123; throw new NullPointerException(&quot;age&quot;); &#125; else &#123; this.age = age; &#125; &#125;&#125; @Getter/@Setter 自动生成getter和setter方法 123456public class Person &#123; @Getter private String name; @Setter private Integer age;&#125; 对应的字节码文件： 123456789101112131415public class Person &#123; private String name; private Integer age; public Person() &#123; &#125; public String getName() &#123; return this.name; &#125; public void setAge(Integer age) &#123; this.age = age; &#125;&#125; @Cleanup 自动关闭流代码 12@CleanupInputStream in = new FileInputStream(args[0]); 对应的字节码文件： 1234InputStream in = new FileInputStream(args[0]);if (Collections.singletonList(in).get(0) != null) &#123; in.close();&#125; @AllArgsConstructor/@NoArgsConstructor/@RequiredArgsConstructor 自动生成全参构造函数和无参构造函数 123456@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; private String name; private Integer age;&#125; 对应的字节码文件 123456789101112public class Person &#123; private String name; private Integer age; public Person(String name, Integer age) &#123; this.name = name; this.age = age; &#125; public Person() &#123; &#125;&#125; @Builder 自动生成建造者模式的bean 12345@Builderpublic class Person &#123; private String name; private Integer age;&#125; 对应的字节码文件 123456789101112131415161718192021222324252627282930313233343536373839public class Person &#123; private String name; private Integer age; Person(String name, Integer age) &#123; this.name = name; this.age = age; &#125; public static Person.PersonBuilder builder() &#123; return new Person.PersonBuilder(); &#125; public static class PersonBuilder &#123; private String name; private Integer age; PersonBuilder() &#123; &#125; public Person.PersonBuilder name(String name) &#123; this.name = name; return this; &#125; public Person.PersonBuilder age(Integer age) &#123; this.age = age; return this; &#125; public Person build() &#123; return new Person(this.name, this.age); &#125; public String toString() &#123; return &quot;Person.PersonBuilder(name=&quot; + this.name + &quot;, age=&quot; + this.age + &quot;)&quot;; &#125; &#125;&#125; @EqualsAndHashCode 自动生成equals和hashcode方法 12345@EqualsAndHashCodepublic class Person &#123; private String name; private Integer age;&#125; 对应的字节码文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Person &#123; private String name; private Integer age; public Person() &#123; &#125; public boolean equals(Object o) &#123; if (o == this) &#123; return true; &#125; else if (!(o instanceof Person)) &#123; return false; &#125; else &#123; Person other = (Person)o; if (!other.canEqual(this)) &#123; return false; &#125; else &#123; Object this$name = this.name; Object other$name = other.name; if (this$name == null) &#123; if (other$name != null) &#123; return false; &#125; &#125; else if (!this$name.equals(other$name)) &#123; return false; &#125; Object this$age = this.age; Object other$age = other.age; if (this$age == null) &#123; if (other$age != null) &#123; return false; &#125; &#125; else if (!this$age.equals(other$age)) &#123; return false; &#125; return true; &#125; &#125; &#125; protected boolean canEqual(Object other) &#123; return other instanceof Person; &#125; public int hashCode() &#123; int PRIME = true; int result = 1; Object $name = this.name; int result = result * 59 + ($name == null ? 43 : $name.hashCode()); Object $age = this.age; result = result * 59 + ($age == null ? 43 : $age.hashCode()); return result; &#125;&#125; @ToString 自动生成toString()方法 12345@ToStringpublic class Person &#123; private String name; private Integer age;&#125; 对应的字节码文件 1234567891011public class Person &#123; private String name; private Integer age; public Person() &#123; &#125; public String toString() &#123; return &quot;Person(name=&quot; + this.name + &quot;, age=&quot; + this.age + &quot;)&quot;; &#125;&#125; @Value 自动生成全参构造函数、Getter方法、equals方法、hashCode法、toString方法 12345@Valuepublic class Person &#123; private String name; private Integer age;&#125; 注意：@Value不会生成Setter方法 @Synchronized 自动为被标记的方法添加synchronized锁 123456789101112131415161718public class SynchronizedExample &#123; private final Object readLock = new Object(); @Synchronized public static void hello() &#123; System.out.println(&quot;world&quot;); &#125; @Synchronized public int answerToLife() &#123; return 42; &#125; @Synchronized(&quot;readLock&quot;) public void foo() &#123; System.out.println(&quot;bar&quot;); &#125;&#125; 对应的字节码文件 1234567891011121314151617181920212223public class SynchronizedExample &#123; private static final Object $LOCK = new Object[0]; private final Object $lock = new Object[0]; private final Object readLock = new Object(); public static void hello() &#123; synchronized($LOCK) &#123; System.out.println(&quot;world&quot;); &#125; &#125; public int answerToLife() &#123; synchronized($lock) &#123; return 42; &#125; &#125; public void foo() &#123; synchronized(readLock) &#123; System.out.println(&quot;bar&quot;); &#125; &#125;&#125; @Delegate 为标记属性生成委托方法 12345678910public class DelegateExample &#123; public void show() &#123; System.out.println(&quot;show...&quot;); &#125;&#125;@AllArgsConstructorpublic class Demo &#123; @Delegate private final DelegateExample delegateExample;&#125; 对应的字节码文件 1234567891011121314151617181920public class DelegateExample &#123; public DelegateExample() &#123; &#125; public void show() &#123; System.out.println(&quot;show...&quot;); &#125;&#125;public class Demo &#123; private final DelegateExample delegateExample; public Demo(DelegateExample delegateExample) &#123; this.delegateExample = delegateExample; &#125; // 委托方法 public void show() &#123; this.delegateExample.show(); &#125;&#125;","tags":[{"name":"Lombok","slug":"Lombok","permalink":"https://gcdd1993.github.io/tags/Lombok/"}]},{"title":"消息队列（三）Apache ActiveMQ","date":"2019-04-02T03:33:26.000Z","path":"p/32495/","text":"在Ubuntu上安装ActiveMQ系统初始化1234$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean 搭建activemq服务1234567891011$ mkdir /home/active-mq$ cd /home/active-mq$ wget http://www.apache.org/dist/activemq/5.15.9/apache-activemq-5.15.9-bin.tar.gz# 具体版本请查看http://www.apache.org/dist/activemq$ tar -zxvf apache-activemq-5.15.9-bin.tar.gz# 如果未安装jdk，执行 sudo apt-get install openjdk-8-jdk$ ./activemq startINFO: Loading &#x27;/home/active-mq/apache-activemq-5.15.9//bin/env&#x27;INFO: Using java &#x27;/usr/bin/java&#x27;INFO: Starting - inspect logfiles specified in logging.properties and log4j.properties to get detailsINFO: pidfile created : &#x27;/home/active-mq/apache-activemq-5.15.9//data/activemq.pid&#x27; (pid &#x27;6356&#x27;) 监控浏览器打开http://localhost:8161/admin/，输入admin，admin 至此，ActiveMQ搭建完成。 理解JMS( Java Message Service)Java消息服务指的是两个应用程序之间进行异步通信的API，它为标准消息协议和消息服务提供了一组通用接口，包括创建、发送、读取消息等，用于支持JAVA应用程序开发。 JMS模型 点对点（P2P）或队列模型 只有一个消费者将获得消息 生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。 每一个成功处理的消息都由接收者签收 发布/订阅模型 多个消费者可以获得消息 在发布者和订阅者之间存在时间依赖性。发布者需要创建一个订阅（subscription），以便客户能够购订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者创建了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 传统API传统API提供的主要接口如下： ConnectionFactory：客户端用来创建连接的受管对象。简化API也会使用此接口。 Connection：客户端到JMS提供者之间的活动连接。 Session：发送和接收消息的一个单线程上下文。 MessageProducer：由Session创建的对象，用于发送消息到Queue或Topic MessageConsumer：由Session创建的对象，用于接收Queue或Topic中的消息 简化API简化API与传统API提供的消息功能是一样的，但是它需要的接口更少、使用更方便。 简化API提供的主要接口如下： ConnectionFactory：客户端用来创建连接的受管对象。传统API也会使用此接口。 JMSContext：客户端到JMS提供者之间的活动连接，以及发送和接收消息的一个单线程上下文。 JMSProducer：由JMSContext创建的对象，用于发送消息到Queue或Topic JMSConsumer：由JMSContext创建的对象，用于接收Queue或Topic中的消息 在简化API中，一个JMSContext对象封装了传统API中Connection和Session两个对象的行为。 开发一个JMS客户端一个使用传统API的JMS客户端典型的使用步骤如下： 使用JNDI查找一个ConnectionFactory对象 使用JNDI查找一个或多个Destination对象 使用ConnectionFactory创建一个JMS Connection对象 使用Connection创建一个或多个JMS Session对象 使用Session和Destination对象创建需要的MessageProducer和MessageConsumer对象 通知Connection对象开始投递消息 Active MQ是完全实现JMS规范的JMS客户端 Hello World创建Hello World项目创建gradle项目，并编辑build.gradle 12compile group: &#x27;org.apache.activemq&#x27;, name: &#x27;activemq-all&#x27;, version: &#x27;5.15.9&#x27;compile group: &#x27;com.fasterxml.jackson.core&#x27;, name: &#x27;jackson-databind&#x27;, version: &#x27;2.9.8&#x27; 创建生产者12345678910111213141516171819202122232425262728293031public class HelloWorldProducer implements Runnable &#123; @Override public void run() &#123; try &#123; // 1. 创建连接工厂 ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(&quot;vm://localhost&quot;); // 2. 创建连接 Connection connection = connectionFactory.createConnection(); connection.start(); // 3. 创建会话 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 4. 创建目的地（主题或队列） Destination destination = session.createQueue(&quot;TEST.FOO&quot;); // 5. 从会话创建到目的地的消息发布者 MessageProducer producer = session.createProducer(destination); producer.setDeliveryMode(DeliveryMode.NON_PERSISTENT); // 6. 创建并发布消息 String text = &quot;Hello world! From: &quot; + Thread.currentThread().getName() + &quot; : &quot; + this.hashCode(); TextMessage message = session.createTextMessage(text); System.out.println(&quot;Sent message: &quot; + message.hashCode() + &quot; : &quot; + Thread.currentThread().getName()); producer.send(message); // 7. 销毁资源 session.close(); connection.close(); &#125; catch (JMSException e) &#123; System.out.println(&quot;Caught: &quot; + e); e.printStackTrace(); &#125; &#125;&#125; 创建消费者12345678910111213141516171819202122232425262728293031323334353637383940public class HelloWorldConsumer implements Runnable, ExceptionListener &#123; @Override public void run() &#123; try &#123; // 1. 创建连接工厂 ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(&quot;vm://localhost&quot;); // 2. 创建连接 Connection connection = connectionFactory.createConnection(); connection.start(); // 3. 创建会话 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 4. 创建目的地（主题或队列） Destination destination = session.createQueue(&quot;TEST.FOO&quot;); // 5. 从会话创建到目的地的消息消费者 MessageConsumer consumer = session.createConsumer(destination); // 6. 等待接收消息 Message message = consumer.receive(1000); if (message instanceof TextMessage) &#123; TextMessage textMessage = (TextMessage) message; String text = textMessage.getText(); System.out.println(&quot;Received: &quot; + text); &#125; else &#123; System.out.println(&quot;Received: &quot; + message); &#125; // 7. 销毁资源 consumer.close(); session.close(); connection.close(); &#125; catch (JMSException e) &#123; System.out.println(&quot;Caught: &quot; + e); e.printStackTrace(); &#125; &#125; @Override public synchronized void onException(JMSException exception) &#123; System.out.println(&quot;JMS Exception occured. Shutting down client.&quot;); &#125;&#125; 测试类12345678910111213141516171819202122232425262728293031323334353637public class App &#123; public static void main(String[] args) throws InterruptedException &#123; thread(new HelloWorldProducer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); Thread.sleep(1000); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); Thread.sleep(1000); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldProducer(), false); Thread.sleep(1000); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldConsumer(), false); thread(new HelloWorldProducer(), false); &#125; public static void thread(Runnable runnable, boolean daemon) &#123; Thread brokerThread = new Thread(runnable); brokerThread.setDaemon(daemon); brokerThread.start(); &#125;&#125; 运行我们的测试程序，控制台将会打印： 12345678910111213Sent message: 507732978 : Thread-6Sent message: 2056557229 : Thread-0Sent message: 39234146 : Thread-8Sent message: 1100925878 : Thread-13Sent message: 1566392082 : Thread-17Sent message: 1329793151 : Thread-1Sent message: 988436874 : Thread-16Received: Hello world! From: Thread-6 : 1442537083Received: Hello world! From: Thread-1 : 1531760310Received: Hello world! From: Thread-0 : 1817576164Received: Hello world! From: Thread-8 : 262381200Received: Hello world! From: Thread-17 : 1647178742Received: Hello world! From: Thread-13 : 1610404140","tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://gcdd1993.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"https://gcdd1993.github.io/tags/ActiveMQ/"}]},{"title":"消息队列（二）RabbitMQ","date":"2019-04-01T10:20:18.000Z","path":"p/45284/","text":"在Ubuntu上安装RabbitMQ系统初始化12345678$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean$ echo 127.0.0.1 mq &gt; /etc/hosts$ echo rabbitmq &gt; /etc/hostname$ export HOSTNAME=mq 搭建rabbitmq服务1234$ echo &#x27;deb http://www.rabbitmq.com/debian/ testing main&#x27;| sudo tee /etc/apt/sources.list.d/rabbitmq.list$ wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install rabbitmq-server 创建管理账户1234567$ sudo rabbitmqctl add_user test test$ sudo rabbitmqctl add_vhost /test$ sudo rabbitmqctl set_user_tags test administrator$ sudo rabbitmqctl set_permissions -p /test test &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;$ sudo rabbitmq-plugins enable rabbitmq_management AMQP规范AMQP（高级消息队列协议）是一个网络协议。它支持符合要求的客户端应用（application）和消息中间件代理（messaging middleware broker）之间进行通信。 消息代理和他们所扮演的角色消息代理（message brokers）从发布者（publishers）亦称生产者（producers）那儿接收消息，并根据既定的路由规则把接收到的消息发送给处理消息的消费者（consumers）。 由于AMQP是一个网络协议，所以这个过程中的发布者，消费者，消息代理 可以存在于不同的设备上。 AMQP 0-9-1 模型简介AMQP 0-9-1的工作过程如下图：消息（message）被发布者（publisher）发送给交换机（exchange），交换机常常被比喻成邮局或者邮箱。然后交换机将收到的消息根据路由规则分发给绑定的队列（queue）。最后AMQP代理会将消息投递给订阅了此队列的消费者，或者消费者按照需求自行获取。 队列，交换机和绑定统称为AMQP实体（AMQP entities）。 交换机和交换机类型交换机是用来发送消息的AMQP实体。交换机拿到一个消息之后将它路由给一个或零个队列。它使用哪种路由算法是由交换机类型和被称作绑定（bindings）的规则所决定的。AMQP 0-9-1的代理提供了四种交换机 Name（交换机类型） Default pre-declared names（预声明的默认名称） Direct exchange（直连交换机） (Empty string) and amq.direct Fanout exchange（扇型交换机） amq.fanout Topic exchange（主题交换机） amq.topic Headers exchange（头交换机） amq.match (and amq.headers in RabbitMQ) 除交换机类型外，在声明交换机时还可以附带许多其他的属性，其中最重要的几个分别是： Name Durability （消息代理重启后，交换机是否还存在） Auto-delete （当所有与之绑定的消息队列都完成了对此交换机的使用后，删掉它） Arguments（依赖代理本身） 交换机可以有两个状态：持久（durable）、暂存（transient）。持久化的交换机会在消息代理（broker）重启后依旧存在，而暂存的交换机则不会（它们需要在代理再次上线后重新被声明）。 队列AMQP中的队列（queue）跟其他消息队列或任务队列中的队列是很相似的：它们存储着即将被应用消费掉的消息。 队列跟交换机共享某些属性，但是队列也有一些另外的属性。 Name Durable（消息代理重启后，队列依旧存在） Exclusive（只被一个连接（connection）使用，而且当连接关闭后队列即被删除） Auto-delete（当最后一个消费者退订后即被删除） Arguments（一些消息代理用他来完成类似与TTL的某些额外功能） 队列在声明（declare）后才能被使用。如果一个队列尚不存在，声明一个队列会创建它。如果声明的队列已经存在，并且属性完全相同，那么此次声明不会对原有队列产生任何影响。如果声明中的属性与已存在队列的属性有差异，那么一个错误代码为406的通道级异常就会被抛出。 绑定绑定（Binding）是交换机（exchange）将消息（message）路由给队列（queue）所需遵循的规则。 消费者 将消息投递给应用 (“push API”) 应用根据需要主动获取消息 (“pull API”) 消息确认 自动确认：当消息代理（broker）将消息发送给应用后立即删除。 显式确认：待应用（application）发送一个确认回执（acknowledgement）后再删除消息。 拒绝消息当拒绝一条消息时，可以 销毁消息 重新放入消息队列 当此队列只有一个消费者时，请确认不要由于拒绝消息并且选择了重新放入队列的行为而引起消息在同一个消费者身上无限循环的情况发生。 Hello World 生产(Producing)的意思就是发送。发送消息的程序就是一个生产者(producer)。我们一般用”P”来表示: 队列(queue)就是存在于RabbitMQ中邮箱的名称。虽然消息的传输经过了RabbitMQ和你的应用程序，但是它只能被存储于队列当中。实质上队列就是个巨大的消息缓冲区，它的大小只受主机内存和硬盘限制。多个生产者（producers）可以把消息发送给同一个队列，同样，多个消费者（consumers）也能够从同一个队列（queue）中获取数据。队列可以绘制成这样（图上是队列的名称）： 在这里，消费（Consuming）和接收(receiving)是同一个意思。一个消费者（consumer）就是一个等待获取消息的程序。我们把它绘制为”C”： 需要指出的是生产者、消费者、代理需不要待在同一个设备上；事实上大多数应用也确实不在会将他们放在一台机器上。 创建gradle项目，并配置build.gradle： 1compile group: &#x27;com.rabbitmq&#x27;, name: &#x27;amqp-client&#x27;, version: &#x27;5.6.0&#x27; 创建生产者12345678910111213141516171819public class Send &#123; private final static String QUEUE_NAME = &quot;hello&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; // 1. 创建RabbitMQ连接工厂 ConnectionFactory factory = new ConnectionFactory(); // 2. 设置host,rabbitmq-server的监听地址 factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); // 4. 创建频道 Channel channel = connection.createChannel(); // 5. 连接到具体频道 channel.queueDeclare(QUEUE_NAME, false, false, false, null); String message = &quot;Hello World!&quot;; // 6. 发布消息 channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); &#125;&#125; 创建消费者1234567891011121314151617181920public class Recv &#123; private final static String QUEUE_NAME = &quot;hello&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); // 4. 创建频道 Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received &#x27;&quot; + message + &quot;&#x27;&quot;); &#125;; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125;&#125; 可以看出，生产者和消费者需要声明是同一个队列 测试我们先执行Send.main，控制台将打印： 1[x] Sent &#x27;Hello World!&#x27; 然后执行Recv.main，控制台将打印： 12[*] Waiting for messages. To exit press CTRL+C[x] Received &#x27;Hello World!&#x27; 任务队列 工作队列（又称：任务队列——Task Queues）是为了避免等待一些占用大量资源、时间的操作。当我们把任务（Task）当作消息发送到队列中，一个运行在后台的工作者（worker）进程就会取出任务然后处理。当你运行多个工作者（workers），任务就会在它们之间共享。 这个概念在网络应用中是非常有用的，它可以在短暂的HTTP请求中处理一些复杂的任务。 修改Send.java代码，来间隔10秒发送一个消息： 1234567for (int i = 1; i &lt;= 100; i++) &#123; String message = String.format(&quot;发送第%d条消息&quot;, i); // 6. 发布消息 channel.basicPublish(&quot;&quot;, &quot;hello&quot;, null, message.getBytes()); System.out.println(&quot; [x] Sent &#x27;&quot; + message + &quot;&#x27;&quot;); Thread.sleep(10000);&#125; 修改Recv.java，来完成一个任务，这里，假装任务执行需要耗时1s： 1234567891011121314151617181920DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received &#x27;&quot; + message + &quot;&#x27;&quot;); try &#123; doWork(message); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(&quot; [x] Done&quot;); &#125;&#125;;channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; &#123;&#125;);private static void doWork(String task) throws InterruptedException &#123; for (char ch : task.toCharArray()) &#123; if (ch == &#x27;-&#x27;) &#123; Thread.sleep(1000); &#125; &#125;&#125; 我们先开启Recv.java，然后开启Send.java，控制台将会打印 Send.java 123[x] Sent &#x27;发送第1条消息&#x27;[x] Sent &#x27;发送第2条消息&#x27;[x] Sent &#x27;发送第3条消息&#x27; Recv.java 123456[*] Waiting for messages. To exit press CTRL+C[x] Received &#x27;发送第1条消息&#x27;[x] Done[x] Received &#x27;发送第2条消息&#x27;[x] Done[x] Received &#x27;发送第3条消息&#x27; 循环调度使用工作队列的一个好处就是它能够并行的处理队列。如果堆积了很多任务，我们只需要添加更多的工作者（workers）就可以了，扩展很简单。 让我们尝试同时运行两个worker实例，他们都会从队列中获取消息： Send.java 123[x] Sent &#x27;发送第1条消息&#x27;[x] Sent &#x27;发送第2条消息&#x27;[x] Sent &#x27;发送第3条消息&#x27; Recv.java-1 12345[*] Waiting for messages. To exit press CTRL+C[x] Received &#x27;发送第1条消息&#x27;[x] Done[x] Received &#x27;发送第3条消息&#x27;[x] Done Recv.java-2 123[*] Waiting for messages. To exit press CTRL+C[x] Received &#x27;发送第2条消息&#x27;[x] Done 默认来说，RabbitMQ会按顺序得把消息发送给每个消费者（consumer）。平均每个消费者都会收到同等数量得消息。这种发送消息得方式叫做——轮询（round-robin）。试着添加三个或更多得工作者（workers）。 消息确认当处理一个比较耗时得任务的时候，你也许想知道消费者（consumers）是否运行到一半就挂掉。当前的代码中，当消息被RabbitMQ发送给消费者（consumers）之后，马上就会在内存中移除。这种情况，你只要把一个工作者（worker）停止，正在处理的消息就会丢失。同时，所有发送到这个工作者的还没有处理的消息都会丢失。 我们不想丢失任何任务消息。如果一个工作者（worker）挂掉了，我们希望任务会重新发送给其他的工作者（worker）。 为了防止消息丢失，RabbitMQ提供了消息响应（acknowledgments）。消费者会通过一个ack（响应），告诉RabbitMQ已经收到并处理了某条消息，然后RabbitMQ就会释放并删除这条消息。 如果消费者（consumer）挂掉了，没有发送响应，RabbitMQ就会认为消息没有被完全处理，然后重新发送给其他消费者（consumer）。这样，及时工作者（workers）偶尔的挂掉，也不会丢失消息。 消息是没有超时这个概念的；当工作者与它断开连的时候，RabbitMQ会重新发送消息。这样在处理一个耗时非常长的消息任务的时候就不会出问题了。 消息响应默认是开启的。之前的例子中我们可以使用no_ack=True标识把它关闭。是时候移除这个标识了，当工作者（worker）完成了任务，就发送一个响应。 修改Worker.java 12345678910111213141516// 一次只接受一条消息channel.basicQos(1);DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received &#x27;&quot; + message + &quot;&#x27;&quot;); try &#123; doWork(message); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); &#125;&#125;;boolean autoAck = false;channel.basicConsume(QUEUE_NAME, autoAck, deliverCallback, consumerTag -&gt; &#123;&#125;); 运行上面的代码，我们发现即使使用CTRL+C杀掉了一个工作者（worker）进程，消息也不会丢失。当工作者（worker）挂掉这后，所有没有响应的消息都会重新发送。 消息持久化如果你没有特意告诉RabbitMQ，那么在它退出或者崩溃的时候，将会丢失所有队列和消息。为了确保信息不会丢失，有两个事情是需要注意的：我们必须把“队列”和“消息”设为持久化。 首先，为了不让队列消失，需要把队列声明为持久化（durable）： 12boolean durable = true;channel.queueDeclare(QUEUE_NAME, durable, false, false, null); 尽管这行代码本身是正确的，但是仍然不会正确运行。因为我们已经定义过一个叫hello的非持久化队列。RabbitMq不允许你使用不同的参数重新定义一个队列，它会返回一个错误。但我们现在使用一个快捷的解决方法——用不同的名字，例如task_queue。 12boolean durable = true;channel.queueDeclare(&quot;task_queue&quot;, durable, false, false, null); 这时候，我们就可以确保在RabbitMq重启之后queue_declare队列不会丢失。现在我们需要将消息标记为持久性 - 通过将MessageProperties（实现BasicProperties）设置为值PERSISTENT_TEXT_PLAIN。 12345import com.rabbitmq.client.MessageProperties;channel.basicPublish(&quot;&quot;, &quot;task_queue&quot;, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes()); 公平调度你应该已经发现，它仍旧没有按照我们期望的那样进行分发。比如有两个工作者（workers），处理奇数消息的比较繁忙，处理偶数消息的比较轻松。然而RabbitMQ并不知道这些，它仍然一如既往的派发消息。 这时因为RabbitMQ只管分发进入队列的消息，不会关心有多少消费者（consumer）没有作出响应。它盲目的把第n-th条消息发给第n-th个消费者。 我们可以使用basicQos方法，并设置prefetchCount = 1。这样是告诉RabbitMQ，再同一时刻，不要发送超过1条消息给一个工作者（worker），直到它已经处理了上一条消息并且作出了响应。这样，RabbitMQ就会把消息分发给下一个空闲的工作者（worker）。 12int prefetchCount = 1;channel.basicQos(prefetchCount); 发布／订阅在上篇教程中，我们搭建了一个工作队列，每个任务只分发给一个工作者（worker）。在本篇教程中，我们要做的跟之前完全不一样 —— 分发一个消息给多个消费者（consumers）。这种模式被称为“发布／订阅”。 为了描述这种模式，我们将会构建一个简单的日志系统。 交换机（Exchanges）RabbitMQ中完整的消息模型： 发布者（producer）是发布消息的应用程序。 队列（queue）用于消息存储的缓冲。 消费者（consumer）是接收消息的应用程序。 RabbitMQ消息模型的核心理念是：发布者（producer）不会直接发送任何消息给队列。事实上，发布者（producer）甚至不知道消息是否已经被投递到队列。 发布者（producer）只需要把消息发送给一个交换机（exchange）。交换机非常简单，它一边从发布者方接收消息，一边把消息推送到队列。交换机必须知道如何处理它接收到的消息，是应该推送到指定的队列还是是多个队列，或者是直接忽略消息。这些规则是通过交换机类型（exchange type）来定义的。 有几个可供选择的交换机类型：直连交换机（direct）, 主题交换机（topic）, （头交换机）headers和 扇型交换机（fanout）。我们在这里主要说明最后一个 —— 扇型交换机（fanout）。先创建一个fanout类型的交换机，命名为logs： 1channel.exchangeDeclare(&quot;logs&quot;, &quot;fanout&quot;); 扇型交换机（fanout）很简单，你可能从名字上就能猜测出来，它把消息发送给它所知道的所有队列。 现在，我们就可以发送消息到一个具名交换机了： 1channel.basicPublish( &quot;logs&quot;, &quot;&quot;, null, message.getBytes()); 临时队列要创建一个临时队列，我们需要做两件事情： 当我们连接上RabbitMQ的时候，我们需要一个全新的、空的队列。我们可以手动创建一个随机的队列名，或者让服务器为我们选择一个随机的队列名（推荐）。 当与消费者（consumer）断开连接的时候，这个队列应当被立即删除。 在Java客户端中，当我们没有向queueDeclare（）提供参数时，我们使用生成的名称创建一个非持久的，独占的自动删除队列： 12// 服务器分配的随机队列名，可能像这样 amq.gen-U0srCoW8TsaXjNh73pnVAw==String queueName = channel.queueDeclare().getQueue(); 绑定（Bindings） 我们已经创建了一个扇型交换机（fanout）和一个队列。现在我们需要告诉交换机如何发送消息给我们的队列。交换器和队列之间的联系我们称之为绑定（binding）。 1channel.queueBind(queueName, &quot;logs&quot;, &quot;&quot;); 路由(Routing)前面的例子，我们已经创建过绑定（bindings），代码如下： 1channel.queueBind(queueName, EXCHANGE_NAME, &quot;&quot;); 绑定（binding）是指交换机（exchange）和队列（queue）的关系。可以简单理解为：这个队列（queue）对这个交换机（exchange）的消息感兴趣。 绑定的时候可以带上一个额外的routing_key参数。为了避免与basic_publish的参数混淆，我们把它叫做绑定键（binding key）。以下是如何创建一个带绑定键的绑定。 1channel.queueBind(queueName, EXCHANGE_NAME, &quot;black&quot;); 绑定键的意义取决于交换机（exchange）的类型。我们之前使用过的扇型交换机（fanout exchanges）会忽略这个值。 直连交换机（Direct exchange）我们的日志系统广播所有的消息给所有的消费者（consumers）。我们打算扩展它，使其基于日志的严重程度进行消息过滤。 我们使用的扇型交换机（fanout exchange）没有足够的灵活性 —— 它能做的仅仅是广播。 我们将会使用直连交换机（direct exchange）来代替。路由的算法很简单 —— 交换机将会对绑定键（binding key）和路由键（routing key）进行精确匹配，从而确定消息该分发到哪个队列。 下图能够很好的描述这个场景： 在这个场景中，我们可以看到直连交换机 X和两个队列进行了绑定。第一个队列使用orange作为绑定键，第二个队列有两个绑定，一个使用black作为绑定键，另外一个使用green。 这样以来，当路由键为orange的消息发布到交换机，就会被路由到队列Q1。路由键为black或者green的消息就会路由到Q2。其他的所有消息都将会被丢弃。 多个绑定（Multiple bindings） 多个队列使用相同的绑定键是合法的。这个例子中，我们可以添加一个X和Q1之间的绑定，使用black绑定键。这样一来，直连交换机就和扇型交换机的行为一样，会将消息广播到所有匹配的队列。带有black路由键的消息会同时发送到Q1和Q2。 发送日志我们将会发送消息到一个直连交换机，把日志级别作为路由键。这样接收日志的脚本就可以根据严重级别来选择它想要处理的日志。我们先看看发送日志。 我们需要创建一个交换机（exchange）： 1channel.exchangeDeclare(EXCHANGE_NAME, &quot;direct&quot;); 然后我们发送一则消息： 1channel.basicPublish(EXCHANGE_NAME, severity, null, message.getBytes()); 订阅处理接收消息的方式和之前差不多，只有一个例外，我们将会为我们感兴趣的每个严重级别分别创建一个新的绑定。 12345String queueName = channel.queueDeclare().getQueue();for(String severity : argv)&#123; channel.queueBind(queueName, EXCHANGE_NAME, severity);&#125; 示例代码 Routing 主题交换机直连交换机的限制 —— 没办法基于多个标准执行路由操作。 发送到主题交换机（topic exchange）的消息不可以携带随意什么样子的路由键（routing_key），它的路由键必须是一个由.分隔开的词语列表。这些单词随便是什么都可以，但是最好是跟携带它们的消息有关系的词汇。以下是几个推荐的例子：”stock.usd.nyse”, “nyse.vmw”, “quick.orange.rabbit”。词语的个数可以随意，但是不要超过255字节。 绑定键也必须拥有同样的格式。主题交换机背后的逻辑跟直连交换机很相似 —— 一个携带着特定路由键的消息会被主题交换机投递给绑定键与之想匹配的队列。但是它的绑定键和路由键有两个特殊应用方式： * (星号) 用来表示一个单词. # (井号) 用来表示任意数量（零个或多个）单词。 下边用图说明： 我们创建了三个绑定：Q1的绑定键为 *.orange.*，Q2的绑定键为 *.*.rabbit 和 lazy.# 。 这三个绑定键被可以总结为： Q1 对所有的桔黄色动物都感兴趣。 Q2 则是对所有的兔子和所有懒惰的动物感兴趣。 主题交换机是很强大的，它可以表现出跟其他交换机类似的行为 当一个队列的绑定键为 “#”（井号） 的时候，这个队列将会无视消息的路由键，接收所有的消息。 当 * (星号) 和 # (井号) 这两个特殊字符都未在绑定键中出现的时候，此时主题交换机就拥有的直连交换机的行为。 远程过程调用（RPC）如果我们需要将一个函数运行在远程计算机上并且等待从那儿获取结果时，这种模式通常被称为远程过程调用（Remote Procedure Call）或者RPC。 我们会使用RabbitMQ来构建一个RPC系统：包含一个客户端和一个RPC服务器。 客户端接口为了展示RPC服务如何使用，我们创建了一个简单的客户端类。它会暴露出一个名为“call”的方法用来发送一个RPC请求，并且在收到回应前保持阻塞。 123FibonacciRpcClient fibonacciRpc = new FibonacciRpcClient();String result = fibonacciRpc.call(&quot;4&quot;);System.out.println( &quot;fib(4) is &quot; + result); 回调队列一般来说通过RabbitMQ来实现RPC是很容易的。一个客户端发送请求信息，服务器端将其应用到一个回复信息中。为了接收到回复信息，客户端需要在发送请求的时候同时发送一个回调队列（callback queue）的地址。 12345678callbackQueueName = channel.queueDeclare().getQueue();BasicProperties props = new BasicProperties .Builder() .replyTo(callbackQueueName) .build();channel.basicPublish(&quot;&quot;, &quot;rpc_queue&quot;, props, message.getBytes()); 消息属性AMQP协议给消息预定义了一系列的14个属性。大多数属性很少会用到，除了以下几个： delivery_mode（投递模式）：将消息标记为持久的（值为2）或暂存的（除了2之外的其他任何值）。第二篇教程里接触过这个属性，记得吧？ content_type（内容类型）:用来描述编码的mime-type。例如在实际使用中常常使用application/json来描述JOSN编码类型。 reply_to（回复目标）：通常用来命名回调队列。 correlation_id（关联标识）：用来将RPC的响应和请求关联起来。 关联标识上边介绍的方法中，我们建议给每一个RPC请求新建一个回调队列。这不是一个高效的做法，幸好这儿有一个更好的办法 —— 我们可以为每个客户端只建立一个独立的回调队列。 这就带来一个新问题，当此队列接收到一个响应的时候它无法辨别出这个响应是属于哪个请求的。correlation_id 就是为了解决这个问题而来的。我们给每个请求设置一个独一无二的值。稍后，当我们从回调队列中接收到一个消息的时候，我们就可以查看这条属性从而将响应和请求匹配起来。如果我们接手到的消息的correlation_id是未知的，那就直接销毁掉它，因为它不属于我们的任何一条请求。 为什么我们接收到未知消息的时候不抛出一个错误，而是要将它忽略掉？这是为了解决服务器端有可能发生的竞争情况。尽管可能性不大，但RPC服务器还是有可能在已将应答发送给我们但还未将确认消息发送给请求的情况下死掉。如果这种情况发生，RPC在重启后会重新处理请求。这就是为什么我们必须在客户端优雅的处理重复响应，同时RPC也需要尽可能保持幂等性。 总结 我们的RPC如此工作: 当客户端启动的时候，它创建一个匿名独享的回调队列。 在RPC请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。 将请求发送到一个 rpc_queue 队列中。 RPC工作者（又名：服务器）等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给reply_to字段指定的队列。 客户端等待回调队列里的数据。当有消息出现的时候，它会检查correlation_id属性。如果此属性的值与请求匹配，将它返回给应用。","tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://gcdd1993.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gcdd1993.github.io/tags/RabbitMQ/"}]},{"title":"消息队列（一）简介","date":"2019-04-01T10:01:43.000Z","path":"p/27791/","text":"消息队列(MQ)概述消息队列（Message Queue），是分布式系统中重要的组件，其通用的使用场景可以简单地描述为： 当不需要立即获得结果，但是并发量又需要进行控制的时候，差不多就是需要使用消息队列的时候。 消息队列主要解决了应用耦合、异步处理、流量削锋等问题。 当前使用较多的消息队列有RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、MetaMq等，而部分数据库如Redis、Mysql以及phxsql也可实现消息队列的功能。 消息队列使用场景消息队列在实际应用中包括如下四个场景： 应用耦合：多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败； 异步处理：多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间； 限流削峰：广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况； 消息驱动的系统：系统分为消息队列、消息生产者、消息消费者，生产者负责产生消息，消费者(可能有多个)负责对消息进行处理； 下面详细介绍上述四个场景以及消息队列如何在上述四个场景中使用： 异步处理具体场景：用户为了使用某个应用，进行注册，系统需要发送注册邮件并验证短信。对这两个操作的处理方式有两种：串行及并行。 串行方式新注册信息生成后，先发送注册邮件，再发送验证短信； 在这种方式下，需要最终发送验证短信后再返回给客户端。 并行处理新注册信息写入后，由发短信和发邮件并行处理； 在这种方式下，发短信和发邮件 需处理完成后再返回给客户端。 假设以上三个子系统处理的时间均为50ms，且不考虑网络延迟，则总的处理时间： 串行：50+50+50=150ms 并行：50+50 = 100ms 使用消息队列 并在写入消息队列后立即返回成功给客户端，则总的响应时间依赖于写入消息队列的时间，而写入消息队列的时间本身是可以很快的，基本可以忽略不计，因此总的处理时间相比串行提高了2倍，相比并行提高了一倍； 应用耦合具体场景：用户使用QQ相册上传一张图片，人脸识别系统会对该图片进行人脸识别，一般的做法是，服务器接收到图片后，图片上传系统立即调用人脸识别系统，调用完成后再返回成功，如下图所示： 该方法有如下缺点： 人脸识别系统被调失败，导致图片上传失败； 延迟高，需要人脸识别系统处理完成后，再返回给客户端，即使用户并不需要立即知道结果； 图片上传系统与人脸识别系统之间互相调用，需要做耦合； 若使用消息队列： 客户端上传图片后，图片上传系统将图片信息如uin、批次写入消息队列，直接返回成功；而人脸识别系统则定时从消息队列中取数据，完成对新增图片的识别。 此时图片上传系统并不需要关心人脸识别系统是否对这些图片信息的处理、以及何时对这些图片信息进行处理。事实上，由于用户并不需要立即知道人脸识别结果，人脸识别系统可以选择不同的调度策略，按照闲时、忙时、正常时间，对队列中的图片信息进行处理。 限流削峰具体场景：购物网站开展秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统可以从消息队列中取数据，相当于消息队列做了一次缓冲。 该方法有如下优点： 请求先入消息队列，而不是由业务处理系统直接处理，做了一次缓冲,极大地减少了业务处理系统的压力； 队列长度可以做限制，事实上，秒杀时，后入队列的用户无法秒杀到商品，这些请求可以直接被抛弃，返回活动已结束或商品已售完信息； 消息驱动的系统具体场景：用户新上传了一批照片， 人脸识别系统需要对这个用户的所有照片进行聚类，聚类完成后由对账系统重新生成用户的人脸索引(加快查询)。这三个子系统间由消息队列连接起来，前一个阶段的处理结果放入队列中，后一个阶段从队列中获取消息继续处理。 该方法有如下优点： 避免了直接调用下一个系统导致当前系统失败； 每个子系统对于消息的处理方式可以更为灵活，可以选择收到消息时就处理，可以选择定时处理，也可以划分时间段按不同处理速度处理； 消息队列的两种模式消息队列包括两种模式，点对点模式（point to point， queue）和发布/订阅模式（publish/subscribe，topic）。 点对点模式 消息队列 发送者 (生产者) 接收者（消费者） 消息发送者生产消息发送到queue中，然后消息接收者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息接收者不可能消费到已经被消费的消息。 点对点模式特点： 每个消息只有一个接收者（Consumer）(即一旦被消费，消息就不再在消息队列中)； 发送者和接收者间没有依赖性，发送者发送消息之后，不管有没有接收者在运行，都不会影响到发送者下次发送消息； 接收者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接收的消息； 发布/订阅模式发布/订阅模式下包括三个角色： 角色主题（Topic） 发布者(Publisher) 订阅者(Subscriber) 发布者将消息发送到Topic,系统将这些消息传递给多个订阅者。 发布/订阅模式特点： 每个消息可以有多个订阅者； 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息； 为了消费消息，订阅者需要提前订阅该角色主题，并保持在线运行； 常用消息队列 RabbitMQ ActiveMQ RocketMQ Apache Kafka","tags":[{"name":"mq","slug":"mq","permalink":"https://gcdd1993.github.io/tags/mq/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://gcdd1993.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Spring IoC Container源码分析（二）-bean初始化流程","date":"2019-03-29T02:11:12.000Z","path":"p/60483/","text":"准备Person实例 12345@Datapublic class Person &#123; private String name; private int age;&#125; xml bean配置 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;bean id=&quot;person&quot; class=&quot;com.gcdd1993.spring.framework.base.domain.Person&quot;/&gt;&lt;/beans&gt; 入口 12AbstractApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;config.xml&quot;);applicationContext.getBean(&quot;person&quot;); 使用Debug进入ClassPathXmlApplicationContext构造函数，源码如下 123456789public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; super(parent)一步步向上调用父类构造函数，路径为 ClassPathXmlApplicationContext -&gt; AbstractXmlApplicationContext -&gt; AbstractRefreshableConfigApplicationContext -&gt; AbstractRefreshableApplicationContext -&gt; AbstractApplicationContext 历经整个继承体系，最终到达AbstractApplicationContext: 1234public AbstractApplicationContext(ApplicationContext parent) &#123; this(); setParent(parent);&#125; 最后会设置当前ApplicationContext的父级ApplicationContext setConfigLocations(configLocations)设置配置文件路径，解析的细节参照官方文档Resource一节，不是本文讨论的重点，在此略过。 123456789101112public void setConfigLocations(String... locations) &#123; if (locations != null) &#123; Assert.noNullElements(locations, &quot;Config locations must not be null&quot;); this.configLocations = new String[locations.length]; for (int i = 0; i &lt; locations.length; i++) &#123; this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125;&#125; refresh()此方法是Spring容器的核心方法，源码(精简了try catch部分)如下： 12345678910111213141516171819202122232425262728293031323334353637public void refresh() throws BeansException, IllegalStateException &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh();&#125; 此处可以看到Spring编码方式近似于流程图的，重点部分都抽出为了单独的方法，流程清晰，易于理解。我们一步步看： prepareRefresh() 上下文刷新前预热 1234567891011121314151617181920protected void prepareRefresh() &#123; this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Refreshing &quot; + this); &#125; // Initialize any placeholder property sources in the context environment initPropertySources(); // Validate that all properties marked as required are resolvable // see ConfigurablePropertyResolver#setRequiredProperties getEnvironment().validateRequiredProperties(); // Allow for the collection of early ApplicationEvents, // to be published once the multicaster is available... this.earlyApplicationEvents = new LinkedHashSet&lt;ApplicationEvent&gt;();&#125; 设置上下文基本信息，如startupDate(启动时刻)、closed(是否关闭)、active(是否存活)等等。 解析占位符资源，并验证标记为required的资源是否可用 obtainFreshBeanFactory() 初始化beanFactory(bean工厂，实际存放bean的就是它了) 12345678protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; refreshBeanFactory(); ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean factory for &quot; + getDisplayName() + &quot;: &quot; + beanFactory); &#125; return beanFactory;&#125; 核心方法refreshBeanFactory() 123456789101112131415161718protected final void refreshBeanFactory() throws BeansException &#123; if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; createBeanFactory(); 设置beanFactory属性 loadBeanDefinitions(beanFactory); loadBeanDefinitions(beanFactory) 解析bean定义，有几个bean就有几个BeanDefinition。注意，Spring并不是拿到配置就直接用反射实例化bean，而是先将bean配置解析为BeanDefinition。 BeanDefinition保存了实例化bean需要的一切信息，包括属性，依赖等。以ConcurrentHashMap&lt;String, BeanDefinition&gt;保存在DefaultListableBeanFactory的beanDefinitionMap里。 prepareBeanFactory(beanFactory) 设置beanFactory的其余属性 postProcessBeanFactory(beanFactory) 空实现，给子类一个机会，自定义beanFactory后置处理器 BeanFactoryPostProcessor定义： 12345public interface BeanFactoryPostProcessor &#123; void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;&#125; invokeBeanFactoryPostProcessors(beanFactory) 执行上一步中的beanFactory后置处理器的回调方法void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) registerBeanPostProcessors(beanFactory) 注册bean后置处理器，实现bean初始化前后的自定义逻辑 BeanPostProcessor定义： 123456public interface BeanPostProcessor &#123; // 在bean实例化前调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; // 在bean实例化后调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; initMessageSource() 注册国际化相关bean initApplicationEventMulticaster() 初始化Spring事件发布相关bean onRefresh() 空实现，给子类一个机会，初始化特殊bean registerListeners() 注册监听器 finishBeanFactoryInitialization(beanFactory) 实例化所有非懒加载的bean 直到这里，才开始真正实例化bean 123456789101112131415161718192021222324252627282930313233protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 1. 实例化bean的类型转换器 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // 2. 实例化属性占位符解析器 if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(new StringValueResolver() &#123; @Override public String resolveStringValue(String strVal) &#123; return getEnvironment().resolvePlaceholders(strVal); &#125; &#125;); &#125; // 3. 实例化LoadTimeWeaverAware String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // 4. 停止使用临时ClassLoader进行类型匹配 beanFactory.setTempClassLoader(null); // 5. 禁止再修改bean定义 beanFactory.freezeConfiguration(); // 6. 实例化所有非懒加载单例bean beanFactory.preInstantiateSingletons();&#125; preInstantiateSingletons() 根据每一个bean定义，实例化bean 为每一个实现SmartInitializingSingleton的bean执行回调方法 实例化bean部分的代码： 1234567891011121314151617181920212223242526272829303132for (String beanName : beanNames) &#123; // 获取bean定义 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 只有不是abstract、单例且不是懒加载的bean才在这里实例化 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // 如果是FactoryBean if (isFactoryBean(beanName)) &#123; // 先实例化实例对应的FactoryBean final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; // 使用FactoryBean的getObject()方法返回真正的实例 getBean(beanName); &#125; &#125; else &#123; getBean(beanName); &#125; &#125;&#125; getBean(String name)该方法调用了一个doGetBean，doGetBean代码较长，而且有部分代码是为了解决并发场景下单例的生成，我们挑出重点的看： 从父BeanFactory检查是否存在该bean的定义，如果存在，委托父BeanFactory来实例化 12345678910111213BeanFactory parentBeanFactory = getParentBeanFactory();if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (args != null) &#123; // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125;&#125; 获得bean定义，如果存在依赖，先实例化每一个依赖bean，注意：不允许循环依赖 12345678910111213141516171819202122232425final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);checkMergedBeanDefinition(mbd, beanName, args);// Guarantee initialization of beans that the current bean depends on.String[] dependsOn = mbd.getDependsOn();//如果存在依赖，先实例化每一个依赖beanif (dependsOn != null) &#123; // 实例化每一个依赖bean for (String dep : dependsOn) &#123; // 检查循环依赖 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; // 实例化依赖bean registerDependentBean(dep, beanName); try &#123; getBean(dep); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;&#x27;&quot; + beanName + &quot;&#x27; depends on missing bean &#x27;&quot; + dep + &quot;&#x27;&quot;, ex); &#125; &#125;&#125; 实例化bean 方法调用流程： createBean &gt; doCreateBean &gt; populateBean 其中doCreateBean： 从BeanDefinition生成BeanWrapper 将BeanWrapper和BeanDefinition.getPropertyValues() 传给populateBean，实例化bean finishRefresh()12345678910111213protected void finishRefresh() &#123; // 初始化生命周期处理器 initLifecycleProcessor(); // 刷新生命周期处理器状态 running = true getLifecycleProcessor().onRefresh(); // 发布上下文初始化完成事件ContextRefreshedEvent publishEvent(new ContextRefreshedEvent(this)); // 如果处于活动状态，将自己注册到LiveBeans LiveBeansView.registerApplicationContext(this);&#125; 总结Spring IoC Container时序图","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"FastDFS 单机部署指南","date":"2019-03-22T07:39:17.000Z","path":"p/56864/","text":"简介FastDFS是一个开源的分布式文件系统，官方介绍有详细的介绍，不多赘述。本文主要是FastDFS的搭建及采坑指南。 Step By Step Guide系统 阿里云ECS Ubuntu 16.04 编译环境按需安装，这里是针对新的ubuntu系统 1$ apt-get install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim 磁盘目录 说明 位置 所有安装包 /usr/local/src 数据存储位置 /data/dfs/ 12$ mkdir /data/dfs #创建数据存储目录（对于阿里云ECS，最好建立在数据盘上，是用来存放文件的）$ cd /usr/local/src #切换到安装目录准备下载安装包 安装libfatscommon1234$ wget https://github.com/happyfish100/libfastcommon/archive/master.zip$ unzip master.zip$ cd libfastcommon-1.0.39/$ ./make.sh &amp;&amp; ./make.sh install #编译安装 安装FastDFS1234567891011$ cd ../ #返回上一级目录$ wget https://github.com/happyfish100/fastdfs/archive/master.zip$ unzip master.zip$ cd fastdfs-master/$ ./make.sh &amp;&amp; ./make.sh install #编译安装#配置文件准备$ cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf$ cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf$ cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用$ cp /usr/local/src/fastdfs-master/conf/http.conf /etc/fdfs/ #供nginx访问使用$ cp /etc/nginx/mime.types /etc/fdfs/ #供nginx访问使用 安装fastdfs-nginx-module官网的文档，是针对没有安装过Nginx的机器，重新编译了一遍Nginx，把module直接编译进Nginx了。但是针对已经安装Nginx的服务器来说，显然是不好的。 根据Nginx官方文档-编译第三方动态模块，编译了fastdfs-nginx-module，以供已存在的Nginx使用。 我已经编译好了fastdfs-nginx-module，可以直接下载，并跳到加载并使用模块，如果想知其所以然，可以往下看。 准备fastdfs-nginx-module源码包123$ cd ../ #返回上一级目录$ wget https://github.com/happyfish100/fastdfs-nginx-module/archive/master.zip$ unzip master.zip 获取对应版本的Nginx源码包1234$ nginx -v # 确认服务器的Nginx版本nginx version: nginx/1.14.2$ wget http://nginx.org/download/nginx-1.14.2.tar.gz$ tar -xzvf nginx-1.14.2.tar.gz 编译动态模块123$ cd nginx-1.14.2/$ ./configure --with-compat --add-dynamic-module=/usr/local/src/fastdfs-nginx-module-master/src$ make modules 将模块库（.so文件）复制到/etc/nginx/modules1$ cp ngx_http_fastdfs_module.so /etc/nginx/modules/ 加载并使用模块Tips: 要将模块加载到Nginx,在nginx.conf文件开头添加load_module命令 1234$ vim /etc/nginx/nginx.conf# 添加如下命令load_module modules/ngx_http_fastdfs_module.so;# 保存退出 添加FastDFS配置使模块生效12345678910111213$ vim /etc/nginx/conf.d/fastdfs.conf# 添加如下配置server &#123; listen 8888; ## 该端口为storage.conf中的http.server_port相同 server_name &#123;your_domain&#125;; location ~/group[0-9]/ &#123; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 单机部署这里只描述下单机环境的部署方式，集群在官方文档有，没有实际使用过。 tracker配置123456$ vim /etc/fdfs/tracker.conf# 建议修改以下内容bind_addr=&#123;你的内网IP&#125;base_path=/data/dfs # 建议修改为数据盘位置# 可选修改port=22122 # tracker服务器端口 storage配置12345678$ vim /etc/fdfs/storage.conf# 建议修改base_path=/data/dfs # 数据和日志文件存储根目录（建议修改为数据盘位置）store_path0=/data/dfs # 第一个存储目录（建议修改为数据盘位置）tracker_server=&#123;tracker.bind_addr&#125;:&#123;tracker.port&#125; # tracker服务器IP和端口http.server_port=8888 # http访问文件的端口（默认8888,看情况修改,和nginx中保持一致）# 可选修改port=23000 # storage服务端口（默认23000,一般不修改） client测试12345678$ vim /etc/fdfs/client.conf# 建议修改base_path=/data/dfstracker_server=&#123;tracker.bind_addr&#125;:&#123;tracker.port&#125; # tracker服务器IP和端口# 保存后测试$ fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/nginx-1.14.2.tar.gzgroup1/M00/00/00/CgoKvlyUmi-AMVKDAA9-WL9wzEw.tar.gz # 下载时通过该ID下载# 返回ID表示成功 如：group1/M00/00/00/xx.tar.gz 配置nginx访问12345678910vim /etc/fdfs/mod_fastdfs.conf# 建议修改tracker_server=&#123;tracker.bind_addr&#125;:&#123;tracker.port&#125; # tracker服务器IP和端口url_have_group_name=truestore_path0=/data/dfs# 修改完保存$ nginx -s reloadngx_http_fastdfs_set pid=8364 # 看见这条消息说明nginx模块启动成功了$ lsof -i:8888 # 查看Nginx下载端口是否正常启动nginx 31061 root 10u IPv4 20389985 0t0 TCP *:8888 (LISTEN) 测试下载在浏览器输入 1http://&#123;IP&#125;:8888/group1/M00/00/00/CgoKvlyUmi-AMVKDAA9-WL9wzEw.tar.gz?filename=nginx-1.14.2.tar.gz //刚才上传返回的ID 弹出下载文件框，说明部署成功！ 相关命令防火墙1$ sudo ufw enable|disable tracker1234$ /etc/init.d/fdfs_trackerd start # 启动tracker服务$ /etc/init.d/fdfs_trackerd restart # 重启动tracker服务$ /etc/init.d/fdfs_trackerd stop # 停止tracker服务$ update-rc.d fdfs_trackerd enable # 自启动tracker服务 storage1234$ /etc/init.d/fdfs_storaged start # 启动storage服务$ /etc/init.d/fdfs_storaged restart # 重动storage服务$ /etc/init.d/fdfs_storaged stop # 停止动storage服务$ update-rc.d fdfs_storaged enable # 自启动storage服务 nginx123$ service nginx start # 启动nginx$ nginx -s reload # 重启nginx$ nginx -s stop # 停止nginx 问题执行nginx -s reload 后，访问50212# 查看nginx日志$ vim /var/log/nginx/error.log 如果发现错误日志：include file &quot;http.conf&quot; not exists, line: &quot;#include http.conf&quot;，fastdfs nginx模块缺少配置文件，执行以下命令补全配置文件即可。 12$ cp /usr/local/src/fastdfs-master/conf/http.conf /etc/fdfs/ #供nginx访问使用$ cp /etc/nginx/mime.types /etc/fdfs/ #供nginx访问使用","tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://gcdd1993.github.io/tags/FastDFS/"}]},{"title":"Spring-Framework-官方文档阅读（一）Spring IoC Container","date":"2019-03-20T10:53:06.000Z","path":"p/433/","text":"前言通读Spring IoC容器官方文档，对IoC容器有一个大致的了解。 环境 JDK1.8 Spring Framework Version ：4.3.18.RELEASE 容器概述 接口org.springframework.context.ApplicationContext代表Spring IoC容器，负责实例化，配置和组装bean。在独立应用程序中，通常会创建一个ClassPathXmlApplicationContext或者 FileSystemXmlApplicationContext的实例。 Spring工作原理的高级视图 1.配置元数据创建SimpleBean 12345public class SimpleBean &#123; public void send() &#123; System.out.println(&quot;Hello Spring Bean!&quot;); &#125;&#125; config.xml 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;bean id=&quot;simple&quot; class=&quot;base.SimpleBeanFactoryBean&quot;/&gt;&lt;/beans&gt; 2.实例化容器 1ApplicationContext context = new ClassPathXmlApplicationContext(&quot;config.xml&quot;); 3.使用容器 1234// 检索Spring容器中的beanSimpleBean simpleBean = context.getBean(SimpleBean.class);// 使用beansimpleBean.send(); 还有更灵活的方式来从配置文件获取bean，使用GenericApplicationContext与BeanDefinitionReader结合，直接读取bean定义 12345GenericApplicationContext context = new GenericApplicationContext();new XmlBeanDefinitionReader(context).loadBeanDefinitions(&quot;config.xml&quot;);context.refresh();SimpleBean simpleBean = (SimpleBean) context.getBean(&quot;simple&quot;);simpleBean.send(); Bean概述 Spring IoC容器管理一个或多个bean。这些bean是使用您提供给容器的配置元数据创建的，例如，以XML &lt;bean/&gt;定义的形式 。 在容器本身内，这些bean定义表示为BeanDefinition对象。 除了创建配置好的bean之外，ApplicationContext还允许用户注册在容器外部创建的现有对象。通过getBeanFactory()获得DefaultListableBeanFactory，然后使用registerSingleton()或者registerBeanDefinition()来注册bean。 123456789DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory();ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;config.xml&quot;);User user = new User();user.setId(1L);user.setName(&quot;xiaoming&quot;);beanFactory.registerSingleton(&quot;user&quot;, user);User bean = (User) applicationContext.getBean(&quot;user&quot;);System.out.println(bean); 或者是以下做法： 12345678910ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;config.xml&quot;);DefaultListableBeanFactory beanFactory = (DefaultListableBeanFactory) applicationContext.getBeanFactory();BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(User.class);builder.addPropertyValue(&quot;id&quot;, 1);builder.addPropertyValue(&quot;name&quot;, &quot;xiaoming&quot;);AbstractBeanDefinition beanDefinition = builder.getBeanDefinition();beanFactory.registerBeanDefinition(&quot;user&quot;, beanDefinition);User bean = (User) applicationContext.getBean(&quot;user&quot;);System.out.println(bean); 命名bean每个bean都有一个或多个标识符。这些标识符在托管bean的容器中必须是唯一的。bean通常只有一个标识符，但如果它需要多个标识符，则额外的标识符可以被视为别名。 在基于XML的配置元数据中，使用id和/或name属性指定bean标识符。 实例化bean1.构造函数实例化 12&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;/&gt;&lt;bean name=&quot;anotherExample&quot; class=&quot;examples.ExampleBeanTwo&quot;/&gt; 2.静态工厂方法实例化 123&lt;bean id=&quot;clientService&quot; class=&quot;examples.ClientService&quot; factory-method=&quot;createInstance&quot;/&gt; 12345678public class ClientService &#123; private static ClientService clientService = new ClientService(); private ClientService() &#123;&#125; public static ClientService createInstance() &#123; return clientService; &#125;&#125; 3.实例工厂方法实例化 123456789&lt;!-- the factory bean, which contains a method called createInstance() --&gt;&lt;bean id=&quot;serviceLocator&quot; class=&quot;examples.DefaultServiceLocator&quot;&gt; &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;!-- the bean to be created via the factory bean --&gt;&lt;bean id=&quot;clientService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createClientServiceInstance&quot;/&gt; 12345678public class DefaultServiceLocator &#123; private static ClientService clientService = new ClientServiceImpl(); public ClientService createClientServiceInstance() &#123; return clientService; &#125;&#125; 一个工厂类也可以包含多个工厂方法: 1234567891011&lt;bean id=&quot;serviceLocator&quot; class=&quot;examples.DefaultServiceLocator&quot;&gt; &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;bean id=&quot;clientService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createClientServiceInstance&quot;/&gt;&lt;bean id=&quot;accountService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createAccountServiceInstance&quot;/&gt; 1234567891011121314public class DefaultServiceLocator &#123; private static ClientService clientService = new ClientServiceImpl(); private static AccountService accountService = new AccountServiceImpl(); public ClientService createClientServiceInstance() &#123; return clientService; &#125; public AccountService createAccountServiceInstance() &#123; return accountService; &#125;&#125; 依赖注入构造器注入 基于构造函数的 DI由容器调用具有多个参数的构造函数来完成，每个参数表示一个依赖项。 123456789101112public class SimpleMovieLister &#123; // SimpleMovieLister依赖于MovieFinder private MovieFinder movieFinder; // 一个构造函数，以便Spring容器可以注入一个MovieFinder public SimpleMovieLister(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; // business logic that actually uses the injected MovieFinder is omitted...&#125; 构造函数参数解析 12345678package x.y;public class Foo &#123; public Foo(Bar bar, Baz baz) &#123; // ... &#125;&#125; 12345678910&lt;beans&gt; &lt;bean id=&quot;foo&quot; class=&quot;x.y.Foo&quot;&gt; &lt;constructor-arg ref=&quot;bar&quot;/&gt; &lt;constructor-arg ref=&quot;baz&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;bar&quot; class=&quot;x.y.Bar&quot;/&gt; &lt;bean id=&quot;baz&quot; class=&quot;x.y.Baz&quot;/&gt;&lt;/beans&gt; 显式指定构造函数参数的类型： 1234&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;42&quot;/&gt;&lt;/bean&gt; 使用index属性显式指定构造函数参数的索引： 1234&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;42&quot;/&gt;&lt;/bean&gt; 或者指定构造函数参数名称： 1234&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg name=&quot;years&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg name=&quot;ultimateAnswer&quot; value=&quot;42&quot;/&gt;&lt;/bean&gt; setter注入 基于setter的 DI是在调用无参数构造函数或无参数static工厂方法来实例化bean之后，通过容器调用bean上的setter方法来完成的。 123456789101112public class SimpleMovieLister &#123; // SimpleMovieLister依赖于MovieFinder private MovieFinder movieFinder; // 一个setter方法，以便Spring容器可以注入一个MovieFinder public void setMovieFinder(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; // business logic that actually uses the injected MovieFinder is omitted...&#125; 小结ApplicationContext的依赖注入支持构造器注入和setter注入两种方式。在通过构造函数方法注入了一些依赖项之后，它还支持基于setter的依赖注入。可以用BeanDefinition与PropertyEditor实例结合使用的方式来配置依赖项。 不过，我们一般不直接使用BeanDefinition与PropertyEditor，而是用XML 定义bean或者是注解方式（@Component， @Controller等等），或者是直接编写@Configuration类。然后，这些类在内部转换为实例BeanDefinition并用于加载整个Spring IoC容器实例。 解决循环依赖如果主要使用构造函数注入，则可能出现无法解析的循环依赖关系场景。例如：类A通过构造函数注入需要类B的实例，而类B通过构造函数注入类A的实例。如果将A类和B类的bean配置为相互注入，则Spring IoC容器会在运行时检测到此循环引用，并抛出BeanCurrentlyInCreationException异常。一种可行的解决方案是仅使用setter注入。与典型情况（没有循环依赖）不同，bean A和bean B之间的循环依赖强制其中一个bean在完全初始化之前被注入另一个bean（一个经典的鸡/蛋场景）。 使用 depends-ondepends-on可以在初始化bean之前，显式地强制初始化一个或多个bean。下面的例子，在初始化beanOne之前，将强制初始化manager 12&lt;bean id=&quot;beanOne&quot; class=&quot;ExampleBean&quot; depends-on=&quot;manager&quot;/&gt;&lt;bean id=&quot;manager&quot; class=&quot;ManagerBean&quot; /&gt; 懒加载的bean默认情况下，ApplicationContext会立即配置并初始化所有单例bean，但是我们可以使用lazy-init=&quot;true&quot;将其设置为按需加载。 12&lt;bean id=&quot;lazy&quot; class=&quot;com.foo.ExpensiveToCreateBean&quot; lazy-init=&quot;true&quot;/&gt;&lt;bean name=&quot;not.lazy&quot; class=&quot;com.foo.AnotherBean&quot;/&gt; 注意：懒加载不要使用在数据库连接池上，因为无法立即获知数据库连接状态，将导致运行时创建连接池失败，不可预知的后果。 自动装配协作者Spring容器可以自动连接协作bean之间的关系。您可以通过检查ApplicationContext的内容，允许Spring自动为您的bean解析协作者（其他bean）。 自动装配模式 no：无自动装配，必须使用ref来定义Bean引用。 byName：按属性名称自动装配。 byType：按属性类型自动装配，如果存在多个同类型Bean，则抛出致命异常。 constructor：类似于byType，如果容器中没有构造函数参数类型的一个bean，则抛出致命异常。 Bean 作用域singleton Spring IoC容器只创建该bean定义的对象的一个实例。此单个实例存储在此类单例bean的缓存中，并且该Bean的所有后续请求和引用都将返回缓存对象。 1234&lt;bean id=&quot;accountService&quot; class=&quot;com.foo.DefaultAccountService&quot;/&gt;&lt;!-- the following is equivalent, though redundant (singleton scope is the default) --&gt;&lt;bean id=&quot;accountService&quot; class=&quot;com.foo.DefaultAccountService&quot; scope=&quot;singleton&quot;/&gt; prototype 和单例对立，通常，对所有有状态bean使用原型范围，对无状态bean使用单例范围。 1&lt;bean id=&quot;accountService&quot; class=&quot;com.foo.DefaultAccountService&quot; scope=&quot;prototype&quot;/&gt; Request, session, global session, application, and WebSocket 在web程序中使用，对应于HTTP请求作用域 自定义bean的性质生命周期回调初始化回调实现org.springframework.beans.factory.InitializingBean接口，可以为bean设置初始化方法，该接口定义了一个方法： 1void afterPropertiesSet() throws Exception; 官方不建议使用该接口，因为会增加与Spring的耦合度。可以使用@PostConstruct或指定bean的初始化方法。 使用xml配置文件 1&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/&gt; 使用Java @Bean注解 1@Bean(initMethod = &quot;init&quot;) 销毁回调实现org.springframework.beans.factory.DisposableBean可以为bean设置销毁回调方法，该接口定义了一个方法： 1void destroy() throws Exception; 同样的，不建议实现该接口，可以使用@PreDestroy或指定bean的初始化方法。 使用xml配置文件 1&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; destroy-method=&quot;cleanup&quot;/&gt; 使用Java @Bean注解 1@Bean(destroyMethod = &quot;cleanup&quot;) 从Spring 2.5开始，您有三个控制bean生命周期行为的选项： InitializingBean和 DisposableBean回调接口 init()和destroy()方法 @PostConstruct和@PreDestroy注解 如果为一个bean同时配置了上述方法，则执行方法顺序为： @PostConstruct定义的方法 InitializingBean回调接口定义的afterPropertiesSet() 自定义配置的init()方法 销毁： @PreDestroy定义的方法 DisposableBean回调接口 定义的destroy() 自定义配置的destroy()方法 ApplicationContextAware和BeanNameAware ApplicationContextAware：实现该接口，将注入ApplicationContext实例的引用 BeanNameAware：实现该接口，将注入BeanName 除了ApplicationContextAware和BeanNameAware，Spring还提供了一系列Aware接口，这些接口将为实现类注入对应的实例。 ApplicationContextAware：声明 ApplicationContext ApplicationEventPublisherAware：ApplicationContext的事件发布者 BeanClassLoaderAware：用于加载bean类的类加载器。 BeanFactoryAware：声明 BeanFactory BeanNameAware：声明bean的名称 BootstrapContextAware LoadTimeWeaverAware MessageSourceAware NotificationPublisherAware：Spring JMX通知发布者 PortletConfigAware：当前PortletConfig容器 PortletContextAware：当前PortletContext容器 ResourceLoaderAware：配置的加载程序，用于对资源进行低级访问 ServletConfigAware：当前ServletConfig容器 ServletContextAware：当前ServletContext容器 Bean的继承在xml配置文件里，我们可以定义bean的继承体系，使用parent属性定义父类。 123456789101112&lt;bean id=&quot;inheritedTestBean&quot; abstract=&quot;true&quot; class=&quot;org.springframework.beans.TestBean&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;parent&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;1&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;inheritsWithDifferentClass&quot; class=&quot;org.springframework.beans.DerivedTestBean&quot; parent=&quot;inheritedTestBean&quot; init-method=&quot;initialize&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;override&quot;/&gt; &lt;!-- the age property value of 1 will be inherited from parent --&gt;&lt;/bean&gt; 在源码里，子类是通过ChildBeanDefinition来定义的。 容器扩展点一般来说，我们不需要去继承ApplicationContext实现类，不过Spring预留了一些接口，让我们可以扩展Spring IoC容器。 BeanPostProcessor12345678910111213public interface BeanPostProcessor &#123; //在每个bean初始化之前调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; //在每个bean初始化完毕后调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125;可以定义多个`BeanPostProcessor`，然后实现`Ordered`接口并修改属性order来控制`BeanPostProcessor`的执行顺序。注意：`ConfigurableBeanFactory`提供​```javavoid addBeanPostProcessor(BeanPostProcessor beanPostProcessor); 来手动注册BeanPostProcessor，这些BeanPostProcessor不需要遵循Orderd排序规则，总是在自动注入的BeanPostProcessor之前执行。 一个BeanPostProcessor的实现例子RequiredAnnotationBeanPostProcessor 使用BeanFactoryPostProcessor自定义配置元数据123public interface BeanFactoryPostProcessor &#123; void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;&#125; 类似于BeanPostProcessor，不同的是，BeanFactoryPostProcessor操作配置元数据。也就是说，Spring容器允许BeanFactoryPostProcessor读取配置并更改。 这些BeanPostProcessor将在每个bean初始化时自动执行，以便将更改应用于定义容器的配置元数据。Spring包含许多预定义的BeanPostProcessor,例如PropertyOverrideConfigurer和PropertyPlaceholderConfigurer。 使用FactoryBean自定义实例化逻辑123456public interface FactoryBean&lt;T&gt; &#123; // 自定义bean的初始化逻辑 T getObject() throws Exception; Class&lt;?&gt; getObjectType(); boolean isSingleton();&#125; 配置实现FactoryBean&lt;T&gt;的bean是，返回的是getObject()生成的bean，如果要返回 FactoryBean实例本身，应该使用getBean(&quot;&amp;myBean&quot;) 基于注解的容器配置 @Required @Autowired @Resource @Qualifier @PostConstruct and @PreDestroy 类路径扫描和托管组件 @Component,@Controller,@Repository,@Service @Scope,@SessionScope @ComponentScan JSR 330标准注解和Spring注解对照 Spring javax.inject.* @Autowired @Inject @Component @Named / @ManagedBean @Scope(“singleton”) @Singleton @Qualifier @Qualifier / @Named @Value - @Required - @Lazy - ObjectFactory Provider Environment 抽象主要包含两个方面：profiles（多环境） and properties（配置）. 多环境配置 代码方式 1234AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles(&quot;development&quot;);ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh(); 配置方式 1spring.profiles.active 配置抽象代码演示下： 12345678ApplicationContext ctx = new GenericApplicationContext();Environment env = ctx.getEnvironment();// 是否包含foo的配置boolean containsFoo = env.containsProperty(&quot;foo&quot;);System.out.println(&quot;Does my environment contain the &#x27;foo&#x27; property? &quot; + containsFoo);// 向环境中添加配置MutablePropertySources sources = ctx.getEnvironment().getPropertySources();sources.addFirst(new MyPropertySource()); 使用@PropertySource添加配置 1234567891011121314@Configuration@PropertySource(&quot;classpath:/com/myco/app.properties&quot;)public class AppConfig &#123; @Autowired Environment env; @Bean public TestBean testBean() &#123; TestBean testBean = new TestBean(); testBean.setName(env.getProperty(&quot;testbean.name&quot;)); return testBean; &#125;&#125; BeanFactory还是ApplicationContext？尽量使用ApplicationContext，因为ApplicationContext包含BeanFactory的所有功能： 功能 BeanFactory ApplicationContext bean初始化/编辑 支持 支持 自动注册BeanPostProcessor 不支持 支持 自动注册BeanFactoryPostProcessor 不支持 支持 方便的MessageSource访问（适用于i18n） 不支持 支持 发布ApplicationEvent 不支持 支持 要使用BeanFactory实现显式注册bean后置处理器，您需要编写如下代码： 12345678DefaultListableBeanFactory factory = new DefaultListableBeanFactory();// populate the factory with bean definitions// now register any needed BeanPostProcessor instancesMyBeanPostProcessor postProcessor = new MyBeanPostProcessor();factory.addBeanPostProcessor(postProcessor);// now start using the factory 要使用BeanFactory实现时显式注册BeanFactoryPostProcessor，您必须编写如下代码： 12345678910DefaultListableBeanFactory factory = new DefaultListableBeanFactory();XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(factory);reader.loadBeanDefinitions(new FileSystemResource(&quot;beans.xml&quot;));// bring in some property values from a Properties filePropertyPlaceholderConfigurer cfg = new PropertyPlaceholderConfigurer();cfg.setLocation(new FileSystemResource(&quot;jdbc.properties&quot;));// now actually do the replacementcfg.postProcessBeanFactory(factory);","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Spring Boot Starter 开发指南","date":"2019-03-18T09:16:07.000Z","path":"p/20136/","text":"Spring Boot Starter是什么？依赖管理是任何复杂项目的关键部分。以手动的方式来实现依赖管理不太现实，你得花更多时间，同时你在项目的其他重要方面能付出的时间就会变得越少。 Spring Boot starter 就是为了解决这个问题而诞生的。Starter POM 是一组方便的依赖描述符，您可以将其包含在应用程序中。您可以获得所需的所有 Spring 和相关技术的一站式服务，无需通过示例代码搜索和复制粘贴依赖。 揭开Spring Boot自动装配的神秘面纱Auto Configuration 类当Spring Boot启动时，它会在类路径中查找名为spring.factories的文件。该文件位于META-INF目录中。让我们看一下spring-boot-autoconfigure项目中这个文件的片段： 12345org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration 此文件定义了一些Spring Boot将尝试运行的自动装配类。例如以上的代码片段，Spring Boot将尝试运行RabbitMQ，Cassandra，MongoDB和Hibernate的所有配置类。这些类是否实际运行将取决于类路径上是否存在依赖类。例如，如果在类路径中找到MongoDB的类，则将运行MongoAutoConfiguration，并初始化所有与mongo相关的bean。此条件初始化由@ConditionalOnClass注释启用。让我们看一下MongoAutoConfiguration类的代码片段，看看它的用法： 1234567@Configuration@ConditionalOnClass(MongoClient.class)@EnableConfigurationProperties(MongoProperties.class)@ConditionalOnMissingBean(type = &quot;org.springframework.data.mongodb.MongoDbFactory&quot;)public class MongoAutoConfiguration &#123; // configuration code&#125; 如果存在MongoClient类，将运行该自动装配类初始化MongoClient相关bean。 在application.properties自定义配置Spring Boot使用一些预先配置的默认值初始化bean。要覆盖这些默认值，我们通常会在application.properties文件中使用某个特定名称声明它们。Spring Boot容器会自动获取这些属性。在MongoAutoConfiguration的代码片段中，@EnableConfigurationProperties(MongoProperties.class)表示，使用MongoProperties类来声明自定义属性： 1234567@ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;)public class MongoProperties &#123; private String host; // other fields with standard getters and setters&#125; @ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;)定义了配置前缀，我们可以在application.properties这样来使用它： 1spring.data.mongodb.host = localhost 这样，初始化的时候，localhost将被注入到host属性中 自定义Spring Boot StarterSpring Boot自动装配虽然神奇，但是编写起来却异常简单，我们只需要按部就班的执行以下两个流程： 编写属性容器*Properties，并编写对应的*AutoConfiguration自动装配类 一个pom文件，用于定义引入库和自动装配类的依赖项 概念解析用于*Properties的注解 @ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;) ：用于指定配置前缀 用于*AutoConfiguration的注解 @Configuration：标记为配置类，由Spring容器初始化并接管 @EnableConfigurationProperties：注入配置属性容器 @ConditionalOnBean：条件装配 重点说下条件装配，以@ConditionalOnBean为例，当Spring容器中存在指定Bean的时候装配 1234567@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnBeanCondition.class)public @interface ConditionalOnBean&#123; //properties&#125; @Conditional(OnBeanCondition.class)指定了实现条件装配的逻辑代码 OnBeanCondition声明如下： 1class OnBeanCondition extends SpringBootCondition implements ConfigurationCondition&#123;&#125; 所以，我们自己也可以继承SpringBootCondition并实现ConfigurationCondition来自定义条件装配注解。 比较常用的几个条件装配注解： @ConditionalOnBean：当Spring容器中存在指定Bean时装配 @ConditionalOnClass：当存在指定Class时装配 @ConditionalOnMissingBean：当Spring容器中不存在指定Bean时装配 @ConditionalOnMissingClass：当不存在指定Class时装配 小试牛刀 我们将自动配置模块称为greeter-spring-boot-autoconfigure。该模块将有两个主要类，即GreeterProperties，它将通过application.properties文件和GreeterAutoConfiguartion设置自定义属性，并为greeter库创建bean。 准备，创建假想的一个第三方工程：Greet 123456789101112131415161718192021222324252627282930313233343536373839public class Greeter &#123; private GreetingConfig greetingConfig; public Greeter(GreetingConfig greetingConfig) &#123; this.greetingConfig = greetingConfig; &#125; public String greet(LocalDateTime localDateTime) &#123; String name = greetingConfig.getProperty(USER_NAME); int hourOfDay = localDateTime.getHour(); if (hourOfDay &gt;= 5 &amp;&amp; hourOfDay &lt; 12) &#123; return String.format(&quot;Hello %s, %s&quot;, name, greetingConfig.get(MORNING_MESSAGE)); &#125; else if (hourOfDay &gt;= 12 &amp;&amp; hourOfDay &lt; 17) &#123; return String.format(&quot;Hello %s, %s&quot;, name, greetingConfig.get(AFTERNOON_MESSAGE)); &#125; else if (hourOfDay &gt;= 17 &amp;&amp; hourOfDay &lt; 20) &#123; return String.format(&quot;Hello %s, %s&quot;, name, greetingConfig.get(EVENING_MESSAGE)); &#125; else &#123; return String.format(&quot;Hello %s, %s&quot;, name, greetingConfig.get(NIGHT_MESSAGE)); &#125; &#125; public String greet() &#123; return greet(LocalDateTime.now()); &#125;&#125;public class GreeterConfigParams &#123; public static final String USER_NAME = &quot;user.name&quot;; public static final String MORNING_MESSAGE = &quot;morning.message&quot;; public static final String AFTERNOON_MESSAGE = &quot;afternoon.message&quot;; public static final String EVENING_MESSAGE = &quot;evening.message&quot;; public static final String NIGHT_MESSAGE = &quot;night.message&quot;;&#125;public class GreetingConfig extends Properties &#123; private static final long serialVersionUID = 5662570853707247891L;&#125; 编写Properties和AutoConfiguration： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@ConfigurationProperties(prefix = &quot;gcdd1993.greeter&quot;)public class GreeterProperties &#123; private String userName; private String morningMessage; private String afternoonMessage; private String eveningMessage; private String nightMessage; // getter and setter&#125;@Configuration@ConditionalOnClass(Greeter.class)@EnableConfigurationProperties(GreeterProperties.class)public class GreeterAutoConfiguration &#123; @Autowired private GreeterProperties greeterProperties; @Bean @ConditionalOnMissingBean public GreetingConfig greeterConfig() &#123; String userName = greeterProperties.getUserName() == null ? System.getProperty(&quot;user.name&quot;) : greeterProperties.getUserName(); GreetingConfig greetingConfig = new GreetingConfig(); greetingConfig.put(USER_NAME, userName); if (greeterProperties.getMorningMessage() != null) &#123; greetingConfig.put(MORNING_MESSAGE, greeterProperties.getMorningMessage()); &#125; if (greeterProperties.getAfternoonMessage() != null) &#123; greetingConfig.put(AFTERNOON_MESSAGE, greeterProperties.getAfternoonMessage()); &#125; if (greeterProperties.getEveningMessage() != null) &#123; greetingConfig.put(EVENING_MESSAGE, greeterProperties.getEveningMessage()); &#125; if (greeterProperties.getNightMessage() != null) &#123; greetingConfig.put(NIGHT_MESSAGE, greeterProperties.getNightMessage()); &#125; return greetingConfig; &#125; @Bean @ConditionalOnMissingBean public Greeter greeter(GreetingConfig greetingConfig) &#123; return new Greeter(greetingConfig); &#125;&#125; 然后在src/main/resources/META-INF目录下创建spring.factories文件 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.gcdd.autoconfigure.GreeterAutoConfiguration 测试一下： 创建配置文件application.properties： 12gcdd1993.greeter.userName=gcdd1993gcdd1993.greeter.eveningMessage=good evening 使用Greeter bean 123456789101112131415@SpringBootApplicationpublic class GreeterSampleApplication implements CommandLineRunner &#123; @Autowired private Greeter greeter; public static void main(String[] args) &#123; SpringApplication.run(GreeterSampleApplication.class, args); &#125; @Override public void run(String... args) throws Exception &#123; String message = greeter.greet(); System.out.println(message); &#125;&#125; 执行main方法，将会输出一行： 1Hello gcdd1993, good evening 为配置类添加提示我们知道，在Idea中，编写配置文件的时候，有智能提示 其实这不是Idea搞的鬼，是由META-INF/spring-configuration-metadata.json文件配置好的，Idea只是负责解析这个文件，提供我们智能化的提示信息。 想要达到这个目的很简单，添加依赖org.springframework.boot:spring-boot-configuration-processor就行了。 Maven 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt; Gradle 1compile group: &#x27;org.springframework.boot&#x27;, name: &#x27;spring-boot-configuration-processor&#x27;, version: &#x27;2.1.6.RELEASE&#x27; 总结以上就是Spring Boot Starter的全部内容了，如果要发布到maven仓库，供别人使用，可以使用mvn install打包发布至maven仓库。 👉 本文代码地址","tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://gcdd1993.github.io/tags/Spring-Boot/"}]},{"title":"Ubuntu 安装MongoDB","date":"2019-03-15T03:38:31.000Z","path":"p/49357/","text":"Ubuntu16.04安装MongoDB指南 系统初始化1234$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean 安装mongodb1sudo apt-get install mongodb mongodb默认是监听在127.0.0.1端口的，要开启外网连接，需要修改mongodb配置文件： 1vim /etc/mongodb.conf bind_ip = 127.0.0.1 修改为bind_ip = 0.0.0.0 连接mongodb使用工具robo 3t，添加连接信息 启用密码访问mongodb默认是不开启密码登录的，如果要开启，修改mongodb配置文件： 取消#auth = true前面的注释，并重启mongodbservice mongodb restart 添加用户信息: 12use test_db;db.createUser(&#123;user:&#x27;cool&#x27;, pwd:&#x27;cool&#x27;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;test_db&quot; &#125; ]&#125;); 连接连接方式跟上面类似，唯一不同的是要添加authentication，指定database，username，password，以及选择Mongodb-CR验证方式","tags":[{"name":"NoSql","slug":"NoSql","permalink":"https://gcdd1993.github.io/tags/NoSql/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://gcdd1993.github.io/tags/MongoDB/"}]},{"title":"Spring Event事件驱动","date":"2019-03-14T05:15:01.000Z","path":"p/18285/","text":"Spring事件驱动模型，简单来说类似于Message-Queue消息队列中的Pub/Sub发布/订阅模式，也类似于Java设计模式中的观察者模式。 自定义事件Spring的事件接口位于org.springframework.context.ApplicationEvent，源码如下： 1234567891011public abstract class ApplicationEvent extends EventObject &#123; private static final long serialVersionUID = 7099057708183571937L; private final long timestamp; public ApplicationEvent(Object source) &#123; super(source); this.timestamp = System.currentTimeMillis(); &#125; public final long getTimestamp() &#123; return this.timestamp; &#125;&#125; 继承了Java的事件对象EventObject，所以可以使用getSource()方法来获取到事件传播对象。 自定义Spring事件123456789101112public class CustomSpringEvent extends ApplicationEvent &#123; private String message; public CustomSpringEvent(Object source, String message) &#123; super(source); this.message = message; &#125; public String getMessage() &#123; return message; &#125;&#125; 然后定义事件监听器，该监听器实际上等同于消费者，需要交给Spring容器管理。 1234567@Componentpublic class CustomSpringEventListener implements ApplicationListener&lt;CustomSpringEvent&gt; &#123; @Override public void onApplicationEvent(CustomSpringEvent event) &#123; System.out.println(&quot;Received spring custom event - &quot; + event.getMessage()); &#125;&#125; 最后定义事件发布者 1234567891011@Componentpublic class CustomSpringEventPublisher &#123; @Autowired private ApplicationEventPublisher applicationEventPublisher; public void doStuffAndPublishAnEvent(final String message) &#123; System.out.println(&quot;Publishing custom event. &quot;); CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message); applicationEventPublisher.publishEvent(customSpringEvent); &#125;&#125; 创建测试类 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class CustomSpringEventPublisherTest &#123; @Autowired private CustomSpringEventPublisher publisher; @Test public void publishStringEventTest() &#123; publisher.doStuffAndPublishAnEvent(&quot;111&quot;); &#125;&#125; 运行测试类，可以看到控制台打印了两条重要信息 1234//发布事件Publishing custom event. //监听器得到了事件，并相应处理Received spring custom event - 111 由于Spring事件是发布/订阅的模式,而发布订阅模式有以下三种情况 1生产者 - 1消费者 1生产者 - 多消费者 多生产者 - 多消费者 上面举的例子是第一种情况，我们来试试其他两个情况 继续创建一个事件监听器作为消费者： 1234567@Componentpublic class CustomSpringEventListener2 implements ApplicationListener&lt;CustomSpringEvent&gt; &#123; @Override public void onApplicationEvent(CustomSpringEvent event) &#123; System.out.println(&quot;CustomSpringEventListener2 Received spring custom event - &quot; + event.getMessage()); &#125;&#125; 运行测试类后，可以观察到，控制台顺序打印了两条消费信息： 123Publishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111 说明，Spring的发布订阅模式是广播模式，所有消费者都能接受到消息，并正常消费 再试试第三种多生产者 - 多消费者的情况 继续创建一个发布者， 1234567891011@Componentpublic class CustomSpringEventPublisher2 &#123; @Autowired private ApplicationEventPublisher applicationEventPublisher; public void doStuffAndPublishAnEvent(final String message) &#123; System.out.println(&quot;CustomSpringEventPublisher2 Publishing custom event. &quot;); CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message); applicationEventPublisher.publishEvent(customSpringEvent); &#125;&#125; 控制台输出： 123456CustomSpringEventPublisher Publishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111CustomSpringEventPublisher2 Publishing custom event. CustomSpringEventListener1 Received spring custom event - 222CustomSpringEventListener2 Received spring custom event - 222 从以上输出内容，我们可以猜测到，Spring的事件发布订阅机制是同步进行的，也就是说，事件必须被所有消费者消费完成之后，发布者的代码才能继续往下走，这显然不是我们想要的效果，那有没有异步执行的事件呢？ Spring中的异步事件要使用Spring 的异步事件，我们需要自定义异步事件配置类 1234567891011@Configurationpublic class AsynchronousSpringEventsConfig &#123; @Bean(name = &quot;applicationEventMulticaster&quot;) public ApplicationEventMulticaster simpleApplicationEventMulticaster() &#123; SimpleApplicationEventMulticaster eventMulticaster = new SimpleApplicationEventMulticaster(); eventMulticaster.setTaskExecutor(new SimpleAsyncTaskExecutor()); return eventMulticaster; &#125;&#125; 发布和订阅的代码不用变动，直接运行测试类，控制台将打印出： 123456CustomSpringEventPublisher Publishing custom event. CustomSpringEventPublisher2 Publishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 222CustomSpringEventListener1 Received spring custom event - 222 可以看到，两个发布者几乎同时运行，证明监听器是异步执行的，没有阻塞住发布者的代码。准确的说，监听器将在一个单独的线程中异步处理事件。 Spring自带的事件类型事件驱动在Spring中是被广泛采用的，我们查看ApplicationEvent的子类可以发现许多Event事件，在此不赘述。 注解驱动的监听器从Spring 4.2开始，事件监听器不需要是实现ApplicationListener接口的bean，它可以通过@EventListener注解在任何被Spring容器管理的bean的公共方法上。 1234567@Componentpublic class AnnotationDrivenContextStartedListener &#123; @EventListener public void handleContextStart(CustomSpringEvent cse) &#123; System.out.println(&quot;Handling Custom Spring Event.&quot;); &#125;&#125; 控制台输出结果： 1234CustomSpringEventPublisher Publishing custom event.Handling Custom Spring Event.CustomSpringEventPublisher2 Publishing custom event. Handling Custom Spring Event. 同样的，我们可以看出，这个事件监听器是同步执行的，如果要改为异步监听器，在事件方法上加上@Async，并且在Spring应用中开启异步支持(在SpringBootApplication上添加@EnableAsync)。 12345678@Componentpublic class AnnotationDrivenContextStartedListener &#123; @Async @EventListener public void handleContextStart(CustomSpringEvent cse) &#123; System.out.println(&quot;Handling Custom Spring Event.&quot;); &#125;&#125; 再次运行测试类: 1234CustomSpringEventPublisher Publishing custom event. CustomSpringEventPublisher2 Publishing custom event. Handling Custom Spring Event.Handling Custom Spring Event. 泛型支持创建一个通用泛型事件模型 12345678910@Datapublic class GenericSpringEvent&lt;T&gt; &#123; private T message; protected boolean success; public GenericSpringEvent(T what, boolean success) &#123; this.message = what; this.success = success; &#125;&#125; 注意GenericSpringEvent和CustomSpringEvent之间的区别。我们现在可以灵活地发布任何任意事件，并且不再需要从ApplicationEvent扩展。 这样的话，我们无法像之前一样，通过继承ApplicationListener的方式来定义一个监听器，因为ApplicationListener定义了事件必须是ApplicationEvent的子类。所以，我们只能使用注解驱动的监听器。 通过在@EventListener注释上定义布尔SpEL表达式，也可以使事件监听器成为条件。在这种情况下，只会为成功的String的GenericSpringEvent调用事件处理程序： 1234567@Componentpublic class AnnotationDrivenEventListener &#123; @EventListener(condition = &quot;#event.success&quot;) public void handleSuccessful(GenericSpringEvent&lt;String&gt; event) &#123; System.out.println(&quot;Handling generic event (conditional).&quot;); &#125;&#125; 定义具体类型的事件: 12345public class StringGenericSpringEvent extends GenericSpringEvent&lt;String&gt; &#123; public StringGenericSpringEvent(String message, boolean success) &#123; super(message, success); &#125;&#125; 定义发布者： 1234567891011@Componentpublic class StringGenericSpringEventPublisher &#123; @Autowired private ApplicationEventPublisher applicationEventPublisher; public void doStuffAndPublishAnEvent(final String message, final boolean success) &#123; System.out.println(&quot;CustomSpringEventPublisher Publishing custom event. &quot;); StringGenericSpringEvent springEvent = new StringGenericSpringEvent(message, success); applicationEventPublisher.publishEvent(springEvent); &#125;&#125; 测试类： 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestpublic class CustomSpringEventPublisherTest &#123; @Autowired private StringGenericSpringEventPublisher publisher; @Test public void publishStringEventTest() &#123; publisher.doStuffAndPublishAnEvent(&quot;success&quot;, true); publisher.doStuffAndPublishAnEvent(&quot;failed&quot;, false); &#125;&#125; 运行结果： 123CustomSpringEventPublisher Publishing custom event. Handling generic event (conditional) successCustomSpringEventPublisher Publishing custom event. 监听器只处理了成功的事件，成功忽略掉了失败的事件。这样的好处是，可以为同一个事件定义成功和失败不同的操作。 Spring事件的事务绑定从Spring 4.2开始，框架提供了一个新的@TransactionalEventListener注解，它是@EventListener的扩展，允许将事件的侦听器绑定到事务的一个阶段。绑定可以进行以下事务阶段： AFTER_COMMIT(默认的)：在事务成功后触发 AFTER_ROLLBACK:事务回滚时触发 AFTER_COMPLETION：事务完成后触发，不论是否成功 BEFORE_COMMIT：事务提交之前触发 总结 Spring中处理事件的基础知识：创建一个简单的自定义事件，发布它，然后在监听器中处理它。 在配置中启用事件的异步处理。 Spring 4.2中引入的改进，例如注释驱动的侦听器，更好的泛型支持以及绑定到事务阶段的事件。 👉 本文代码地址","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Spring的BeanFactory和FactoryBean","date":"2019-03-12T09:01:29.000Z","path":"p/17046/","text":"官方定义 BeanFactory：Spring Bean容器的根接口 FactoryBean：各个对象的工厂接口，如果bean实现了这个接口，它将被用作对象的工厂，而不是直接作为bean实例。 源码解析BeanFactory12345678910111213141516public interface BeanFactory &#123; //标注是获取FactoryBean的实现类，而不是调用getObject()获取的实例 String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; Object getBean(String name) throws BeansException; &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException; boolean containsBean(String name); boolean isSingleton(String name) throws NoSuchBeanDefinitionException; boolean isPrototype(String name) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; String[] getAliases(String name);&#125; 从源码的方法定义上，就可以看出，BeanFactory作为bean的容器管理器，提供了一系列获取bean以及获取bean属性的方法。 写一个小例子试验下： SimpleBean： 12345public class SimpleBean &#123; public void send() &#123; System.out.println(&quot;Hello Spring Bean!&quot;); &#125;&#125; Spring配置文件config.xml： 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;bean id=&quot;simpleBean&quot; class=&quot;base.SimpleBeanFactoryBean&quot;/&gt;&lt;/beans&gt; 测试方法： 12345678910111213141516171819202122232425public static void main(String[] args) throws Exception &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;config.xml&quot;); BeanFactory beanFactory = context.getAutowireCapableBeanFactory(); System.out.println(&quot;通过名称获取bean&quot;); SimpleBean simpleBean = (SimpleBean) beanFactory.getBean(&quot;simpleBean&quot;); simpleBean.send(); System.out.println(&quot;通过名称和类型获取bean&quot;); simpleBean = beanFactory.getBean(&quot;simpleBean&quot;, SimpleBean.class); simpleBean.send(); System.out.println(&quot;通过类型获取bean&quot;); simpleBean = beanFactory.getBean(SimpleBean.class); simpleBean.send(); boolean containsBean = beanFactory.containsBean(&quot;simpleBean&quot;); System.out.println(&quot;是否包含 simpleBean ? &quot; + containsBean); boolean singleton = beanFactory.isSingleton(&quot;simpleBean&quot;); System.out.println(&quot;是否是单例? &quot; + singleton); boolean match = beanFactory.isTypeMatch(&quot;simpleBean&quot;, ResolvableType.forClass(SimpleBean.class)); System.out.println(&quot;是否是SimpleBean类型 ? &quot; + match); match = beanFactory.isTypeMatch(&quot;simpleBean&quot;, SimpleBean.class); System.out.println(&quot;是否是SimpleBean类型 ? &quot; + match); Class&lt;?&gt; aClass = beanFactory.getType(&quot;simpleBean&quot;); System.out.println(&quot;simpleBean 的类型是 &quot; + aClass.getName()); String[] aliases = beanFactory.getAliases(&quot;simpleBean&quot;); System.out.println(&quot;simpleBean 的别名 : &quot; + Arrays.toString(aliases));&#125; 控制台结果： 123456789101112通过名称获取beanHello Spring Bean!通过名称和类型获取beanHello Spring Bean!通过类型获取beanHello Spring Bean!是否包含 simpleBean ? true是否是单例? true是否是SimpleBean类型 ? true是否是SimpleBean类型 ? truesimpleBean 的类型是 base.SimpleBeansimpleBean 的别名 : [] FactoryBean123456789101112131415161718public interface FactoryBean&lt;T&gt; &#123; /** * 获取一个bean，如果配置了工厂bean，在getBean的时候，将会调用此方法，获取一个bean */ T getObject() throws Exception; /** * 获取bean的类型 */ Class&lt;?&gt; getObjectType(); /** * 是否是单例 */ boolean isSingleton();&#125; 接口是泛型，定义了三个方法，其中getObject()是工厂模式的体现，将会通过此方法返回一个bean的实例。 一个小例子： 123456789101112131415161718public class SimpleBeanFactoryBean implements FactoryBean&lt;SimpleBean&gt; &#123; @Override public SimpleBean getObject() throws Exception &#123; System.out.println(&quot;MyFactoryBean getObject&quot;); return new SimpleBean(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; System.out.println(&quot;MyFactoryBean getObjectType&quot;); return SimpleBean.class; &#125; @Override public boolean isSingleton() &#123; return false; &#125;&#125; 以上可以修改为单例模式，可以做成线程安全的单例，可塑性较高。 配置文件config.xml: 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;bean id=&quot;simple&quot; class=&quot;base.SimpleBeanFactoryBean&quot;/&gt;&lt;/beans&gt; 注意，我们在这里只配置了SimpleBeanFactoryBean，并没有配置SimpleBean，接下来看下getBean方法的输出。 123ApplicationContext context = new ClassPathXmlApplicationContext(&quot;config.xml&quot;);SimpleBean simpleBean = context.getBean(SimpleBean.class);simpleBean.send(); 控制台输出： 123MyFactoryBean getObjectTypeMyFactoryBean getObjectHello Spring Bean! 由此我们可以看出FactoryBean的执行流程 通过getObjectType获取bean的类型 调用getObject方法获取bean的实例 总结BeanFactory和FactoryBean其实没有关系，只是名称比较像而已。 BeanFactory是IOC最基本的容器，负责生产和管理bean，它为其他具体的IOC容器提供了最基本的规范。 FactoryBean是一个接口，当在IOC容器中的Bean实现了FactoryBean后，通过getBean(String BeanName)获取到的Bean对象并不是FactoryBean的实现类对象，而是这个实现类中的getObject()方法返回的对象。要想获取FactoryBean的实现类，就要getBean(&amp;BeanName)，在BeanName之前加上&amp;。","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Jackson使用指南","date":"2019-01-21T12:12:32.000Z","path":"p/56384/","text":"从事JAVA开发工作以来,一直都离不开Jackson的序列化反序列化,对于Jackson的使用也一直处于够用但不深入的状态，下面是日常使用过程中对Jackson的总结。 Jackson常用注解序列化注解@JsonAnyGetter 像普通属性一样序列化Map 123456789public class ExtendableBean &#123; public String name; private Map&lt;String, String&gt; properties; @JsonAnyGetter public Map&lt;String, String&gt; getProperties() &#123; return properties; &#125;&#125; 序列化示例： 12345&#123; &quot;name&quot;:&quot;My bean&quot;, &quot;attr2&quot;:&quot;val2&quot;, &quot;attr1&quot;:&quot;val1&quot;&#125; @JsonGetter 将指定的方法标记为getter方法。可以用来代替@JsonProperty 123456789public class MyBean &#123; public int id; private String name; @JsonGetter(&quot;name&quot;) public String getTheName() &#123; return name; &#125;&#125; 序列化示例： 1234&#123; &quot;id&quot;: 1, &quot;name&quot;:&quot;My bean&quot;&#125; @JsonPropertyOrder 用在类上，在序列化的时候自定义属性输出顺序 12345@JsonPropertyOrder(&#123; &quot;name&quot;, &quot;id&quot; &#125;)public class MyBean &#123; public int id; public String name;&#125; 序列化示例： 1234&#123; &quot;name&quot;:&quot;My bean&quot;, &quot;id&quot;: 1&#125; @JsonRawValue 完全按照原样序列化属性的值 123456public class RawBean &#123; public String name; @JsonRawValue public String json;&#125; 例如： 1RawBean bean = new RawBean(&quot;My bean&quot;, &quot;&#123;\\&quot;attr\\&quot;:false&#125;&quot;); 将序列化为： 123456&#123; &quot;name&quot;:&quot;My bean&quot;, &quot;json&quot;:&#123; &quot;attr&quot;:false &#125;&#125; 而不是： 1234&#123; &quot;name&quot;:&quot;My bean&quot;, &quot;json&quot;:&quot;&#123;\\&quot;attr\\&quot;:false&#125;&quot;&#125; @JsonValue 定义整个实体的序列化方法，Jackson将会使用该方法的输出作为序列化输出。 12345678910111213public enum TypeEnumWithValue &#123; TYPE1(1, &quot;Type A&quot;), TYPE2(2, &quot;Type 2&quot;); private Integer id; private String name; // standard constructors @JsonValue public String getName() &#123; return name; &#125;&#125; 序列化示例： 123&#123; &quot;name&quot;: &quot;Type 2&quot;&#125; @JsonRootName 如果需要将实体包装一层，可以使用@JsonRootName来指定根包装器的名称 12345@JsonRootName(value = &quot;user&quot;)public class UserWithRoot &#123; public int id; public String name;&#125; 序列化示例： 123456&#123; &quot;user&quot;: &#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;John&quot; &#125;&#125; 如果不用该注解，将会序列化为： 1234&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;John&quot;&#125; @JsonSerialize 用于指定自定义序列化器来序列化实体 123456public class Event &#123; public String name; @JsonSerialize(using = CustomDateSerializer.class) public Date eventDate;&#125; 自定义序列化器如下： 1234567891011121314151617181920public class CustomDateSerializer extends StdSerializer&lt;Date&gt; &#123; private static SimpleDateFormat formatter = new SimpleDateFormat(&quot;dd-MM-yyyy hh:mm:ss&quot;); public CustomDateSerializer() &#123; this(null); &#125; public CustomDateSerializer(Class&lt;Date&gt; t) &#123; super(t); &#125; @Override public void serialize( Date value, JsonGenerator gen, SerializerProvider arg2) throws IOException, JsonProcessingException &#123; gen.writeString(formatter.format(value)); &#125;&#125; 输出示例： 1234&#123; &quot;name&quot;: &quot;test&quot;, &quot;eventDate&quot;: &quot;20-12-2014 02:30:00&quot;&#125; 反序列化注解@JsonCreator 指定反序列化使用的构造函数或方法 待反序列化Json示例： 1234&#123; &quot;id&quot;:1, &quot;theName&quot;:&quot;My bean&quot;&#125; 12345678910public class BeanWithCreator &#123; public int id; public String name; @JsonCreator public BeanWithCreator(@JsonProperty(&quot;id&quot;) int id, @JsonProperty(&quot;theName&quot;) String name) &#123; this.id = id; this.name = name; &#125;&#125; @JacksonInject 指定某个字段从注入赋值，而不是从Json 123456public class BeanWithInject &#123; @JacksonInject public int id; public String name;&#125; 示例用法： 1234567String json = &quot;&#123;\\&quot;name\\&quot;:\\&quot;My bean\\&quot;&#125;&quot;; InjectableValues inject = new InjectableValues.Std() .addValue(int.class, 1);BeanWithInject bean = new ObjectMapper().reader(inject) .forType(BeanWithInject.class) .readValue(json); @JsonAnySetter 在反序列化时，将Map当成普通属性 待反序列化Json： 12345&#123; &quot;name&quot;:&quot;My bean&quot;, &quot;attr2&quot;:&quot;val2&quot;, &quot;attr1&quot;:&quot;val1&quot;&#125; 123456789public class ExtendableBean &#123; public String name; private Map&lt;String, String&gt; properties; @JsonAnySetter public void add(String key, String value) &#123; properties.put(key, value); &#125;&#125; properties字段的值将会是由 attr2 -&gt; val2,attr1 -&gt; val1组成的键值对。 @JsonSetter 将方法标记为setter方法，可以指定属性名称 123456789public class MyBean &#123; public int id; private String name; @JsonSetter(&quot;name&quot;) public void setTheName(String name) &#123; this.name = name; &#125;&#125; @JsonDeserialize 用于指定自定义反序列化器来反序列化实体 123456public class Event &#123; public String name; @JsonDeserialize(using = CustomDateDeserializer.class) public Date eventDate;&#125; 对应的反序列化器： 123456789101112131415161718192021222324252627public class CustomDateDeserializer extends StdDeserializer&lt;Date&gt; &#123; private static SimpleDateFormat formatter = new SimpleDateFormat(&quot;dd-MM-yyyy hh:mm:ss&quot;); public CustomDateDeserializer() &#123; this(null); &#125; public CustomDateDeserializer(Class&lt;?&gt; vc) &#123; super(vc); &#125; @Override public Date deserialize( JsonParser jsonparser, DeserializationContext context) throws IOException &#123; String date = jsonparser.getText(); try &#123; return formatter.parse(date); &#125; catch (ParseException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; Jackson设置属性是否参与序列化@JsonIgnoreProperties 在类上指定要忽略的属性 12345@JsonIgnoreProperties(&#123; &quot;id&quot; &#125;)public class BeanWithIgnore &#123; public int id; public String name;&#125; @JsonIgnore 在具体属性上忽略，使其不参与序列化过程 123456public class BeanWithIgnore &#123; @JsonIgnore public int id; public String name;&#125; 与@JsonIgnoreProperties是等效的。 @JsonIgnoreType 用在类上，将忽略该类所有属性 12345678910public class User &#123; public int id; public Name name; @JsonIgnoreType public static class Name &#123; public String firstName; public String lastName; &#125;&#125; @JsonInclude 用于排除值为empty/null/default的属性 12345@JsonInclude(Include.NON_NULL)public class MyBean &#123; public int id; public String name;&#125; @JsonAutoDetect 强制序列化私有属性，不管它有没有getter方法 12345@JsonAutoDetect(fieldVisibility = Visibility.ANY)public class PrivateBean &#123; private int id; private String name;&#125; Jackson处理多态一般都是组合起来使用，有下面三个注解： @JsonTypeInfo 指定序列化中包含的类型信息的详细信息 @JsonSubTypes 指定带注释类型的子类型 @JsonTypeName 指定用于带注释的类的逻辑类型名称 1234567891011121314151617181920212223242526public class Zoo &#123; public Animal animal; @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = As.PROPERTY, property = &quot;type&quot;) @JsonSubTypes(&#123; @JsonSubTypes.Type(value = Dog.class, name = &quot;dog&quot;), @JsonSubTypes.Type(value = Cat.class, name = &quot;cat&quot;) &#125;) public static class Animal &#123; public String name; &#125; @JsonTypeName(&quot;dog&quot;) public static class Dog extends Animal &#123; public double barkVolume; &#125; @JsonTypeName(&quot;cat&quot;) public static class Cat extends Animal &#123; boolean likesCream; public int lives; &#125;&#125; 上述例子中，指定属性type为判断具体子类的依据，例如：type=dog，将被序列化为Dog类型。 Jackson通用注解（序列化反序列化都生效）@JsonProperty 指定JSON中的属性名称 1234567891011121314public class MyBean &#123; public int id; private String name; @JsonProperty(&quot;name&quot;) public void setTheName(String name) &#123; this.name = name; &#125; @JsonProperty(&quot;name&quot;) public String getTheName() &#123; return name; &#125;&#125; @JsonFormat 用于在序列化日期/时间值时指定格式。 12345678public class Event &#123; public String name; @JsonFormat( shape = JsonFormat.Shape.STRING, pattern = &quot;dd-MM-yyyy hh:mm:ss&quot;) public Date eventDate;&#125; @JsonUnwrapped 将对象中所有的属性与当前平级，不太好描述，简单说就是拆开包装。 1234567891011public class UnwrappedUser &#123; public int id; @JsonUnwrapped public Name name; public static class Name &#123; public String firstName; public String lastName; &#125;&#125; 序列化示例： 12345&#123; &quot;id&quot;:1, &quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;&#125; 如果不加@JsonUnwrapped注解，将被序列化为： 1234567&#123; &quot;id&quot;:1, &quot;name&quot;: &#123; &quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot; &#125;&#125; @JsonView 指定视图，类似分组进行序列化/反序列化 定义视图： 1234public class Views &#123; public static class Public &#123;&#125; public static class Internal extends Public &#123;&#125;&#125; 定义实体： 12345678910public class Item &#123; @JsonView(Views.Public.class) public int id; @JsonView(Views.Public.class) public String itemName; @JsonView(Views.Internal.class) public String ownerName;&#125; 序列化示例： 123String result = new ObjectMapper() .writerWithView(Views.Public.class) .writeValueAsString(item); 这时，将只会序列化id和itemName字段 @JsonManagedReference, @JsonBackReference @JsonManagedReference和@JsonBackReference注释用于处理父/子关系并解决循环问题。 例如，有两个相互引用的类： 1234567public class ItemWithRef &#123; public int id; public String itemName; @JsonManagedReference public UserWithRef owner;&#125; 1234567public class UserWithRef &#123; public int id; public String name; @JsonBackReference public List&lt;ItemWithRef&gt; userItems;&#125; 不加注解，会循环调用，导致内存溢出，这时候可以使用@JsonManagedReference和@JsonBackReference来避免内存溢出。 @JsonIdentityInfo 用于指定在序列化/反序列化值时使用对象标识，例如，处理无限递归类型的问题。 12345678@JsonIdentityInfo( generator = ObjectIdGenerators.PropertyGenerator.class, property = &quot;id&quot;)public class ItemWithIdentity &#123; public int id; public String itemName; public UserWithIdentity owner;&#125; @JsonFilter 指定序列化期间要使用的过滤器。 12345@JsonFilter(&quot;myFilter&quot;)public class BeanWithFilter &#123; public int id; public String name;&#125; 示例代码： 12345678910BeanWithFilter bean = new BeanWithFilter(1, &quot;My bean&quot;);FilterProvider filters = new SimpleFilterProvider().addFilter( &quot;myFilter&quot;, SimpleBeanPropertyFilter.filterOutAllExcept(&quot;name&quot;));String result = new ObjectMapper() .writer(filters) .writeValueAsString(bean); 自定义Jackson注解可以使用@JacksonAnnotationsInside来开发自定义注解 12345@Retention(RetentionPolicy.RUNTIME) @JacksonAnnotationsInside @JsonInclude(Include.NON_NULL) @JsonPropertyOrder(&#123; &quot;name&quot;, &quot;id&quot;, &quot;dateCreated&quot; &#125;) public @interface CustomAnnotation &#123;&#125; 如何使用自定义注解： 123456@CustomAnnotationpublic class BeanWithCustomAnnotation &#123; public int id; public String name; public Date dateCreated;&#125; 自定义注解可以增强代码复用，把一些通用的Jackson注解组合起来，形成一个新注解，新注解可以代替组合的注解。 Jackson MixIn 注解 动态地为某些类型增加统一的Jackson注解 实体： 12345public class Item &#123; public int id; public String itemName; public User owner;&#125; MixIn类： 12@JsonIgnoreTypepublic class MyMixInForIgnoreType &#123;&#125; 我们可以动态地让User类型不参与序列化： 1234Item item = new Item(1, &quot;book&quot;, null);ObjectMapper mapper = new ObjectMapper();mapper.addMixIn(User.class, MyMixInForIgnoreType.class);result = mapper.writeValueAsString(item); 禁用Jackson注解假设我们有一个带Jackson注解的实体： 123456@JsonInclude(Include.NON_NULL)@JsonPropertyOrder(&#123; &quot;name&quot;, &quot;id&quot; &#125;)public class MyBean &#123; public int id; public String name;&#125; 我们可以这样来禁用该实体上的所有Jackson注解： 123MyBean bean = new MyBean(1, null);ObjectMapper mapper = new ObjectMapper();mapper.disable(MapperFeature.USE_ANNOTATIONS); Jackson的ObjectMapper用法java类 转换为 json可以直接序列化为Json字符串： 1objectMapper.writeValueAsString(car); 或者，可以序列化到文件，文件内容是Json字符串： 1objectMapper.writeValue(new File(&quot;target/car.json&quot;), car); json 转换为 java类从字符串： 12String json = &quot;&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;&quot;;objectMapper.readValue(json, Car.class); 从文件： 1objectMapper.readValue(new File(&quot;target/json_car.json&quot;), Car.class); 从URL： 1objectMapper.readValue(new URL(&quot;target/json_car.json&quot;), Car.class); json转换为Jackson JsonNode1234String json = &quot;&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;&quot;;JsonNode jsonNode = objectMapper.readTree(json);String color = jsonNode.get(&quot;color&quot;).asText();// Output: color -&gt; Black json 转换为 java集合123String jsonCarArray = &quot;[&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;, &#123; \\&quot;color\\&quot; : \\&quot;Red\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;]&quot;;List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;()&#123;&#125;); json 转换为 Map12String json = &quot;&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;&quot;;Map&lt;String, Object&gt; map = objectMapper.readValue(json, new TypeReference&lt;Map&lt;String,Object&gt;&gt;()&#123;&#125;); ObjectMapper的常用配置忽略不识别的字段（json属性与目标实体存在属性上的差异）： 1objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 允许原始值为null： 1objectMapper.configure(DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES, false); 允许将枚举序列化/反序列化为数字： 1objectMapper.configure(DeserializationFeature.FAIL_ON_NUMBERS_FOR_ENUMS, false); 配置自定义序列化/反序列化器假设有一个序列化器： 123456789101112131415161718public class CustomCarSerializer extends StdSerializer&lt;Car&gt; &#123; public CustomCarSerializer() &#123; this(null); &#125; public CustomCarSerializer(Class&lt;Car&gt; t) &#123; super(t); &#125; @Override public void serialize( Car car, JsonGenerator jsonGenerator, SerializerProvider serializer) &#123; jsonGenerator.writeStartObject(); jsonGenerator.writeStringField(&quot;car_brand&quot;, car.getType()); jsonGenerator.writeEndObject(); &#125;&#125; 一个反序列化器： 1234567891011121314151617181920212223public class CustomCarDeserializer extends StdDeserializer&lt;Car&gt; &#123; public CustomCarDeserializer() &#123; this(null); &#125; public CustomCarDeserializer(Class&lt;?&gt; vc) &#123; super(vc); &#125; @Override public Car deserialize(JsonParser parser, DeserializationContext deserializer) &#123; Car car = new Car(); ObjectCodec codec = parser.getCodec(); JsonNode node = codec.readTree(parser); // try catch block JsonNode colorNode = node.get(&quot;color&quot;); String color = colorNode.asText(); car.setColor(color); return car; &#125;&#125; 用ObjectMapper使用他们： 1234//添加自定义序列化器module.addSerializer(Car.class, new CustomCarSerializer());//添加自定义反序列化器module.addDeserializer(Car.class, new CustomCarDeserializer()); 处理日期格式化123ObjectMapper objectMapper = new ObjectMapper();DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm a z&quot;);objectMapper.setDateFormat(df); 处理集合反序列化为数组： 12345String jsonCarArray = &quot;[&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;, &#123; \\&quot;color\\&quot; : \\&quot;Red\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;]&quot;;ObjectMapper objectMapper = new ObjectMapper();objectMapper.configure(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY, true);Car[] cars = objectMapper.readValue(jsonCarArray, Car[].class); 反序列化为集合： 1234String jsonCarArray = &quot;[&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;, &#123; \\&quot;color\\&quot; : \\&quot;Red\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;]&quot;;ObjectMapper objectMapper = new ObjectMapper();List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;()&#123;&#125;); ObjectMapper的基本用法ObjectMapper可以通过configure方法设置全局序列化/反序列化行为，例如：1objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 常用的一些设置： DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES：忽略不识别的字段 DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES：允许使用属性的默认值进行反序列化 DeserializationFeature.FAIL_ON_NUMBERS_FOR_ENUMS：允许将枚举值序列化/反序列化为数字 注册自定义序列化/反序列化程序1234567//创建一个模块SimpleModule module = new SimpleModule(&quot;CustomCarSerializer&quot;, new Version(1, 0, 0, null, null, null));//将自定义序列化/反序列化程序注册到模块module.addSerializer(Car.class, new CustomCarSerializer());//module.addDeserializer(Car.class, new CustomCarDeserializer());//注册模块mapper.registerModule(module); 处理日期格式12DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm a z&quot;);mapper.setDateFormat(df); 处理集合处理数组1234String jsonCarArray = &quot;[&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;, &#123; \\&quot;color\\&quot; : \\&quot;Red\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;]&quot;;ObjectMapper objectMapper = new ObjectMapper();objectMapper.configure(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY, true);Car[] cars = objectMapper.readValue(jsonCarArray, Car[].class); 处理集合123String jsonCarArray = &quot;[&#123; \\&quot;color\\&quot; : \\&quot;Black\\&quot;, \\&quot;type\\&quot; : \\&quot;BMW\\&quot; &#125;, &#123; \\&quot;color\\&quot; : \\&quot;Red\\&quot;, \\&quot;type\\&quot; : \\&quot;FIAT\\&quot; &#125;]&quot;;ObjectMapper objectMapper = new ObjectMapper();List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;()&#123;&#125;); Jackson注解扩展@JsonIdentityReference 使用指定的标识来序列化Java对象，而不是序列化整个对象 例如： 123456@JsonIdentityInfo(generator = ObjectIdGenerators.PropertyGenerator.class, property = &quot;id&quot;)@JsonIdentityReference(alwaysAsId = true)public class BeanWithoutIdentityReference &#123; private int id; private String name;&#125; 将被序列化为： 11 @JsonAppend 运行在序列化时添加额外的属性 123456789@JsonAppend(attrs = &#123; @JsonAppend.Attr(value = &quot;version&quot;) &#125;)public class BeanWithAppend &#123; private int id; private String name; // constructor, getters and setters&#125; 例如，我们在序列化时手动增加version = 1.0的属性 123BeanWithAppend bean = new BeanWithAppend(2, &quot;Bean With Append Annotation&quot;);ObjectWriter writer = mapper.writerFor(BeanWithAppend.class).withAttribute(&quot;version&quot;, &quot;1.0&quot;);String jsonString = writer.writeValueAsString(bean); 序列化结果： 12345&#123; &quot;id&quot;: 2, &quot;name&quot;: &quot;Bean With Append Annotation&quot;, &quot;version&quot;: &quot;1.0&quot;&#125; @JsonNaming 指定序列化的时候属性命名方式 有四种选项： KEBAB_CASE 由连字符分割，例如：kebab-case LOWER_CASE 所有的字母都转换为小写，例如：lowercase SNAKE_CASE 所有的字母都转换为小写，并且由下划线分割，例如：snake_case UPPER_CAMEL_CASE 所有名称元素，包括第一个元素，都以大写字母开头，后跟小写字母，并且没有分隔符，例如：UpperCamelCase 使用举例： 12345@JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class)public class NamingBean &#123; private int id; private String beanName;&#125; @JsonPropertyDescription 用于生成字段的描述信息 例如，有下面一个实体： 12345public class PropertyDescriptionBean &#123; private int id; @JsonPropertyDescription(&quot;This is a description of the name property&quot;) private String name;&#125; 我们可以输出该类的信息： 1234SchemaFactoryWrapper wrapper = new SchemaFactoryWrapper();mapper.acceptJsonFormatVisitor(PropertyDescriptionBean.class, wrapper);JsonSchema jsonSchema = wrapper.finalSchema();String jsonString = mapper.writeValueAsString(jsonSchema); 结果如下： 1234567891011121314151617&#123; &quot;type&quot;: &quot;object&quot;, &quot;id&quot;: &quot;urn:jsonschema:com:baeldung:jackson:annotation:extra:PropertyDescriptionBean&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;This is a description of the name property&quot; &#125;, &quot;id&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125;&#125; @JsonPOJOBuilder 自定义生成器类，来控制json的反序列化行为 @JsonPOJOBuilder有两个属性： buildMethodName 将JSON字段绑定到bean的属性后，用于实例化预期bean的无参构造的名称。默认名称为build。 withPrefix 用于自动检测JSON和bean属性之间匹配的名称前缀。默认前缀为with。 假设我们要反序列化的json如下： 1234&#123; &quot;id&quot;: 5, &quot;name&quot;: &quot;POJO Builder Bean&quot;&#125; 对应的pojo： 1234567@JsonDeserialize(builder = BeanBuilder.class)public class POJOBuilderBean &#123; private int identity; private String beanName; // constructor, getters and setters&#125; 对应的生成器： 12345678910111213141516171819@JsonPOJOBuilder(buildMethodName = &quot;createBean&quot;, withPrefix = &quot;construct&quot;)public class BeanBuilder &#123; private int idValue; private String nameValue; public BeanBuilder constructId(int id) &#123; idValue = id; return this; &#125; public BeanBuilder constructName(String name) &#123; nameValue = name; return this; &#125; public POJOBuilderBean createBean() &#123; return new POJOBuilderBean(idValue, nameValue); &#125;&#125; 使用ObjectMapper反序列化： 12String jsonString = &quot;&#123;\\&quot;id\\&quot;:5,\\&quot;name\\&quot;:\\&quot;POJO Builder Bean\\&quot;&#125;&quot;;POJOBuilderBean bean = mapper.readValue(jsonString, POJOBuilderBean.class); 👉 代码仓库👉 Jackson JSON Tutorial","tags":[{"name":"jackson","slug":"jackson","permalink":"https://gcdd1993.github.io/tags/jackson/"}]},{"title":"后端跨域的N种方法","date":"2019-01-21T11:11:31.000Z","path":"p/34331/","text":"简单来说，CORS是一种访问机制，英文全称是Cross-Origin Resource Sharing，即我们常说的跨域资源共享，通过在服务器端设置响应头，把发起跨域的原始域名添加到Access-Control-Allow-Origin 即可。 返回新的CorsFilter(全局跨域) 在任意配置类，返回一个新的CorsFilter Bean，并添加映射路径和具体的CORS配置信息。 12345678910111213141516171819202122232425@Configurationpublic class GlobalCorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; //1.添加CORS配置信息 CorsConfiguration config = new CorsConfiguration(); //放行哪些原始域 config.addAllowedOrigin(&quot;*&quot;); //是否发送Cookie信息 config.setAllowCredentials(true); //放行哪些原始域(请求方式) config.addAllowedMethod(&quot;*&quot;); //放行哪些原始域(头部信息) config.addAllowedHeader(&quot;*&quot;); //暴露哪些头部信息(因为跨域访问默认不能获取全部头部信息) config.addExposedHeader(&quot;*&quot;); //2.添加映射路径 UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource(); configSource.registerCorsConfiguration(&quot;/**&quot;, config); //3.返回新的CorsFilter. return new CorsFilter(configSource); &#125;&#125; 使用注解(局部跨域)在方法上(@RequestMapping)使用注解 @CrossOrigin123456@RequestMapping(&quot;/hello&quot;)@ResponseBody@CrossOrigin(&quot;http://localhost:8080&quot;) public String index()&#123; return &quot;Hello World&quot;;&#125; 在控制器(@Controller)上使用注解 @CrossOrigin12345678910@Controller@CrossOrigin(origins = &quot;http://domain.com&quot;, maxAge = 3600)public class AccountController &#123; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String index()&#123; return &quot;Hello World&quot;; &#125;&#125; 手工设置响应头(局部跨域) 使用HttpServletResponse对象添加响应头（Access-Control-Allow-Origin）来授权原始域，这里Origin的值也可以设置为”*” ，表示全部放行。 123456@RequestMapping(&quot;/hello&quot;)@ResponseBodypublic String index(HttpServletResponse response)&#123; response.addHeader(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); return &quot;Hello World&quot;;&#125; Nginx配置跨域1234567891011121314151617181920upstream server &#123; server 127.0.0.1:8091;&#125;server &#123; listen 80; server_name domain.com; location ^~/api &#123; //添加跨域请求头 proxy_set_header Access-Control-Allow-Origin *; proxy_set_header Access-Control-Allow-Methods *; proxy_set_header Access-Control-Allow-Headers *; if ($request_method = &#x27;OPTIONS&#x27;) &#123; return 204; &#125; rewrite ^/api/(.+?)$ /$1 break; proxy_pass http://server; &#125;&#125;","tags":[{"name":"跨域","slug":"跨域","permalink":"https://gcdd1993.github.io/tags/%E8%B7%A8%E5%9F%9F/"}]},{"title":"解决Spring Security自定义filter重复执行问题","date":"2019-01-14T11:29:51.000Z","path":"p/356/","text":"今天做项目的时候，发现每次拦截器日志都会打两遍，很纳闷，怀疑是Filter被执行了两遍。结果debug之后发现还真是！记录一下这个神奇的BUG！ 问题描述项目中使用的是Spring-security作为权限框架，然后做了一个JwtAuthenticationTokenFilter作为拦截器拦截请求，校验Token，但是每次请求都会打两遍日志。下面是精简的源代码: 自定义的Filter类 1234567891011121314151617@Slf4j@Componentpublic class JwtAuthenticationTokenFilter extends OncePerRequestFilter &#123; @Override protected void doFilterInternal( HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws ServletException, IOException &#123; //...省略 //打出两遍日志的地方 log.info(&quot;User:&#123;&#125; request path:&#123;&#125;, method:&#123;&#125;, param:&#123;&#125;&quot;, username, request.getServletPath(), request.getMethod(), request.getParameterMap() == null ? null : OBJECT_MAPPER.writeValueAsString(request.getParameterMap())); //...省略 chain.doFilter(request, response); &#125;&#125; WebSecurityConfig配置类 123456789101112131415161718@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; //...省略 @Bean public JwtAuthenticationTokenFilter authenticationTokenFilterBean() throws Exception &#123; return new JwtAuthenticationTokenFilter(); &#125; @Override protected void configure(HttpSecurity httpSecurity) throws Exception &#123; //...省略 //把JwtAuthenticationTokenFilter加入到RememberMeAuthenticationFilter之前 httpSecurity.addFilterBefore(authenticationTokenFilterBean(), RememberMeAuthenticationFilter.class); &#125; //...省略&#125; 请求日志如下: 问题解决把自定义FilterJwtAuthenticationTokenFilter的@Component取消掉就可以了，不让它被Spring容器管理。 原因在spring容器托管的OncePerRequestFilter的bean，都会自动加入到servlet的filter chain，而上面的定义，还额外把filter加入到了spring security的ememberMeAuthenticationFilter之前。而spring security也是一系列的filter，在mvc的filter之前执行。因此在鉴权通过的情况下，就会先后各执行一次。 参考资料解决spring security自定义filter重复执行问题","tags":[{"name":"Spring-Security","slug":"Spring-Security","permalink":"https://gcdd1993.github.io/tags/Spring-Security/"}]},{"title":"Spring Cloud feign使用okhttp3","date":"2019-01-13T16:13:04.000Z","path":"p/7382/","text":"spring cloud feign使用okhttp3 指南maven 1234&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 12feign.httpclient.enabled=falsefeign.okhttp.enabled=true 配置 12345678910111213141516171819@Configuration@ConditionalOnClass(Feign.class)@AutoConfigureBefore(FeignAutoConfiguration.class)public class FeignOkHttpConfig &#123; @Autowired OkHttpLoggingInterceptor okHttpLoggingInterceptor; @Bean public okhttp3.OkHttpClient okHttpClient()&#123; return new okhttp3.OkHttpClient.Builder() .readTimeout(60, TimeUnit.SECONDS) .connectTimeout(60, TimeUnit.SECONDS) .writeTimeout(120, TimeUnit.SECONDS) .connectionPool(new ConnectionPool()) // .addInterceptor(); .build(); &#125;&#125; 实践不需要额外编写FeignOkHttpConfig，feign本身已经存在FeignOkHttpAutoConfiguration了，不需要额外配置。","tags":[{"name":"Spring-Cloud","slug":"Spring-Cloud","permalink":"https://gcdd1993.github.io/tags/Spring-Cloud/"}]},{"title":"Spring-Cloud微服务踩坑记录","date":"2019-01-13T16:08:58.000Z","path":"p/61811/","text":"记录在开发微服务过程中遇到的问题以及解决方案。 No Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-netflix-ribbon?@feignclient和@requestmapping混用的时候出错重写springmvc扫描controller时不带有@feignclient才实例化 123456789101112131415161718192021@Configuration@ConditionalOnClass(&#123;Feign.class&#125;)public class FeignConfiguration &#123; @Bean public WebMvcRegistrations feignWebRegistrations() &#123; return new WebMvcRegistrationsAdapter() &#123; @Override public RequestMappingHandlerMapping getRequestMappingHandlerMapping() &#123; return new FeignRequestMappingHandlerMapping(); &#125; &#125;; &#125; private static class FeignRequestMappingHandlerMapping extends RequestMappingHandlerMapping &#123; @Override protected boolean isHandler(Class&lt;?&gt; beanType) &#123; return super.isHandler(beanType) &amp;&amp; !AnnotatedElementUtils.hasAnnotation(beanType, FeignClient.class); &#125; &#125;&#125; SpringCloud使用Zuul出现“Forwarding error”错误解决方法在application.yml中添加ribbon的超时时间设置： 1234567891011121314ribbon: ReadTimeout: 3000 ConnectTimeout: 3000zuul: host: connect-timeout-millis: 3000 socket-timeout-millis: 3000hystrix: command: default: execution: isolation: thread: timeout-in-milliseconds: 3000","tags":[{"name":"Spring-Cloud","slug":"Spring-Cloud","permalink":"https://gcdd1993.github.io/tags/Spring-Cloud/"}]},{"title":"我的JAVA环境搭建","date":"2019-01-12T17:15:13.000Z","path":"p/12106/","text":"每次重装系统后的开发环境搭建，总是会花费大量的时间精力，软件下载安装啦，配置修改啦等等，索性把这些流程记录一下，毕竟时间就是金钱。 软件列表 JDK1.8 IntelliJ IDEA Navicat数据库管理工具 Postman Git SourceTree XShell5 DevCenter(cassandra数据库管理工具) RedisDesktopManager(redis管理工具) 这些工具已经可以满足我的日常工作了，什么印象笔记，markdownPad2等等不包含于此。这些都可以在我的博客下载到 -&gt; 常用软件集合 软件配置IntelliJ IDEA破解IntelliJ IDEA 注册码 主题当然是选黑色主题了，毕竟提倡保护眼睛。 字体我一般使用默认字体+14号大小。 设置编辑器的快捷键，也就是keymap由于以前用惯了Eclipse,所以还得改为Eclipse的快捷键 代码自动提示不区分大小写这个比较重要，毕竟谁也不可能无时无刻注意大小写，到时候不快捷提示就浪费太多时间了，也影响开发体验。 自动导入包和导入包优化的设置 Java代码默认注释一般创建一个java类的时候，需要指定创建者以及创建时间 注释代码可以自己决定，这里举个例子: 1234/** * @author gaochen * @date $&#123;DATE&#125; */ 然后创建的类是这样的: 1234/** * @author gaochen * @date 2019/1/31 */ IntelliJ IDEA启动设置不默认打开前一个项目 IntelliJ IDEA常用插件 .ignore:自动生成.ignore文件，并支持一键添加文件到.ignore列表 Grep Console:在控制台支持筛选，类似shell命令的cat 1.txt | grep ‘11’ Lombok plugin:使用lombok必须要装的一个插件 CodeGlance:代码编辑区迷你缩放图插件，非常好用 HighlightBracketPair:自动化高亮显示光标所在代码块对应的括号，可以定制颜色和形状，再也不怕看代码看到眼花了 Rainbow Brackets:彩色显示所有括号,有点类似上一个 Alibaba Java Coding Guidelines:阿里巴巴Java开发手册配套插件，一键扫描帮你优化代码。 Codota：让代码提示更加智能（只支持Java） Navicat破解Navicat Premium 12.0.18 / 12.0.24安装与激活 参考资料 Intellij IDEA插件推荐","tags":[{"name":"Java","slug":"Java","permalink":"https://gcdd1993.github.io/tags/Java/"}]},{"title":"EntityManager的Clear方法的使用","date":"2019-01-11T14:37:28.000Z","path":"p/12153/","text":"在日常开发中，如果使用hibernate的话，常常会被hibernate的事务搞得焦头烂额。今天解决了之前项目中一直存在的问题，记录一下。 问题描述有一张表TemplateCopy,如下 12345678910111213141516public class TemplateCopy &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String name; private String description; @OneToMany(mappedBy = &quot;template&quot;) private Set&lt;SubDomainWeightsCopy&gt; subDomainWeights; @OneToMany(mappedBy = &quot;template&quot;) private Set&lt;QuestionWeightsCopy&gt; questionWeights;&#125; 关联了两张表: 1234567891011121314151617181920212223public class SubDomainWeightsCopy &#123; @JsonIgnore @Id @ManyToOne @JoinColumn(name = &quot;template_id&quot;) private TemplateCopy template; @Id @ManyToOne @JoinColumn(name = &quot;sub_domain_id&quot;) private SubDomainCopy subDomain; private BigDecimal weights; //权重 private BigDecimal score; @Data public static class RelationId implements Serializable &#123; private Integer template; private Integer subDomain; &#125;&#125; 1234567891011121314151617181920212223public class QuestionWeightsCopy implements IWeightsValue &#123; @JsonIgnore @Id @ManyToOne @JoinColumn(name = &quot;template_id&quot;) private TemplateCopy template; @Id @ManyToOne @JoinColumn(name = &quot;question_id&quot;) private QuestionCopy question; private BigDecimal weights; private BigDecimal score; @Data public static class RelationId implements Serializable &#123; private Integer template; private Integer question; &#125;&#125; 简单的看一下，TemplateCopy中有一堆SubDomainWeightsCopy，和一堆QuestionWeightsCopy，我们在保存TemplateCopy的时候，通常按照如下来保存 123451. templateCopy = save(TemplateCopy)2. QuestionWeightsCopy.setTemplateCopy(templateCopy)3. save(QuestionWeightsCopy)4. SubDomainWeightsCopy.setTemplateCopy(templateCopy)5. save(SubDomainWeightsCopy) 到这就好了，数据库已经保存了关联关系。但是，这时候如果返回save好的templateCopy，subDomainWeights和questionWeights将会是null。 问题解决使用EntityManager的clear方法 保存完毕后，执行entityManager.clear(); 然后再次查询该对象，即可完整返回该对象。 EntityManager clear的作用？EntityManager clear方法会清空其关联的缓存，从而强制在事务中稍后执行新的数据库查询。 什么时候使用EntityManager clear 在进行批处理时，为了避免巨大的缓存占用内存并因长时间的脏检查而增加刷新的时间 在进行DML或SQL查询时，它将完全绕过实体管理器缓存。在这种情况下，由于缓存，将不会实际去数据库查，会直接将缓存返回。所以造成了数据库已经保存了，但是查出来还是未保存的状态。这时候需要清除缓存以避免这种不一致。(本案例就是这种情况的实际例子) 参考StackOverFlow大神回答","tags":[{"name":"Spring","slug":"Spring","permalink":"https://gcdd1993.github.io/tags/Spring/"}]},{"title":"Postman使用技巧","date":"2019-01-11T06:13:35.000Z","path":"p/4537/","text":"Postman是什么Postman是chrome的一款插件,用于做接口请求测试,无论是前端,后台还是测试人员,都可以用postman来测试接口,用起来非常方便。 Postman安装官网下载(翻墙)https://www.getpostman.com/downloads/ 蓝奏云https://www.lanzous.com/i2en5xc Postman常用功能安装好之后，我们先打开Postman，可以看到界面分成左右两个部分，右边是我们后头要讲的collection，左边是现在要讲的request builder。在request builder中，我们可以通过Postman快速的随意组装出我们希望的request。一般来说，所有的HTTP Request都分成4个部分，URL, method, headers和body。而Postman针对这几部分都有针对性的工具。 URL要组装一条Request, URL永远是你首先要填的内容，在Postman里面你曾输入过的URL是可以通过下拉自动补全的哦。如果你点击Params按钮，Postman会弹出一个键值编辑器，你可以在哪里输入URL的Parameter，Postman会帮你自动加入到URL当中，反之，如果你的URL当中已经有了参数，那Postman会在你打开键值编辑器的时候把参数自动载入 Headers点击’Headers’按钮，Postman同样会弹出一个键值编辑器。在这里，你可以随意添加你想要的Header attribute，同样Postman为我们通过了很贴心的auto-complete功能，敲入一个字母，你可以从下拉菜单里选择你想要的标准atrribute Method要选择Request的Method是很简单的，Postman支持所有的Method，而一旦你选择了Method，Postman的request body编辑器会根据的你选择，自动的发生改变。 Request Body如果我们要创建的request是类似于POST，那我们就需要编辑Request Body，Postman根据body type的不同，提供了4中编辑方式： form-data x-www-form-urlencoded raw binary （我们这里是可以传文件的哦） postman高级用法colletions(接口集合)在开发过程中，可能会遇到多项目同时开发维护的情况，Postman友好的提供了colletions功能，类似与项目文件夹一样，可以把归属于同一类的接口分类到一起，便于管理维护。 点击NEW -&gt; 选择collection，创建一个项目空间。 输入项目名称，点击create。 colletions folder(集合中的文件夹)每个项目会有多个接口，有些是一类功能，例如，用户管理接口，文章列表接口，Postman提供folder目录来进行细致的分类。 选择一个项目，点击Add Folder 输入目录名称，点击create 每个接口都可以归类到某个项目，或某个项目的子目录中。 Environment(环境变量)Postman允许定义自己的环境变量（Environment），最常见的是将测试 URL 进行定义成变量的形式，这样随着你的域名怎么变，URL 就不用变更，非常方便。除此之外，也可以将一些敏感的测试值定义为环境变量，比如密码。接下来，来看下怎么新建一组环境变量，如下操作打开环境变量的管理入口： 打开管理环境变量的窗口，输入名称，添加一组键值对，如下图所示： 环境变量要以双大括号的方式来引用，可以在右上方下拉框处选择相应的环境变量，我们实测一下刚才添加的Url的变量： 通过脚本设置变量Postman允许用户自定义脚本，并提供了两种类型的脚本： Pre-request Script：执行request请求前先运行，可以在里面预先设置些所需变量 Tests：request返回后执行的,可以对返回信息进行提取过滤，或者执行一些验证操作 例子获取如下返回信息中的user_id值: 123456789101112// 假设服务端返回的Body内容如下:&#123; &quot;token&quot;: &#123; &quot;user_id&quot;: &quot;2079876912&quot;, &quot;access_token&quot;: &quot;26A90E317DBC1AD363B2E2CE53F76F2DD85CB172DF7D813099477BAACB69DC49C794BAECEDC68331&quot;, &quot;expires_at&quot;: &quot;2016-06-22T12:46:51.637+0800&quot;, &quot;refresh_token&quot;: &quot;26A90E317DBC1AD3CD1556CF2B3923DD60AEBADDCBC1D9D899262A55D15273F735E407A6BEC56B84&quot;, &quot;mac_key&quot;: &quot;4FAhd4OpfC&quot;, &quot;mac_algorithm&quot;: &quot;hmac-sha-256&quot;, &quot;server_time&quot;: &quot;2016-06-15T12:46:51.649+0800&quot; &#125;&#125; 在Tests中对user_id值进行提取并赋值成全局变量: 1234567891011// 判断是否存在&#x27;user_id&#x27;值tests[&quot;Body contains user_id&quot;] = responseBody.has(&quot;user_id&quot;);if(tests[&quot;Body contains user_id&quot;])&#123; // 将返回信息解析成对象 var responseData = JSON.parse(responseBody); tests[&quot;value_user_id&quot;]=responseData.token.user_id // 设置全局变量 postman.setGlobalVariable(&quot;user_id&quot;,tests[&quot;value_user_id&quot;]);&#125;else&#123; postman.setGlobalVariable(&quot;user_id&quot;,&quot;默认user_id&quot;);&#125; 实践案例项目接口分类管理 登录获取token并设置为全局变量 接口使用登录后的token","tags":[{"name":"工具技巧","slug":"工具技巧","permalink":"https://gcdd1993.github.io/tags/%E5%B7%A5%E5%85%B7%E6%8A%80%E5%B7%A7/"}]},{"title":"Java8新特性一览表","date":"2019-01-09T12:16:58.000Z","path":"p/55630/","text":"总览 forEach() method in Iterable interface(Iterable接口中的forEach()方法) default and static methods in Interfaces(接口中的默认和静态方法) Functional Interfaces and Lambda Expressions(function接口和Lambda表达式) Java Stream API for Bulk Data Operations on Collections(用于集合上的批量数据操作的Java Stream API) Java Time API Collection API improvements Concurrency API improvements Java IO improvements 1.forEach() method in Iterable interface(Iterable接口中的forEach()方法)每当我们需要遍历Collection时，我们需要创建一个Iterator，其目的是迭代，然后我们在循环中为Collection中的每个元素提供业务逻辑。如果没有正确使用迭代器，会抛出异常ConcurrentModificationException。 Java 8在java.lang.Iterable接口中引入了forEach方法，这样在编写代码时我们只关注业务逻辑。 forEach方法将java.util.function.Consumer对象作为参数，因此它有助于将我们的业务逻辑放在我们可以重用的单独位置。让我们通过简单的例子看看每个用法。 12345678910111213141516171819202122232425List&lt;IntegermyList = new ArrayList&lt;Integer&gt;();for(int i=0; i&lt;10; i++) myList.add(i);//使用iteratorIterator&lt;Integeriterator = myList.iterator();while (iterator.hasNext()) &#123; Integer next = iterator.next(); System.out.println(&quot;Iterator Value::&quot; + next);&#125;//foreach + 匿名类myList.forEach(new Consumer&lt;Integer&gt;() &#123; public void accept(Integer t) &#123; System.out.println(&quot;forEach anonymous class Value::&quot;+t); &#125;&#125;);//使用consumer 接口MyConsumer action = new MyConsumer();myList.forEach(action);//使用lambda表达式myList.forEach(System.out::println); 2.default and static methods in Interfaces(接口中的默认和静态方法)jdk8之前，interface方法不能有实现，但是从Java 8开始，接口被增强为具有实现方法。我们可以使用default和static关键字来创建具有方法实现的接口。例如Iterable接口中的forEach方法实现是 123456default void forEach(Consumer&lt;? super Taction) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 示例代码创建一个接口 123456789101112public interface MyInterface &#123; void show(); default void showA() &#123; System.out.println(&quot;我是接口默认方法&quot;); &#125; static void showB() &#123; System.out.println(&quot;我是接口静态方法&quot;); &#125;&#125; 创建该接口实现类 1234567891011121314public class MyClass implements MyInterface &#123; @Override public void show() &#123; System.out.println(&quot;我是实现方法&quot;); &#125; //默认方法支持重写,不覆盖则执行接口的默认方法 @Override public void showA() &#123; System.out.println(&quot;我覆盖了接口的默认方法&quot;); &#125; //静态方法不可以重写&#125; 测试 123456789public class Test &#123; public static void main(String[] args) &#123; MyClass myClass = new MyClass(); myClass.show(); myClass.showA(); //通过类名.方法名调用接口静态方法 MyInterface.showB(); &#125;&#125; 3.Functional Interfaces and Lambda Expressions（function接口和Lambda表达式）如果你注意到上面的接口代码，你会注意到@FunctionalInterface注释。功能接口是Java 8中引入的新概念。只有一个抽象方法的接口就变成了功能接口。我们不需要使用@FunctionalInterface注释将接口标记为功能接口。 @FunctionalInterface注释是一种避免在功能界面中意外添加抽象方法的工具。您可以将其视为@Override注释，并且最佳实践是使用它。实例：java8 的runnable run接口，带有一个抽象方法: 1234@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; 功能接口的主要好处之一是可以使用lambda表达式来实例化它们。我们可以使用匿名类实例化一个接口，但代码看起来很笨重。 1234567//使用匿名类实例化Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;My Runnable&quot;); &#125;&#125;; 由于功能接口只有一个方法，因此lambda表达式可以很容易地提供方法实现。我们只需要提供方法参数和业务逻辑。例如，我们可以使用lambda表达式将上面的实现编写为： 1234//使用lambda表达式Runnable runnable1 = () -System.out.println(&quot;My Runnable&quot;);runnable.run();runnable1.run(); 如果在方法实现中有单个语句，我们也不需要花括号。例如，上面的Interface1匿名类可以使用lambda实例化，如下所示： 12Interface1 interface1 = (s) -System.out.println(s);interface1.method1(&quot;interface1 method&quot;); lambda表达式扩展Java 中的 Lambda 表达式通常使用 (argument) -(body) 语法书写，例如：12(arg1, arg2...) -&gt; &#123; body &#125;(type1 arg1, type2 arg2...) -&gt; &#123; body &#125; Lambda 表达式的结构 一个Lambda表达式可以有零个或多个参数 参数的类型既可以明确声明，也可以根据上下文来推断。例如：(int a)与(a)效果相同 所有参数需包含在圆括号内，参数之间用逗号相隔。例如：(a, b) 或 (int a, int b) 或 (String a, int b, float c) 空圆括号代表参数集为空。例如：() -&gt; 42 当只有一个参数，且其类型可推导时，圆括号（）可省略。例如：a -&gt; return a * a Lambda表达式的主体可包含零条或多条语句 如果Lambda表达式的主体只有一条语句，花括号{}可省略。匿名函数的返回类型与该主体表达式一致 如果Lambda表达式的主体包含一条以上语句，则表达式必须包含在花括号{}中（形成代码块）。匿名函数的返回类型与代码块的返回类型一致，若没有返回则为空 函数式接口扩展函数式接口是只包含一个抽象方法声明的接口,可以使用@FunctionalInterface标记 JDK8之前已有的函数式接口 java.lang.Runnable java.util.concurrent.Callable java.security.PrivilegedAction java.util.Comparator java.io.FileFilter java.nio.file.PathMatcher java.lang.reflect.InvocationHandler java.beans.PropertyChangeListener java.awt.event.ActionListener javax.swing.event.ChangeListener 新定义的函数式接口java.util.function中定义了几组类型的函数式接口以及针对基本数据类型的子接口。 Predicate:传入一个参数，返回一个bool结果，方法为boolean test(T t) Consumer:传入一个参数，无返回值，纯消费。方法为void accept(T t) Function:传入一个参数，返回一个结果，方法为R apply(T t) Supplier:无参数传入，返回一个结果，方法为T get() UnaryOperator:一元操作符，继承Function,传入参数的类型和返回类型相同。 BinaryOperator:二元操作符，传入的两个参数的类型和返回类型相同，继承BiFunction。 【示例】 12345678910111213Predicate&lt;Integer&gt; predicate = (i) -&gt; i &gt; 0;Consumer&lt;Integer&gt; consumer = (i) -&gt; System.out.println(&quot;consumer : &quot; + i);Function&lt;Integer,Boolean&gt; function = (i) -&gt; i &gt; 0;Supplier&lt;Integer&gt; supplier = () -&gt; 1;UnaryOperator&lt;Integer&gt; unaryOperator = (i) -&gt; i * i;BinaryOperator&lt;Integer&gt; binaryOperator = (i1,i2) -&gt; i1 * i2;System.out.println(predicate.test(10));consumer.accept(10);System.out.println(function.apply(10));System.out.println(supplier.get());System.out.println(unaryOperator.apply(100));System.out.println(binaryOperator.apply(100,200)); 4.Java Stream API for Bulk Data Operations on Collections(用于集合上的批量数据操作的Java Stream API)示例代码123456789101112131415161718192021222324252627282930313233//从 Collection 和数组List&lt;Integerlist = new ArrayList&lt;&gt;();for(int i=0;i&lt;100;i++) &#123; list.add(i);&#125;Stream&lt;Integerstream = list.stream(); //串行流Stream&lt;Integerstream1 = list.parallelStream(); //并行流Stream&lt;Integerstream2 = Arrays.stream(list.toArray(new Integer[0]));Stream&lt;Integerstream3 = Stream.of(list.toArray(new Integer[0]));//从 BufferedReaderBufferedReader bufferedReader = new BufferedReader(new FileReader(new File(&quot;path&quot;)));Stream&lt;Stringstream4 = bufferedReader.lines();//静态工厂IntStream stream5 = IntStream.rangeClosed(1, 100);//生成1-100 的int streamStream&lt;Pathstream6 = Files.walk(Paths.get(&quot;path&quot;), 100);//自己构建 通过StreamSupport辅助类从spliterator产生流Stream&lt;Integerstream7 = StreamSupport.stream(list.spliterator(), false);//其它Random random = new Random();IntStream stream8 = random.ints();BitSet bitSet = BitSet.valueOf(new long[]&#123;1L, 2L, 3L&#125;);IntStream stream9 = bitSet.stream();Pattern pattern = Pattern.compile(&quot;\\\\d+&quot;);Stream&lt;Stringstream10 = pattern.splitAsStream(&quot;111sda123sda&quot;);JarFile jarFile = new JarFile(&quot;xxx.jar&quot;);Stream&lt;JarEntrystream11 = jarFile.stream(); 5.Java Time API(Java时间API)Java 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。对日期与时间的操作一直是Java程序员最痛苦的地方之一。标准的 java.util.Date以及后来的java.util.Calendar一点没有改善这种情况（可以这么说，它们一定程度上更加复杂）。 Clock类它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock可以替换System.currentTimeMillis()与TimeZone.getDefault()。 1234// Get the system clock as UTC offset final Clock clock = Clock.systemUTC();System.out.println(clock.instant());System.out.println(clock.millis()); 下面是程序在控制台上的输出: 122019-01-09T14:52:50.111Z1547045570335 LocaleDate与LocalTimeLocaleDate只持有ISO-8601格式且无时区信息的日期部分。相应的，LocaleTime只持有ISO-8601格式且无时区信息的时间部分。LocaleDate与LocalTime都可以从Clock中得到。 12345678910111213// Get the local date and local timefinal LocalDate date = LocalDate.now();final LocalDate dateFromClock = LocalDate.now(clock);System.out.println(date);System.out.println(dateFromClock);// Get the local date and local timefinal LocalTime time = LocalTime.now();final LocalTime timeFromClock = LocalTime.now(clock);System.out.println(time);System.out.println(timeFromClock); 下面是程序在控制台上的输出： 12342019-01-092019-01-0922:52:50.38314:52:50.383 LocaleDateTimeLocaleDateTime把LocaleDate与LocaleTime的功能合并起来，它持有的是ISO-8601格式无时区信息的日期与时间。 123456// Get the local date/timefinal LocalDateTime datetime = LocalDateTime.now();final LocalDateTime datetimeFromClock = LocalDateTime.now(clock);System.out.println(datetime);System.out.println(datetimeFromClock); 下面是程序在控制台上的输出: 122019-01-09T22:55:05.1942019-01-09T14:55:05.194 ZonedDateTime如果你需要特定时区的日期/时间，那么ZonedDateTime是你的选择。它持有ISO-8601格式具具有时区信息的日期与时间。 12345678// Get the zoned date/timefinal ZonedDateTime zonedDatetime = ZonedDateTime.now();final ZonedDateTime zonedDatetimeFromClock = ZonedDateTime.now(clock);final ZonedDateTime zonedDatetimeFromZone = ZonedDateTime.now(ZoneId.of(&quot;America/Los_Angeles&quot;));System.out.println(zonedDatetime);System.out.println(zonedDatetimeFromClock);System.out.println(zonedDatetimeFromZone); 下面是程序在控制台上的输出： 1232019-01-09T22:56:34.033+08:00[Asia/Shanghai]2019-01-09T14:56:34.033Z2019-01-09T06:56:34.035-08:00[America/Los_Angeles] Duration在秒与纳秒级别上的一段时间。Duration使计算两个日期间的不同变的十分简单。 1234567// Get duration between two datesfinal LocalDateTime from = LocalDateTime.of(2018, Month.APRIL, 16, 0, 0, 0);final LocalDateTime to = LocalDateTime.of(2019, Month.APRIL, 16, 23, 59, 59);final Duration duration = Duration.between(from, to);System.out.println(&quot;Duration in days: &quot; + duration.toDays());System.out.println(&quot;Duration in hours: &quot; + duration.toHours()); 上面的例子计算了两个日期2018年4月16号与2019年4月16号之间的过程。下面是程序在控制台上的输出： 12Duration in days: 365Duration in hours: 8783 Collection API improvements(集合API改进)上面已经展示了forEach()方法和Stream API在集合上的使用。java8的Collection API中添加了一些新方法： Iterator default method forEachRemaining(Consumer action)为每个元素执行给定操作，直到所有元素都已处理或操作引发异常。 源码1234567default void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; //传入一个非空消费者 Objects.requireNonNull(action); //遍历执行消费者函数 while (hasNext()) action.accept(next());&#125; 示例代码123456List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);Iterator&lt;Integer&gt; iterator = list.iterator();//创建一个消费者Consumer&lt;Integer&gt; consumer = i -&gt; System.out.println(&quot;consumer print &quot; + i);//iterator的forEachRemaining将集合中的每个元素消费iterator.forEachRemaining(consumer); 控制台输出1234consumer print 1consumer print 2consumer print 3... Collection default method removeIf(Predicate filter)删除满足给定条件的此集合的所有元素。 源码1234567891011121314default boolean removeIf(Predicate&lt;? super E&gt; filter) &#123; //传入一个非空谓语 Objects.requireNonNull(filter); boolean removed = false; final Iterator&lt;E&gt; each = iterator(); while (each.hasNext()) &#123; //遍历元素，执行谓语的校验，如果为真，则删除该元素 if (filter.test(each.next())) &#123; each.remove(); removed = true; &#125; &#125; return removed;&#125; 示例代码12345678List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(1);list.add(2);list.add(3);list.add(4);Predicate&lt;Integer&gt; predicate = i -&gt; i &gt; 1;list.removeIf(predicate);System.out.println(&quot;remove if left items : &quot; + list); 控制台输出12//2,3,4满足条件被删除了remove if left items : [1] Collection spliterator()返回Spliterator实例的方法，该实例可用于顺序或并行遍历元素。 源码1234//该方法是接口默认方法default Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, Spliterator.ORDERED);&#125; 示例代码123456List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);Spliterator&lt;Integer&gt; spliterator = list.spliterator();//创建顺序流Stream&lt;Integer&gt; stream = StreamSupport.stream(spliterator, false);//创建并行流Stream&lt;Integer&gt; parallelStream = StreamSupport.stream(spliterator, true); Map replaceAll(), compute(), merge() methodsreplaceAll()替换Map中所有Entry的value值，这个值由旧的key和value计算得出，接收参数 (K, V) -&gt; V 源码12345678910111213141516public void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function) &#123; Node&lt;K,V&gt;[] tab; if (function == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; //使用给定的函数替换原来的value值，key不变 e.value = function.apply(e.key, e.value); &#125; &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125;&#125; 示例代码123456789Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;1&quot;, &quot;A&quot;);map.put(&quot;2&quot;, &quot;B&quot;);map.put(&quot;3&quot;, &quot;C&quot;);map.put(&quot;4&quot;, &quot;D&quot;);map.put(&quot;5&quot;, &quot;E&quot;);//replaceAll方法map.replaceAll((s, s2) -&gt; s + s2);System.out.println(map); 控制台输出12//原来的value由key + value替换掉了&#123;1=1A, 2=2B, 3=3C, 4=4D, 5=5E&#125; compute()是computeIfPresent和computeIfAbsent方法的组合体 computeIfPresent:如果指定的key不存在，则通过指定的K -&gt; V计算出新的值设置为key的值。 computeIfPresent:如果指定的key存在，则根据旧的key和value计算新的值newValue, 如果newValue不为null，则设置key新的值为newValue, 如果newValue为null, 则删除该key的值。 示例代码123456789101112131415Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;1&quot;, &quot;A&quot;);map.put(&quot;2&quot;, &quot;B&quot;);map.put(&quot;3&quot;, &quot;C&quot;);map.put(&quot;4&quot;, &quot;D&quot;);map.put(&quot;5&quot;, &quot;E&quot;);//key存在，根据旧的key和value计算新的值newValuemap.compute(&quot;1&quot;, (k, v) -&gt; v + &quot; computed&quot;);System.out.println(&quot;key存在&quot; + map.get(&quot;1&quot;));//key不存在，通过指定的K -&gt; V计算出新的值设置为key的值map.compute(&quot;6&quot;, (k, v) -&gt; &quot;F&quot;);System.out.println(&quot;key不存在&quot; + map.get(&quot;6&quot;));//key存在，如果newValue为null, 则删除该key的值map.compute(&quot;1&quot;, (k, v) -&gt; null);System.out.println(&quot;key存在，设置为null &quot; + map.get(&quot;1&quot;)); 控制台输出123key存在A computedkey不存在Fkey存在，设置为null null merge()如果指定的key不存在，则设置指定的value值，否则根据key的旧的值oldvalue，value计算出新的值newValue, 如果newValue为null, 则删除该key，否则设置key的新值newValue。 示例代码123456789101112Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;1&quot;, &quot;A&quot;);map.put(&quot;2&quot;, &quot;B&quot;);map.put(&quot;3&quot;, &quot;C&quot;);map.put(&quot;4&quot;, &quot;D&quot;);map.put(&quot;5&quot;, &quot;E&quot;);//存在key为1,输出 AmergeSystem.out.println(map.merge(&quot;1&quot;, &quot;merge&quot;, (k, v) -&gt; k + v));//新值为null，删除key，输出 nullSystem.out.println(map.merge(&quot;1&quot;, &quot;merge&quot;, (k, v) -&gt; null));//不存在key为6，输出 &quot;merge&quot;System.out.println(map.merge(&quot;6&quot;, &quot;merge&quot;, (k, v) -&gt; k + v)); 控制台输出123Amergenullmerge Performance Improvement for HashMap class with Key Collisions具有键冲突的HashMap类的性能改进 Concurrency API improvements(并发API改进)ConcurrentHashMapJDK8提供的并发友好的HashMap CompletableFuture提供了非常强大的 Future 的扩展功能，可以帮助我们简化异步编程的复杂性，并且提供了函数式编程的能力，可以通过回调的方式处理计算结果，也提供了转换和组合 CompletableFuture 的方法。 Executors newWorkStealingPool()创建持有足够线程的线程池来支持给定的并行级别，并通过使用多个队列，减少竞争，它需要传一个并行级别的参数，如果不传，则被设定为默认的CPU数量。 Java IO improvements(Java IO API的改进)Files.list(Path dir)返回一个延迟填充的Stream，其中的元素是目录中的条目。 123//返回目录下的元素集合流Stream&lt;Path&gt; list = Files.list(new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop&quot;).toPath());list.forEach(System.out::println); Files.lines(Path path)从文件中读取所有行作为流。 123//返回文件中的所有行数Stream&lt;String&gt; lines = Files.lines(new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\new 3.txt&quot;).toPath());lines.forEach(System.out::println); Files.find()通过搜索以给定起始文件为根的文件树中的文件，返回使用Path延迟填充的Stream。 12345//返回符合判断条件的Path流Stream&lt;Path&gt; stream = Files.find(new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop&quot;).toPath(), 1, (path, basicFileAttributes) -&gt; basicFileAttributes.isDirectory());stream.forEach(System.out::println); BufferedReader.lines()返回一个Stream，其元素是从这个BufferedReader读取的行。 1234//返回文件中的所有行数,类似Files.lines()BufferedReader br = new BufferedReader(new FileReader(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\new 3.txt&quot;));Stream&lt;String&gt; stringStream = br.lines();stringStream.forEach(System.out::println); 参考资源 Java 8 Features with Examples 为并发而生的 ConcurrentHashMap（Java 8） 通过实例理解 JDK8 的 CompletableFuture","tags":[{"name":"Java","slug":"Java","permalink":"https://gcdd1993.github.io/tags/Java/"}]},{"title":"PG数据库常用操作","date":"2019-01-09T11:36:07.000Z","path":"p/59866/","text":"记录一下，在开发过程中接触到的一些PG数据库常用操作，以备不时之需。 全量迁移 备份数据 1$ pg_dump -h 172.19.235.145 -U &lt;username&gt; -d &lt;database&gt; &gt; 20180704_dbpe.sql 正式迁移 首先要修改备份文件*.sql的owner，防止权限出现错误。 1$ psql -h &lt;ip&gt; -U &lt;username&gt; -d &lt;database&gt; -f 20180704_dbpe.sql 【注意点】该迁移操作会覆盖原来的数据库，所以最好创建一个新库。 列出所有表名和数据库名1select tablename from pg_tables where schemaname =&#x27;public&#x27;; PostgreSQL 中 有时候想删除数据库（drop database swiftliveqaapi;），发现提示“ERROR: database “xxxxxx” is being accessed by other users DETAIL: There are 30 other sessions using the database.”123用psql 登录进入， 执行语句：SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname=&#x27;数据库名&#x27; AND pid&lt;&gt;pg_backend_pid();然后就可以删除数据库了 修改表的序列为id最大值1SELECT setval(&#x27;表名_id_seq&#x27;, (SELECT MAX(id) FROM 表名)); 查询表结构1234567891011SELECT COLUMN_NAME AS 列名, DATA_TYPE AS 字段类型, CHARACTER_MAXIMUM_LENGTH AS 长度, IS_NULLABLE AS 是否为空, COLUMN_DEFAULT AS 默认值 FROM INFORMATION_SCHEMA.COLUMNS WHERE table_schema = &#x27;public&#x27; AND TABLE_NAME = &#x27;表名&#x27;; PG 数据库状态，启动，停止123$ pg_ctlcluster 9.5 main status$ pg_ctlcluster 9.5 main start$ pg_ctlcluster 9.5 main stop","tags":[{"name":"数据库","slug":"数据库","permalink":"https://gcdd1993.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Spring-Security无法正常捕捉到UsernameNotFoundException异常","date":"2019-01-08T13:25:54.000Z","path":"p/31514/","text":"前言在Web应用开发中,安全一直是非常重要的一个方面。在庞大的spring生态圈中，权限校验框架也是非常完善的。其中，spring security是非常好用的。今天记录一下在开发中遇到的一个spring-security相关的问题。 问题描述使用spring security进行授权登录的时候，发现登录接口无法正常捕捉UsernameNotFoundException异常，捕捉到的一直是BadCredentialsException异常。我们的预期是： UsernameNotFoundException -&gt; 用户名错误 BadCredentialsException -&gt; 密码错误 贴几个比较重要的代码： 1. 登录业务逻辑12345678910111213141516171819202122232425@Servicepublic class AuthServiceImpl implements AuthService &#123; @Autowired private UserDetailsService userDetailsService; @Autowired private AuthenticationManager authenticationManager; @Autowired private JwtTokenUtil jwtTokenUtil; @Override public JwtAuthenticationResponse login(String username, String password) &#123; //构造spring security需要的UsernamePasswordAuthenticationToken UsernamePasswordAuthenticationToken upToken = new UsernamePasswordAuthenticationToken(username, password); //调用authenticationManager.authenticate(upToken)方法验证 //该方法将会执行UserDetailsService的loadUserByUsername验证用户名 //以及PasswordEncoder的matches方法验证密码 val authenticate = authenticationManager.authenticate(upToken); JwtUser userDetails = (JwtUser) authenticate.getPrincipal(); val token = jwtTokenUtil.generateToken(userDetails); return new JwtAuthenticationResponse(token, userDetails.getId(), userDetails.getUsername()); &#125;&#125; 2. spring security 的UserDetailsService 实现类12345678910111213141516@Servicepublic class JwtUserDetailsServiceImpl implements UserDetailsService &#123; @Autowired private UserRepository userRepository; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; AbstractUser abstractUser = userRepository.findByUsername(username); //如果通过用户名找不到用户，则抛出UsernameNotFoundException异常 if (abstractUser == null) &#123; throw new UsernameNotFoundException(String.format(&quot;No abstractUser found with username &#x27;%s&#x27;.&quot;, username)); &#125; else &#123; return JwtUserFactory.create(abstractUser); &#125; &#125;&#125; 3. 登录接口123456789101112try &#123; final JwtAuthenticationResponse jsonResponse = authService.login(authenticationRequest.getUsername(), authenticationRequest.getPassword()); //存入redis redisService.setToken(jsonResponse.getToken()); return ok(jsonResponse);&#125; catch (BadCredentialsException e) &#123; //捕捉到BadCredentialsException，密码不正确 return forbidden(LOGIN_PASSWORD_ERROR, request);&#125; catch (UsernameNotFoundException e) &#123; //捕捉到UsernameNotFoundException，用户名不正确 return forbidden(LOGIN_USERNAME_ERROR, request);&#125; 在上述代码中，如果用户名错误，应该执行123catch (UsernameNotFoundException e) &#123; return forbidden(LOGIN_USERNAME_ERROR, request);&#125; 如果密码错误，应该执行123catch (BadCredentialsException e) &#123; return forbidden(LOGIN_PASSWORD_ERROR, request);&#125; 实际上，不管是抛出什么错，最后抓到的都是BadCredentialsException 问题定位debug大法断点 跟踪经过步进法跟踪代码，发现问题所在，位于 12AbstractUserDetailsAuthenticationProviderpublic Authentication authenticate(Authentication authentication) 结论 loadUserByUsername方法确实抛出了UsernameNotFoundException 走到AbstractUserDetailsAuthenticationProvider的authenticate方法的时候，如果hideUserNotFoundExceptions = true，直接就覆盖了UsernameNotFoundException异常并抛出BadCredentialsException异常，这也就解释了，为什么总是捕捉到BadCredentialsException异常 问题解决既然已经找到了是因为hideUserNotFoundExceptions = true导致的问题，那把hideUserNotFoundExceptions = false不就完事了吗？ 方案1参考stackoverflow大神回答 修改WebSecurityConfig配置，添加AuthenticationProvider Bean12345678@Beanpublic AuthenticationProvider daoAuthenticationProvider() &#123; DaoAuthenticationProvider daoAuthenticationProvider = new DaoAuthenticationProvider(); daoAuthenticationProvider.setUserDetailsService(userDetailsService); daoAuthenticationProvider.setPasswordEncoder(passwordEncoder()); daoAuthenticationProvider.setHideUserNotFoundExceptions(false); return daoAuthenticationProvider;&#125; 配置AuthenticationProvider Bean12345@Autowiredpublic void configureAuthentication(AuthenticationManagerBuilder authenticationManagerBuilder) throws Exception &#123; authenticationManagerBuilder .authenticationProvider(daoAuthenticationProvider());&#125; 方案2由于以前项目中也是一样的技术栈，而且代码也差不多，登录这段逻辑可以说是完全相同，不过之前就一直都没有这个问题。反复查看之后发现，在login的代码有些不同 在 1val authenticate = authenticationManager.authenticate(upToken); 前面还有一个 12//执行UserDetailsService的loadUserByUsername验证用户名userDetailsService.loadUserByUsername(authenticationRequest.getUsername()); 该方法会直接抛出UsernameNotFoundException，而不走spring security的AbstractUserDetailsAuthenticationProvider，也就不存在被转换为BadCredentialsException了。 但是这个方案有个缺点， 如果验证用户名通过以后，再次调用 1val authenticate = authenticationManager.authenticate(upToken); 还会再执行一遍 1userDetailsService.loadUserByUsername(authenticationRequest.getUsername()); 该操作是冗余的，产生了不必要的数据库查询工作。 推荐使用方案1","tags":[{"name":"Spring-Security","slug":"Spring-Security","permalink":"https://gcdd1993.github.io/tags/Spring-Security/"}]},{"title":"常用软件集合","date":"2019-01-08T08:37:14.000Z","path":"p/37491/","text":"常用软件工具收藏集，收藏了在工作生活中遇到的好用实用的软件。 开发工具 BeyondCompare破解版 Navicat Premium 12破解版 markdown pad2破解版 密码:23w2 RedisDesktopManager 免费版 密码:ciq1 QTransate翻译工具 正则表达式测试工具 draw.io拓扑图工具 DevCenter cassandra管理工具 PostMan便携版 Git SourceTree(Git Gui工具) XShell5破解版 实用工具 RSS订阅工具(只限windows) 科学上网ShadowSocks win10激活工具 Office安装工具 OCR文字识别工具 GIF录制工具 冰点文库下载器破解版 推荐工具 现代化Markdown编辑工具","tags":[{"name":"软件","slug":"软件","permalink":"https://gcdd1993.github.io/tags/%E8%BD%AF%E4%BB%B6/"}]},{"title":"BigDecimal精确计算工具类","date":"2019-01-08T07:28:49.000Z","path":"p/14256/","text":"前言在实际开发中，遇到例如货币，统计等商业计算的时候，一般需要采用java.math.BigDecimal类来进行精确计算。而这类操作通常都是可预知的，也就是通用的。所以，写了个工具类来方便以后的工作。这是仓库地址：仓库地址 BigDecimal的构建一般而言，我们主要从int,long,double,float来进行计算，在构建的时候推荐使用 1BigDecimal BigDecimal(String s); 因为通过double构造会损失精度，而String构造是固定的值。创建以下方法作为通用BigDecimal转化器： 123456789101112131415161718/** * Number -&gt; BigDecimal */public static &lt;T extends Number&gt; BigDecimal transform(T v) &#123; if (v instanceof Double) &#123; return new BigDecimal(Double.toString((Double) v)); &#125; else if (v instanceof Integer) &#123; return new BigDecimal(Integer.toString((Integer) v)); &#125; else if (v instanceof Long) &#123; return new BigDecimal(Long.toString((Long) v)); &#125; else if (v instanceof Short) &#123; return new BigDecimal(Short.toString((Short) v)); &#125; else if (v instanceof Float) &#123; return new BigDecimal(Float.toString((Float) v)); &#125; else &#123; return (BigDecimal) v; &#125;&#125; BigDecimal方法计算类型加减乘除四种，BigDecimal提供的方法也是围绕这四种计算类型设计的。 1234BigDecimal add(BigDecimal augend) //加BigDecimal subtract(BigDecimal subtrahend) //减BigDecimal multiply(BigDecimal multiplicand) //乘BigDecimal divide(BigDecimal divisor, int scale, RoundingMode roundingMode) //除 工具类在加减乘除基础上，提供了 链式计算，类似JDK8 lamada api，爽快丝滑的编程体验 支持集合求和、求平均 支持复合计算，例如2*(2+8) BigDecimal精确计算工具类实用案例精确转换为BigDecimal，不指定精度12345System.out.println(PreciseCalculations.transform(121.11)); //转化double -&gt; 121.11System.out.println(PreciseCalculations.transform(Integer.MAX_VALUE)); //转化int -&gt; 2147483647System.out.println(PreciseCalculations.transform(Short.MAX_VALUE)); //转化Short -&gt; 32767System.out.println(PreciseCalculations.transform(Long.MAX_VALUE)); //转化long -&gt; 9223372036854775807System.out.println(PreciseCalculations.transform(121.19F)); //转化float -&gt; 121.19 精确转换为BigDecimal，指定精度12System.out.println(PreciseCalculations.transform(121.1111111111, 5)); //精度大于指定精度 -&gt; 121.11111System.out.println(PreciseCalculations.transform(121.11, 5)); //精度小于指定精度，补零 -&gt; 121.11000 加减乘除1234System.out.println(PreciseCalculations.add(12.11, 12.11)); //加法 -&gt; 24.22System.out.println(PreciseCalculations.subtract(12.11, 12.11)); //减法 -&gt; 0.00System.out.println(PreciseCalculations.multiply(12.11, 12.11)); //乘法 -&gt; 146.6521System.out.println(PreciseCalculations.divide(12.11, 2.35, 5)); //除法 -&gt; 5.15319 负数计算1234// -1.11 * 13 - 90 = -104.43System.out.println(new PreciseCalculation(-1.11).multiply(13).add(-90).getValue()); // -11.11111111 + 90 = 78.88888889System.out.println(PreciseCalculations.add(-11.11111111,90)); 集合 求和 求平均值1234List&lt;Double&gt; list = Arrays.asList(12.11D, 13.11D, 14.11D, 15.321312D);System.out.println(PreciseCalculations.sum(list)); //求和 -&gt; Optional[54.651312]System.out.println(PreciseCalculations.average(list)); //平均值 -&gt; Optional[13.66283]System.out.println(PreciseCalculations.average(Collections.emptyList())); //空集合 -&gt; Optional.empty 复合计算12345// 计算 121.11 * 13 / 60 + 100 - 12 = 114.24050System.out.println(new PreciseCalculation(121.11).multiply(13).divide(60, 5).add(100).subtract(12).getValue());//计算 121.11 * 128.59 / (100 + 12) - 100 = 39.04942System.out.println(new PreciseCalculation(121.11).multiply(128.59).divide( new PreciseCalculation(100).add(12), 5).subtract(100).getValue()); 注意事项 PreciseCalculation 核心类，提供加减乘除、集合精确计算方法，内部维护value值，每次计算该value都会改变。 PreciseCalculations 基于上述的工具类，方便简单计算时使用。","tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://gcdd1993.github.io/tags/JAVA/"}]}]